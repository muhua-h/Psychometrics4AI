{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Multi-Model Behavioral Validation - Study 4\n",
    "\n",
    "This notebook refactors the original Study 4 moral and risk-taking simulations to work with multiple LLM models using the unified portal.py interface.\n",
    "\n",
    "## Models to Test\n",
    "- GPT-4\n",
    "- GPT-4o  \n",
    "- Llama-3.3-70B-Instruct\n",
    "- DeepSeek-V3\n",
    "\n",
    "## Simulations\n",
    "1. **Moral Reasoning**: 5 ethical dilemma scenarios\n",
    "2. **Risk Taking**: Risk assessment scenarios\n",
    "\n",
    "## Data Flow\n",
    "1. Load personality-assigned participants data\n",
    "2. Run moral reasoning simulations across all models\n",
    "3. Run risk-taking simulations across all models\n",
    "4. Save results for comparative analysis"
   ]
  },
  {
   "cell_type": "code",
   "metadata": {},
   "outputs": [],
   "source": "import pandas as pd\nimport sys\nfrom pathlib import Path\n\n# Add shared modules to path\nsys.path.append('../shared')\n\nfrom simulation_utils import (\n    SimulationConfig, \n    run_moral_simulation,\n    run_risk_simulation,\n    retry_failed_participants,\n    save_simulation_results\n)"
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Data Loading"
   ]
  },
  {
   "cell_type": "code",
   "metadata": {},
   "outputs": [],
   "source": "# Load the personality data (from Study 4 original data)\ndata_path = Path('../../study_4/data/york_data_clean.csv')\nif not data_path.exists():\n    print(f\"Data file not found at {data_path}\")\n    print(\"Expected path:\", data_path)\n    print(\"Please ensure the Study 4 data is available.\")\n    raise FileNotFoundError(f\"Data file not found: {data_path}\")\nelse:\n    df = pd.read_csv(data_path)\n    print(f\"Loaded data shape: {df.shape}\")\n    print(f\"Columns with 'bfi' in name: {[col for col in df.columns if 'bfi' in col.lower()][:10]}\")\n    df.head()"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Check for the personality combination column\n",
    "personality_columns = [col for col in df.columns if 'combined' in col.lower() or 'bfi_combined' in col]\n",
    "print(f\"Available personality columns: {personality_columns}\")\n",
    "\n",
    "# Use the appropriate personality column\n",
    "if 'bfi_combined' in df.columns:\n",
    "    personality_key = 'bfi_combined'\n",
    "elif len(personality_columns) > 0:\n",
    "    personality_key = personality_columns[0]\n",
    "else:\n",
    "    print(\"Warning: No combined personality column found. You may need to create it first.\")\n",
    "    personality_key = 'bfi_combined'\n",
    "\n",
    "print(f\"Using personality key: {personality_key}\")\n",
    "\n",
    "# Preview a personality description\n",
    "if personality_key in df.columns:\n",
    "    print(\"\\nSample personality description:\")\n",
    "    print(df.iloc[0][personality_key][:500] + \"...\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Prepare participant data\n",
    "participants_data = df.to_dict('records')\n",
    "print(f\"Prepared {len(participants_data)} participants for simulation\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Simulation Configuration"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Configuration for different models and scenarios\n",
    "models_to_test = ['gpt-4', 'gpt-4o', 'llama', 'deepseek']\n",
    "temperatures = [0.0, 1.0]  # Test both deterministic and stochastic responses\n",
    "batch_size = 20  # Conservative batch size for complex scenarios\n",
    "\n",
    "# Results storage\n",
    "moral_results = {}\n",
    "risk_results = {}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Moral Reasoning Simulations"
   ]
  },
  {
   "cell_type": "code",
   "metadata": {},
   "outputs": [],
   "source": "print(\"Starting Moral Reasoning Simulations\")\nprint(\"=\" * 60)\n\nfor model in models_to_test:\n    for temperature in temperatures:\n        print(f\"\\nRunning moral simulation: {model} with temperature {temperature}\")\n        \n        config = SimulationConfig(\n            model=model,\n            temperature=temperature,\n            batch_size=batch_size,\n            max_workers=8\n        )\n        \n        try:\n            results = run_moral_simulation(\n                participants_data=participants_data,\n                config=config,\n                output_dir=\"study_4_results/moral\"\n            )\n            \n            # Store results\n            key = f\"{model}_temp{temperature}\"\n            moral_results[key] = results\n            \n            # Check for failures\n            failed_count = sum(1 for r in results if isinstance(r, dict) and 'error' in r)\n            success_rate = ((len(results) - failed_count) / len(results)) * 100\n            \n            print(f\"Completed: {len(results)} participants, {failed_count} failed, {success_rate:.1f}% success rate\")\n            \n        except Exception as e:\n            print(f\"Error in moral simulation {model} temp {temperature}: {str(e)}\")\n            moral_results[f\"{model}_temp{temperature}\"] = {\"error\": str(e)}\n\nprint(f\"\\nMoral simulations completed. Results: {list(moral_results.keys())}\")"
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": "<cell_type>markdown</cell_type>## Risk-Taking Simulations",
   "outputs": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"Starting Risk-Taking Simulations\")\n",
    "print(\"=\" * 60)\n",
    "\n",
    "for model in models_to_test:\n",
    "    for temperature in temperatures:\n",
    "        print(f\"\\nRunning risk simulation: {model} with temperature {temperature}\")\n",
    "        \n",
    "        config = SimulationConfig(\n",
    "            model=model,\n",
    "            temperature=temperature,\n",
    "            batch_size=batch_size,\n",
    "            max_workers=8\n",
    "        )\n",
    "        \n",
    "        try:\n",
    "            results = run_risk_simulation(\n",
    "                participants_data=participants_data,\n",
    "                config=config,\n",
    "                output_dir=\"study_4_results/risk\"\n",
    "            )\n",
    "            \n",
    "            # Store results\n",
    "            key = f\"{model}_temp{temperature}\"\n",
    "            risk_results[key] = results\n",
    "            \n",
    "            # Check for failures\n",
    "            failed_count = sum(1 for r in results if isinstance(r, dict) and 'error' in r)\n",
    "            success_rate = ((len(results) - failed_count) / len(results)) * 100\n",
    "            \n",
    "            print(f\"Completed: {len(results)} participants, {failed_count} failed, {success_rate:.1f}% success rate\")\n",
    "            \n",
    "        except Exception as e:\n",
    "            print(f\"Error in risk simulation {model} temp {temperature}: {str(e)}\")\n",
    "            risk_results[f\"{model}_temp{temperature}\"] = {\"error\": str(e)}\n",
    "\n",
    "print(f\"\\nRisk simulations completed. Results: {list(risk_results.keys())}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Retry Failed Participants"
   ]
  },
  {
   "cell_type": "code",
   "metadata": {},
   "outputs": [],
   "source": "# Retry failed participants if needed\nprint(\"Retrying failed participants...\")\n\n# Retry moral reasoning failures\nfor key, results in moral_results.items():\n    if isinstance(results, list):\n        failed_count = sum(1 for r in results if isinstance(r, dict) and 'error' in r)\n        if failed_count > 0:\n            print(f\"Retrying {failed_count} failed participants for moral {key}\")\n            \n            model = key.split('_temp')[0]\n            temperature = float(key.split('_temp')[1])\n            \n            config = SimulationConfig(\n                model=model,\n                temperature=temperature,\n                batch_size=batch_size\n            )\n            \n            from moral_stories import get_prompt\n            updated_results = retry_failed_participants(\n                results=results,\n                participants_data=participants_data,\n                prompt_generator=get_prompt,\n                config=config,\n                personality_key=personality_key\n            )\n            \n            moral_results[key] = updated_results\n\n# Retry risk-taking failures\nfor key, results in risk_results.items():\n    if isinstance(results, list):\n        failed_count = sum(1 for r in results if isinstance(r, dict) and 'error' in r)\n        if failed_count > 0:\n            print(f\"Retrying {failed_count} failed participants for risk {key}\")\n            \n            model = key.split('_temp')[0]\n            temperature = float(key.split('_temp')[1])\n            \n            config = SimulationConfig(\n                model=model,\n                temperature=temperature,\n                batch_size=batch_size\n            )\n            \n            from risk_taking import get_prompt\n            updated_results = retry_failed_participants(\n                results=results,\n                participants_data=participants_data,\n                prompt_generator=get_prompt,\n                config=config,\n                personality_key=personality_key\n            )\n            \n            risk_results[key] = updated_results\n\nprint(\"Retry process completed\")"
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": "# Save simulation results\nresults_dir = Path(\"study_4_results\")\nresults_dir.mkdir(exist_ok=True)\n\n# Save basic summary\nimport json\nfrom datetime import datetime\n\nexperiment_summary = {\n    \"timestamp\": datetime.now().isoformat(),\n    \"study\": \"Study 4 - Behavioral Validation\",\n    \"models_tested\": models_to_test,\n    \"temperatures\": temperatures,\n    \"batch_size\": batch_size,\n    \"total_participants\": len(participants_data),\n    \"personality_key\": personality_key,\n    \"moral_results_summary\": {},\n    \"risk_results_summary\": {}\n}\n\n# Add summary statistics\nfor key, results in moral_results.items():\n    if isinstance(results, list):\n        successful = sum(1 for r in results if not (isinstance(r, dict) and 'error' in r))\n        experiment_summary[\"moral_results_summary\"][key] = {\n            \"total\": len(results),\n            \"successful\": successful,\n            \"success_rate\": (successful / len(results)) * 100\n        }\n\nfor key, results in risk_results.items():\n    if isinstance(results, list):\n        successful = sum(1 for r in results if not (isinstance(r, dict) and 'error' in r))\n        experiment_summary[\"risk_results_summary\"][key] = {\n            \"total\": len(results),\n            \"successful\": successful,\n            \"success_rate\": (successful / len(results)) * 100\n        }\n\n# Save summary\nwith open(results_dir / \"study4_experiment_summary.json\", \"w\") as f:\n    json.dump(experiment_summary, f, indent=4)\n\nprint(f\"Study 4 simulation completed. Results saved to {results_dir}/\")",
   "outputs": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}