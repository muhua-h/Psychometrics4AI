{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Multi-Model Behavioral Validation Analysis - Study 4\n",
    "\n",
    "This notebook analyzes the results from Study 4 moral reasoning and risk-taking behavioral validation across multiple LLM models.\n",
    "\n",
    "## Prerequisites\n",
    "- Run `study_4_multi_model_simulation.ipynb` first to generate simulation results\n",
    "- Results should be saved in `study_4_results/` directory\n",
    "\n",
    "## Analysis Overview\n",
    "1. **Data Loading**: Load saved simulation results and participant data\n",
    "2. **Behavioral Analysis**: Analyze moral reasoning and risk-taking patterns\n",
    "3. **Personality-Behavior Correlations**: Test personality-behavior relationships\n",
    "4. **Cross-Model Validation**: Assess consistency across different LLMs\n",
    "5. **Scenario Analysis**: Examine responses across different scenarios\n",
    "6. **Empirical Validation**: Compare with human behavioral data if available\n",
    "7. **Comprehensive Reporting**: Generate detailed analysis report\n",
    "8. **Export Results**: Save all analysis outputs for further research"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "from scipy.stats import pearsonr, spearmanr, chi2_contingency\n",
    "from scipy.cluster.hierarchy import dendrogram, linkage\n",
    "from sklearn.preprocessing import StandardScaler, LabelEncoder\n",
    "from sklearn.decomposition import PCA\n",
    "import json\n",
    "from pathlib import Path\n",
    "import warnings\n",
    "warnings.filterwarnings('ignore')\n",
    "\n",
    "# Set up plotting style\n",
    "plt.style.use('default')\n",
    "sns.set_palette(\"husl\")\n",
    "plt.rcParams['figure.figsize'] = (15, 10)\n",
    "plt.rcParams['font.size'] = 10\n",
    "\n",
    "print(\"Analysis environment setup complete\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1. Data Loading and Preparation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load simulation results\n",
    "results_dir = Path('study_4_results')\n",
    "if not results_dir.exists():\n",
    "    raise FileNotFoundError(\"Results directory not found. Please run study_4_multi_model_simulation.ipynb first.\")\n",
    "\n",
    "# Load moral reasoning results\n",
    "moral_results = {}\n",
    "moral_dir = results_dir / 'moral'\n",
    "if moral_dir.exists():\n",
    "    for json_file in moral_dir.glob('moral_*.json'):\n",
    "        filename = json_file.stem\n",
    "        parts = filename.split('_')\n",
    "        model = parts[1]\n",
    "        temp_part = parts[2]\n",
    "        temp_value = parts[3]\n",
    "        temperature = f\"{temp_part.replace('temp', '')}.{temp_value}\"\n",
    "        \n",
    "        key = f\"{model}_temp{temperature}\"\n",
    "        \n",
    "        with open(json_file, 'r') as f:\n",
    "            moral_results[key] = json.load(f)\n",
    "        \n",
    "        print(f\"Loaded moral {key}: {len(moral_results[key])} participants\")\n",
    "\n",
    "# Load risk-taking results\n",
    "risk_results = {}\n",
    "risk_dir = results_dir / 'risk'\n",
    "if risk_dir.exists():\n",
    "    for json_file in risk_dir.glob('risk_*.json'):\n",
    "        filename = json_file.stem\n",
    "        parts = filename.split('_')\n",
    "        model = parts[1]\n",
    "        temp_part = parts[2]\n",
    "        temp_value = parts[3]\n",
    "        temperature = f\"{temp_part.replace('temp', '')}.{temp_value}\"\n",
    "        \n",
    "        key = f\"{model}_temp{temperature}\"\n",
    "        \n",
    "        with open(json_file, 'r') as f:\n",
    "            risk_results[key] = json.load(f)\n",
    "        \n",
    "        print(f\"Loaded risk {key}: {len(risk_results[key])} participants\")\n",
    "\n",
    "print(f\"\\nTotal moral result sets: {len(moral_results)}\")\n",
    "print(f\"Total risk result sets: {len(risk_results)}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load participant data\n",
    "participant_data_path = Path('../../study_4/data/york_data_clean.csv')\n",
    "if participant_data_path.exists():\n",
    "    participant_data = pd.read_csv(participant_data_path)\n",
    "    print(f\"Loaded participant data: {participant_data.shape}\")\n",
    "    \n",
    "    # Extract personality columns\n",
    "    personality_columns = [col for col in participant_data.columns if 'bfi' in col.lower()]\n",
    "    print(f\"Available personality columns: {len(personality_columns)}\")\n",
    "    \n",
    "    # Look for Big Five domain scores\n",
    "    big_five_columns = []\n",
    "    for domain in ['extraversion', 'agreeableness', 'conscientiousness', 'neuroticism', 'openness']:\n",
    "        matching_cols = [col for col in participant_data.columns if domain.lower() in col.lower()]\n",
    "        if matching_cols:\n",
    "            big_five_columns.extend(matching_cols)\n",
    "    \n",
    "    print(f\"Big Five domain columns found: {big_five_columns}\")\n",
    "    \n",
    "    # Extract empirical behavioral measures if available\n",
    "    behavioral_columns = []\n",
    "    for behavior in ['moral', 'risk', 'decision', 'choice']:\n",
    "        matching_cols = [col for col in participant_data.columns if behavior.lower() in col.lower()]\n",
    "        behavioral_columns.extend(matching_cols)\n",
    "    \n",
    "    print(f\"Behavioral columns found: {behavioral_columns[:10]}...\")  # Show first 10\n",
    "    \n",
    "else:\n",
    "    print(\"Participant data not found - some analyses will be limited\")\n",
    "    participant_data = None\n",
    "    big_five_columns = []\n",
    "    behavioral_columns = []"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load experiment summary if available\n",
    "summary_path = results_dir / 'study4_experiment_summary.json'\n",
    "if summary_path.exists():\n",
    "    with open(summary_path, 'r') as f:\n",
    "        experiment_summary = json.load(f)\n",
    "    print(f\"\\nExperiment conducted: {experiment_summary.get('timestamp', 'Unknown')}\")\n",
    "    print(f\"Models tested: {experiment_summary.get('models_tested', [])}\")\n",
    "    print(f\"Total participants: {experiment_summary.get('total_participants', 'Unknown')}\")\n",
    "    print(f\"Moral scenarios: {experiment_summary.get('moral_scenarios', 'Unknown')}\")\n",
    "    print(f\"Risk scenarios: {experiment_summary.get('risk_scenarios', 'Unknown')}\")\n",
    "else:\n",
    "    print(\"Experiment summary not found\")\n",
    "    experiment_summary = {}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2. Data Processing and Structure"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def process_behavioral_results(results_dict, scenario_type):\n",
    "    \"\"\"\n",
    "    Process behavioral simulation results into structured DataFrames.\n",
    "    \n",
    "    Returns:\n",
    "    - results_df: Long format DataFrame with all responses\n",
    "    - scenario_stats: Summary statistics by scenario and model\n",
    "    \"\"\"\n",
    "    \n",
    "    # Initialize storage\n",
    "    results_list = []\n",
    "    \n",
    "    # Process each model-temperature combination\n",
    "    for model_temp, results in results_dict.items():\n",
    "        if not isinstance(results, list):\n",
    "            print(f\"Skipping {model_temp}: {results}\")\n",
    "            continue\n",
    "            \n",
    "        model_name = model_temp.split('_temp')[0]\n",
    "        temperature = model_temp.split('_temp')[1]\n",
    "        \n",
    "        # Extract responses for each participant\n",
    "        for i, result in enumerate(results):\n",
    "            if isinstance(result, dict) and 'error' not in result:\n",
    "                # Process each scenario response\n",
    "                for scenario_key, response in result.items():\n",
    "                    if scenario_key.startswith(f'{scenario_type}_scenario_'):\n",
    "                        scenario_num = scenario_key.split('_')[-1]\n",
    "                        \n",
    "                        response_dict = {\n",
    "                            'participant_id': i,\n",
    "                            'model': model_name,\n",
    "                            'temperature': temperature,\n",
    "                            'scenario_type': scenario_type,\n",
    "                            'scenario_number': scenario_num,\n",
    "                            'response': response\n",
    "                        }\n",
    "                        \n",
    "                        # Extract choice/decision if structured\n",
    "                        if isinstance(response, dict):\n",
    "                            response_dict.update({\n",
    "                                'choice': response.get('choice', response.get('decision', None)),\n",
    "                                'reasoning': response.get('reasoning', response.get('explanation', None)),\n",
    "                                'confidence': response.get('confidence', None),\n",
    "                                'raw_response': str(response)\n",
    "                            })\n",
    "                        else:\n",
    "                            # Handle string responses\n",
    "                            response_dict.update({\n",
    "                                'choice': None,\n",
    "                                'reasoning': str(response),\n",
    "                                'confidence': None,\n",
    "                                'raw_response': str(response)\n",
    "                            })\n",
    "                        \n",
    "                        results_list.append(response_dict)\n",
    "    \n",
    "    # Create DataFrame\n",
    "    results_df = pd.DataFrame(results_list)\n",
    "    \n",
    "    # Calculate scenario statistics\n",
    "    scenario_stats = []\n",
    "    if not results_df.empty:\n",
    "        stats = results_df.groupby(['model', 'temperature', 'scenario_number']).agg({\n",
    "            'participant_id': 'count',\n",
    "            'choice': 'nunique',\n",
    "            'confidence': 'mean'\n",
    "        }).rename(columns={\n",
    "            'participant_id': 'n_responses',\n",
    "            'choice': 'n_unique_choices'\n",
    "        }).reset_index()\n",
    "        \n",
    "        scenario_stats = stats\n",
    "    \n",
    "    return results_df, scenario_stats\n",
    "\n",
    "# Process both behavioral domains\n",
    "print(\"Processing moral reasoning results...\")\n",
    "moral_df, moral_stats = process_behavioral_results(moral_results, 'moral')\n",
    "\n",
    "print(\"Processing risk-taking results...\")\n",
    "risk_df, risk_stats = process_behavioral_results(risk_results, 'risk')\n",
    "\n",
    "# Combine results\n",
    "combined_behavioral_df = pd.concat([moral_df, risk_df], ignore_index=True)\n",
    "combined_stats_df = pd.concat([moral_stats, risk_stats], ignore_index=True)\n",
    "\n",
    "print(f\"\\nMoral reasoning results: {moral_df.shape}\")\n",
    "print(f\"Risk-taking results: {risk_df.shape}\")\n",
    "print(f\"Combined behavioral results: {combined_behavioral_df.shape}\")\n",
    "print(f\"Combined statistics: {combined_stats_df.shape}\")\n",
    "\n",
    "if not combined_behavioral_df.empty:\n",
    "    print(f\"\\nAvailable models: {combined_behavioral_df['model'].unique()}\")\n",
    "    print(f\"Available temperatures: {combined_behavioral_df['temperature'].unique()}\")\n",
    "    print(f\"Available scenario types: {combined_behavioral_df['scenario_type'].unique()}\")\n",
    "    print(f\"Available scenarios: {sorted(combined_behavioral_df['scenario_number'].unique())}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3. Behavioral Pattern Analysis"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def analyze_behavioral_patterns(combined_behavioral_df, combined_stats_df):\n",
    "    \"\"\"Analyze behavioral patterns across models and scenarios.\"\"\"\n",
    "    \n",
    "    if combined_behavioral_df.empty:\n",
    "        print(\"No behavioral data to analyze\")\n",
    "        return\n",
    "    \n",
    "    print(\"=== BEHAVIORAL PATTERN ANALYSIS ===\")\n",
    "    \n",
    "    # 1. Response completion rates\n",
    "    print(\"\\n1. Response Completion Rates:\")\n",
    "    completion_rates = combined_behavioral_df.groupby(['scenario_type', 'model', 'temperature']).agg({\n",
    "        'participant_id': 'count'\n",
    "    }).rename(columns={'participant_id': 'n_responses'})\n",
    "    print(completion_rates)\n",
    "    \n",
    "    # 2. Choice diversity by scenario type\n",
    "    print(\"\\n2. Choice Diversity by Scenario Type:\")\n",
    "    if 'choice' in combined_behavioral_df.columns:\n",
    "        choice_diversity = combined_behavioral_df.groupby(['scenario_type', 'model']).agg({\n",
    "            'choice': ['count', 'nunique']\n",
    "        })\n",
    "        choice_diversity.columns = ['total_responses', 'unique_choices']\n",
    "        choice_diversity['diversity_ratio'] = choice_diversity['unique_choices'] / choice_diversity['total_responses']\n",
    "        print(choice_diversity.round(3))\n",
    "    \n",
    "    # 3. Confidence patterns (if available)\n",
    "    if 'confidence' in combined_behavioral_df.columns:\n",
    "        print(\"\\n3. Confidence Patterns:\")\n",
    "        confidence_stats = combined_behavioral_df.groupby(['scenario_type', 'model'])['confidence'].agg([\n",
    "            'count', 'mean', 'std'\n",
    "        ]).round(3)\n",
    "        print(confidence_stats)\n",
    "    \n",
    "    # 4. Scenario-level analysis\n",
    "    print(\"\\n4. Scenario-Level Response Patterns:\")\n",
    "    if not combined_stats_df.empty:\n",
    "        scenario_summary = combined_stats_df.groupby(['scenario_number', 'model']).agg({\n",
    "            'n_responses': 'mean',\n",
    "            'n_unique_choices': 'mean'\n",
    "        }).round(2)\n",
    "        print(scenario_summary.head(10))  # Show first 10 scenarios\n",
    "    \n",
    "    # 5. Create visualizations\n",
    "    fig, axes = plt.subplots(2, 2, figsize=(16, 12))\n",
    "    \n",
    "    # Response rates by model and scenario type\n",
    "    if not combined_behavioral_df.empty:\n",
    "        response_counts = combined_behavioral_df.groupby(['model', 'scenario_type']).size().unstack(fill_value=0)\n",
    "        response_counts.plot(kind='bar', ax=axes[0, 0])\n",
    "        axes[0, 0].set_title('Response Counts by Model and Scenario Type')\n",
    "        axes[0, 0].set_ylabel('Number of Responses')\n",
    "        axes[0, 0].tick_params(axis='x', rotation=45)\n",
    "    \n",
    "    # Choice diversity by model\n",
    "    if 'choice' in combined_behavioral_df.columns:\n",
    "        moral_data = combined_behavioral_df[combined_behavioral_df['scenario_type'] == 'moral']\n",
    "        risk_data = combined_behavioral_df[combined_behavioral_df['scenario_type'] == 'risk']\n",
    "        \n",
    "        if not moral_data.empty:\n",
    "            moral_diversity = moral_data.groupby('model')['choice'].nunique()\n",
    "            moral_diversity.plot(kind='bar', ax=axes[0, 1], color='skyblue', alpha=0.7, label='Moral')\n",
    "        \n",
    "        if not risk_data.empty:\n",
    "            risk_diversity = risk_data.groupby('model')['choice'].nunique()\n",
    "            risk_diversity.plot(kind='bar', ax=axes[0, 1], color='lightcoral', alpha=0.7, label='Risk')\n",
    "        \n",
    "        axes[0, 1].set_title('Choice Diversity by Model')\n",
    "        axes[0, 1].set_ylabel('Number of Unique Choices')\n",
    "        axes[0, 1].legend()\n",
    "        axes[0, 1].tick_params(axis='x', rotation=45)\n",
    "    \n",
    "    # Temperature effects\n",
    "    if len(combined_behavioral_df['temperature'].unique()) > 1:\n",
    "        temp_effects = combined_behavioral_df.groupby(['temperature', 'scenario_type']).size().unstack(fill_value=0)\n",
    "        temp_effects.plot(kind='bar', ax=axes[1, 0])\n",
    "        axes[1, 0].set_title('Temperature Effects on Response Patterns')\n",
    "        axes[1, 0].set_ylabel('Number of Responses')\n",
    "        axes[1, 0].tick_params(axis='x', rotation=0)\n",
    "    \n",
    "    # Scenario difficulty (response variability)\n",
    "    if not combined_stats_df.empty and 'n_unique_choices' in combined_stats_df.columns:\n",
    "        scenario_difficulty = combined_stats_df.groupby('scenario_number')['n_unique_choices'].mean().sort_values()\n",
    "        scenario_difficulty.plot(kind='bar', ax=axes[1, 1])\n",
    "        axes[1, 1].set_title('Scenario Difficulty (Choice Variability)')\n",
    "        axes[1, 1].set_ylabel('Average Unique Choices')\n",
    "        axes[1, 1].tick_params(axis='x', rotation=45)\n",
    "    \n",
    "    plt.tight_layout()\n",
    "    plt.savefig('study_4_results/behavioral_patterns.png', dpi=300, bbox_inches='tight')\n",
    "    plt.show()\n",
    "    \n",
    "    return completion_rates, choice_diversity if 'choice' in combined_behavioral_df.columns else None\n",
    "\n",
    "# Run behavioral pattern analysis\n",
    "completion_rates, choice_diversity = analyze_behavioral_patterns(combined_behavioral_df, combined_stats_df)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 4. Personality-Behavior Correlations"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def analyze_personality_behavior_correlations(combined_behavioral_df, participant_data, big_five_columns):\n",
    "    \"\"\"Analyze correlations between personality traits and behavioral responses.\"\"\"\n",
    "    \n",
    "    if combined_behavioral_df.empty or participant_data is None or not big_five_columns:\n",
    "        print(\"Insufficient data for personality-behavior correlation analysis\")\n",
    "        return None\n",
    "    \n",
    "    print(\"=== PERSONALITY-BEHAVIOR CORRELATION ANALYSIS ===\")\n",
    "    \n",
    "    # Extract Big Five scores for participants\n",
    "    n_participants = len(combined_behavioral_df['participant_id'].unique())\n",
    "    personality_scores = participant_data[big_five_columns].iloc[:n_participants]\n",
    "    \n",
    "    print(f\"Analyzing {len(big_five_columns)} personality dimensions for {n_participants} participants\")\n",
    "    \n",
    "    correlation_results = {}\n",
    "    \n",
    "    # For each model and scenario type\n",
    "    for model in combined_behavioral_df['model'].unique():\n",
    "        for scenario_type in combined_behavioral_df['scenario_type'].unique():\n",
    "            subset = combined_behavioral_df[\n",
    "                (combined_behavioral_df['model'] == model) & \n",
    "                (combined_behavioral_df['scenario_type'] == scenario_type)\n",
    "            ]\n",
    "            \n",
    "            if subset.empty:\n",
    "                continue\n",
    "            \n",
    "            key = f\"{model}_{scenario_type}\"\n",
    "            \n",
    "            # Analyze different aspects of behavior\n",
    "            behavior_metrics = {}\n",
    "            \n",
    "            # 1. Choice consistency (if choices are available)\n",
    "            if 'choice' in subset.columns:\n",
    "                # Calculate choice consistency per participant\n",
    "                choice_consistency = subset.groupby('participant_id')['choice'].apply(\n",
    "                    lambda x: len(x.unique()) / len(x) if len(x) > 0 else 0\n",
    "                )\n",
    "                behavior_metrics['choice_diversity'] = choice_consistency\n",
    "            \n",
    "            # 2. Response length (reasoning complexity)\n",
    "            if 'reasoning' in subset.columns:\n",
    "                reasoning_length = subset.groupby('participant_id')['reasoning'].apply(\n",
    "                    lambda x: np.mean([len(str(reason)) for reason in x if pd.notna(reason)])\n",
    "                )\n",
    "                behavior_metrics['reasoning_complexity'] = reasoning_length\n",
    "            \n",
    "            # 3. Confidence levels (if available)\n",
    "            if 'confidence' in subset.columns:\n",
    "                avg_confidence = subset.groupby('participant_id')['confidence'].mean()\n",
    "                behavior_metrics['avg_confidence'] = avg_confidence\n",
    "            \n",
    "            # Calculate correlations with personality traits\n",
    "            trait_correlations = {}\n",
    "            \n",
    "            for trait in big_five_columns:\n",
    "                trait_values = personality_scores[trait]\n",
    "                \n",
    "                for behavior_name, behavior_values in behavior_metrics.items():\n",
    "                    # Align data\n",
    "                    aligned_trait = []\n",
    "                    aligned_behavior = []\n",
    "                    \n",
    "                    for participant_id in behavior_values.index:\n",
    "                        if participant_id < len(trait_values):\n",
    "                            trait_val = trait_values.iloc[participant_id]\n",
    "                            behavior_val = behavior_values.iloc[participant_id]\n",
    "                            \n",
    "                            if pd.notna(trait_val) and pd.notna(behavior_val):\n",
    "                                aligned_trait.append(trait_val)\n",
    "                                aligned_behavior.append(behavior_val)\n",
    "                    \n",
    "                    # Calculate correlation\n",
    "                    if len(aligned_trait) > 5:  # Minimum sample size\n",
    "                        corr, p_value = pearsonr(aligned_trait, aligned_behavior)\n",
    "                        if not np.isnan(corr):\n",
    "                            trait_correlations[f\"{trait}_{behavior_name}\"] = {\n",
    "                                'correlation': corr,\n",
    "                                'p_value': p_value,\n",
    "                                'n_participants': len(aligned_trait)\n",
    "                            }\n",
    "            \n",
    "            if trait_correlations:\n",
    "                correlation_results[key] = trait_correlations\n",
    "    \n",
    "    # Display results\n",
    "    if correlation_results:\n",
    "        print(\"\\nSignificant Personality-Behavior Correlations (p < 0.05):\")\n",
    "        \n",
    "        significant_results = []\n",
    "        for model_scenario, correlations in correlation_results.items():\n",
    "            for trait_behavior, stats in correlations.items():\n",
    "                if stats['p_value'] < 0.05:\n",
    "                    significant_results.append({\n",
    "                        'model_scenario': model_scenario,\n",
    "                        'trait_behavior': trait_behavior,\n",
    "                        'correlation': stats['correlation'],\n",
    "                        'p_value': stats['p_value'],\n",
    "                        'n_participants': stats['n_participants']\n",
    "                    })\n",
    "        \n",
    "        if significant_results:\n",
    "            sig_df = pd.DataFrame(significant_results)\n",
    "            sig_df = sig_df.sort_values('correlation', key=abs, ascending=False)\n",
    "            print(sig_df.head(10).round(3))\n",
    "            \n",
    "            # Create correlation heatmap\n",
    "            if len(sig_df) > 5:\n",
    "                # Pivot for heatmap\n",
    "                heatmap_data = sig_df.pivot_table(\n",
    "                    index='model_scenario', \n",
    "                    columns='trait_behavior', \n",
    "                    values='correlation'\n",
    "                )\n",
    "                \n",
    "                plt.figure(figsize=(14, 8))\n",
    "                sns.heatmap(heatmap_data, annot=True, cmap='RdBu_r', center=0, \n",
    "                           fmt='.2f', cbar_kws={'label': 'Correlation'})\n",
    "                plt.title('Significant Personality-Behavior Correlations')\n",
    "                plt.ylabel('Model_ScenarioType')\n",
    "                plt.xlabel('Trait_Behavior')\n",
    "                plt.xticks(rotation=45, ha='right')\n",
    "                plt.yticks(rotation=0)\n",
    "                plt.tight_layout()\n",
    "                plt.savefig('study_4_results/personality_behavior_correlations.png', dpi=300, bbox_inches='tight')\n",
    "                plt.show()\n",
    "        else:\n",
    "            print(\"No significant correlations found\")\n",
    "    \n",
    "    return correlation_results\n",
    "\n",
    "# Run personality-behavior correlation analysis\n",
    "personality_behavior_correlations = analyze_personality_behavior_correlations(\n",
    "    combined_behavioral_df, participant_data, big_five_columns\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 5. Cross-Model Validation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def analyze_cross_model_behavioral_consistency(combined_behavioral_df):\n",
    "    \"\"\"Assess consistency of behavioral responses across different LLMs.\"\"\"\n",
    "    \n",
    "    if combined_behavioral_df.empty:\n",
    "        print(\"No behavioral data for cross-model analysis\")\n",
    "        return\n",
    "    \n",
    "    print(\"=== CROSS-MODEL BEHAVIORAL CONSISTENCY ===\")\n",
    "    \n",
    "    # 1. Response similarity across models\n",
    "    print(\"\\n1. Response Similarity Across Models:\")\n",
    "    \n",
    "    model_similarities = {}\n",
    "    \n",
    "    for scenario_type in combined_behavioral_df['scenario_type'].unique():\n",
    "        scenario_data = combined_behavioral_df[combined_behavioral_df['scenario_type'] == scenario_type]\n",
    "        \n",
    "        models = scenario_data['model'].unique()\n",
    "        similarities = []\n",
    "        model_pairs = []\n",
    "        \n",
    "        for i, model1 in enumerate(models):\n",
    "            for j, model2 in enumerate(models):\n",
    "                if i >= j:\n",
    "                    continue\n",
    "                \n",
    "                # Get responses for same scenarios from both models\n",
    "                model1_data = scenario_data[scenario_data['model'] == model1]\n",
    "                model2_data = scenario_data[scenario_data['model'] == model2]\n",
    "                \n",
    "                if model1_data.empty or model2_data.empty:\n",
    "                    continue\n",
    "                \n",
    "                # Calculate agreement for each scenario\n",
    "                scenario_agreements = []\n",
    "                \n",
    "                for scenario_num in scenario_data['scenario_number'].unique():\n",
    "                    model1_scenario = model1_data[model1_data['scenario_number'] == scenario_num]\n",
    "                    model2_scenario = model2_data[model2_data['scenario_number'] == scenario_num]\n",
    "                    \n",
    "                    if not model1_scenario.empty and not model2_scenario.empty:\n",
    "                        # Compare choices if available\n",
    "                        if 'choice' in scenario_data.columns:\n",
    "                            # Calculate choice agreement rate\n",
    "                            common_participants = set(model1_scenario['participant_id']) & set(model2_scenario['participant_id'])\n",
    "                            \n",
    "                            if common_participants:\n",
    "                                agreements = []\n",
    "                                for participant in common_participants:\n",
    "                                    choice1 = model1_scenario[model1_scenario['participant_id'] == participant]['choice'].iloc[0]\n",
    "                                    choice2 = model2_scenario[model2_scenario['participant_id'] == participant]['choice'].iloc[0]\n",
    "                                    \n",
    "                                    if pd.notna(choice1) and pd.notna(choice2):\n",
    "                                        agreements.append(choice1 == choice2)\n",
    "                                \n",
    "                                if agreements:\n",
    "                                    agreement_rate = np.mean(agreements)\n",
    "                                    scenario_agreements.append(agreement_rate)\n",
    "                \n",
    "                if scenario_agreements:\n",
    "                    avg_agreement = np.mean(scenario_agreements)\n",
    "                    similarities.append(avg_agreement)\n",
    "                    model_pairs.append(f'{model1} vs {model2}')\n",
    "        \n",
    "        if similarities:\n",
    "            model_similarities[scenario_type] = {\n",
    "                'similarities': similarities,\n",
    "                'pairs': model_pairs,\n",
    "                'mean_similarity': np.mean(similarities),\n",
    "                'std_similarity': np.std(similarities)\n",
    "            }\n",
    "            \n",
    "            print(f\"\\n{scenario_type.upper()} Scenarios:\")\n",
    "            print(f\"  Mean inter-model agreement: {np.mean(similarities):.3f}\")\n",
    "            print(f\"  Std inter-model agreement: {np.std(similarities):.3f}\")\n",
    "            for pair, sim in zip(model_pairs, similarities):\n",
    "                print(f\"    {pair}: {sim:.3f}\")\n",
    "    \n",
    "    # 2. Temperature consistency within models\n",
    "    print(\"\\n2. Temperature Consistency Within Models:\")\n",
    "    \n",
    "    temp_consistency = {}\n",
    "    \n",
    "    for scenario_type in combined_behavioral_df['scenario_type'].unique():\n",
    "        scenario_data = combined_behavioral_df[combined_behavioral_df['scenario_type'] == scenario_type]\n",
    "        \n",
    "        for model in scenario_data['model'].unique():\n",
    "            temp0_data = scenario_data[(scenario_data['model'] == model) & \n",
    "                                     (scenario_data['temperature'] == '0.0')]\n",
    "            temp1_data = scenario_data[(scenario_data['model'] == model) & \n",
    "                                     (scenario_data['temperature'] == '1.0')]\n",
    "            \n",
    "            if not temp0_data.empty and not temp1_data.empty:\n",
    "                # Calculate consistency across scenarios\n",
    "                scenario_consistencies = []\n",
    "                \n",
    "                for scenario_num in scenario_data['scenario_number'].unique():\n",
    "                    temp0_scenario = temp0_data[temp0_data['scenario_number'] == scenario_num]\n",
    "                    temp1_scenario = temp1_data[temp1_data['scenario_number'] == scenario_num]\n",
    "                    \n",
    "                    if not temp0_scenario.empty and not temp1_scenario.empty:\n",
    "                        # Compare choices if available\n",
    "                        if 'choice' in scenario_data.columns:\n",
    "                            common_participants = set(temp0_scenario['participant_id']) & set(temp1_scenario['participant_id'])\n",
    "                            \n",
    "                            if common_participants:\n",
    "                                agreements = []\n",
    "                                for participant in common_participants:\n",
    "                                    choice0 = temp0_scenario[temp0_scenario['participant_id'] == participant]['choice'].iloc[0]\n",
    "                                    choice1 = temp1_scenario[temp1_scenario['participant_id'] == participant]['choice'].iloc[0]\n",
    "                                    \n",
    "                                    if pd.notna(choice0) and pd.notna(choice1):\n",
    "                                        agreements.append(choice0 == choice1)\n",
    "                                \n",
    "                                if agreements:\n",
    "                                    agreement_rate = np.mean(agreements)\n",
    "                                    scenario_consistencies.append(agreement_rate)\n",
    "                \n",
    "                if scenario_consistencies:\n",
    "                    temp_consistency[f\"{scenario_type}_{model}\"] = {\n",
    "                        'mean_consistency': np.mean(scenario_consistencies),\n",
    "                        'std_consistency': np.std(scenario_consistencies),\n",
    "                        'n_scenarios': len(scenario_consistencies)\n",
    "                    }\n",
    "                    \n",
    "                    print(f\"  {scenario_type}_{model}: {np.mean(scenario_consistencies):.3f} \"\n",
    "                         f\"(±{np.std(scenario_consistencies):.3f})\")\n",
    "    \n",
    "    # 3. Visualization\n",
    "    if model_similarities:\n",
    "        fig, axes = plt.subplots(1, 2, figsize=(15, 6))\n",
    "        \n",
    "        # Inter-model agreement\n",
    "        scenario_types = list(model_similarities.keys())\n",
    "        mean_sims = [model_similarities[s]['mean_similarity'] for s in scenario_types]\n",
    "        std_sims = [model_similarities[s]['std_similarity'] for s in scenario_types]\n",
    "        \n",
    "        axes[0].bar(scenario_types, mean_sims, yerr=std_sims, capsize=5)\n",
    "        axes[0].set_title('Inter-Model Agreement by Scenario Type')\n",
    "        axes[0].set_ylabel('Mean Agreement Rate')\n",
    "        axes[0].set_ylim(0, 1)\n",
    "        \n",
    "        # Temperature consistency\n",
    "        if temp_consistency:\n",
    "            models = list(temp_consistency.keys())\n",
    "            consistencies = [temp_consistency[m]['mean_consistency'] for m in models]\n",
    "            \n",
    "            axes[1].bar(range(len(models)), consistencies)\n",
    "            axes[1].set_title('Temperature Consistency')\n",
    "            axes[1].set_ylabel('Mean Agreement Rate')\n",
    "            axes[1].set_xticks(range(len(models)))\n",
    "            axes[1].set_xticklabels(models, rotation=45, ha='right')\n",
    "            axes[1].set_ylim(0, 1)\n",
    "        \n",
    "        plt.tight_layout()\n",
    "        plt.savefig('study_4_results/cross_model_behavioral_consistency.png', dpi=300, bbox_inches='tight')\n",
    "        plt.show()\n",
    "    \n",
    "    return model_similarities, temp_consistency\n",
    "\n",
    "# Run cross-model behavioral consistency analysis\n",
    "model_similarities, temp_consistency = analyze_cross_model_behavioral_consistency(combined_behavioral_df)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 6. Scenario Analysis"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def analyze_scenario_difficulty(combined_behavioral_df, combined_stats_df):\n",
    "    \"\"\"Analyze which scenarios are most challenging or show most variability.\"\"\"\n",
    "    \n",
    "    if combined_behavioral_df.empty:\n",
    "        print(\"No data for scenario analysis\")\n",
    "        return\n",
    "    \n",
    "    print(\"=== SCENARIO DIFFICULTY ANALYSIS ===\")\n",
    "    \n",
    "    # 1. Response variability by scenario\n",
    "    scenario_metrics = {}\n",
    "    \n",
    "    for scenario_type in combined_behavioral_df['scenario_type'].unique():\n",
    "        scenario_data = combined_behavioral_df[combined_behavioral_df['scenario_type'] == scenario_type]\n",
    "        \n",
    "        scenario_analysis = {}\n",
    "        \n",
    "        for scenario_num in scenario_data['scenario_number'].unique():\n",
    "            scenario_subset = scenario_data[scenario_data['scenario_number'] == scenario_num]\n",
    "            \n",
    "            if scenario_subset.empty:\n",
    "                continue\n",
    "            \n",
    "            metrics = {\n",
    "                'n_responses': len(scenario_subset),\n",
    "                'n_models': len(scenario_subset['model'].unique()),\n",
    "                'response_rate': len(scenario_subset) / (len(scenario_subset['model'].unique()) * len(scenario_subset['participant_id'].unique()))\n",
    "            }\n",
    "            \n",
    "            # Choice diversity\n",
    "            if 'choice' in scenario_subset.columns:\n",
    "                unique_choices = scenario_subset['choice'].nunique()\n",
    "                total_responses = len(scenario_subset['choice'].dropna())\n",
    "                metrics['choice_diversity'] = unique_choices / total_responses if total_responses > 0 else 0\n",
    "                metrics['n_unique_choices'] = unique_choices\n",
    "            \n",
    "            # Response length variability\n",
    "            if 'reasoning' in scenario_subset.columns:\n",
    "                response_lengths = [len(str(r)) for r in scenario_subset['reasoning'] if pd.notna(r)]\n",
    "                if response_lengths:\n",
    "                    metrics['avg_response_length'] = np.mean(response_lengths)\n",
    "                    metrics['response_length_std'] = np.std(response_lengths)\n",
    "            \n",
    "            # Confidence variability\n",
    "            if 'confidence' in scenario_subset.columns:\n",
    "                confidence_values = scenario_subset['confidence'].dropna()\n",
    "                if len(confidence_values) > 0:\n",
    "                    metrics['avg_confidence'] = confidence_values.mean()\n",
    "                    metrics['confidence_std'] = confidence_values.std()\n",
    "            \n",
    "            scenario_analysis[scenario_num] = metrics\n",
    "        \n",
    "        scenario_metrics[scenario_type] = scenario_analysis\n",
    "    \n",
    "    # 2. Identify most and least challenging scenarios\n",
    "    print(\"\\n2. Scenario Difficulty Rankings:\")\n",
    "    \n",
    "    for scenario_type, scenarios in scenario_metrics.items():\n",
    "        print(f\"\\n{scenario_type.upper()} Scenarios:\")\n",
    "        \n",
    "        # Sort by choice diversity (higher = more challenging/ambiguous)\n",
    "        if scenarios and 'choice_diversity' in next(iter(scenarios.values())):\n",
    "            diversity_ranking = sorted(scenarios.items(), \n",
    "                                     key=lambda x: x[1]['choice_diversity'], reverse=True)\n",
    "            \n",
    "            print(\"  Most Challenging (High Choice Diversity):\")\n",
    "            for scenario_num, metrics in diversity_ranking[:3]:\n",
    "                print(f\"    Scenario {scenario_num}: {metrics['choice_diversity']:.3f} diversity, \"\n",
    "                     f\"{metrics['n_unique_choices']} unique choices\")\n",
    "            \n",
    "            print(\"  Least Challenging (Low Choice Diversity):\")\n",
    "            for scenario_num, metrics in diversity_ranking[-3:]:\n",
    "                print(f\"    Scenario {scenario_num}: {metrics['choice_diversity']:.3f} diversity, \"\n",
    "                     f\"{metrics['n_unique_choices']} unique choices\")\n",
    "    \n",
    "    # 3. Visualization\n",
    "    if scenario_metrics:\n",
    "        fig, axes = plt.subplots(2, 2, figsize=(16, 12))\n",
    "        \n",
    "        for i, (scenario_type, scenarios) in enumerate(scenario_metrics.items()):\n",
    "            if i >= 2:  # Only plot first 2 scenario types\n",
    "                break\n",
    "            \n",
    "            scenario_nums = list(scenarios.keys())\n",
    "            \n",
    "            # Choice diversity\n",
    "            if scenarios and 'choice_diversity' in next(iter(scenarios.values())):\n",
    "                diversities = [scenarios[s]['choice_diversity'] for s in scenario_nums]\n",
    "                axes[i, 0].bar(scenario_nums, diversities)\n",
    "                axes[i, 0].set_title(f'{scenario_type.title()} - Choice Diversity')\n",
    "                axes[i, 0].set_ylabel('Choice Diversity Ratio')\n",
    "                axes[i, 0].set_xlabel('Scenario Number')\n",
    "            \n",
    "            # Response length\n",
    "            if scenarios and 'avg_response_length' in next(iter(scenarios.values())):\n",
    "                lengths = [scenarios[s].get('avg_response_length', 0) for s in scenario_nums]\n",
    "                axes[i, 1].bar(scenario_nums, lengths)\n",
    "                axes[i, 1].set_title(f'{scenario_type.title()} - Response Length')\n",
    "                axes[i, 1].set_ylabel('Average Response Length')\n",
    "                axes[i, 1].set_xlabel('Scenario Number')\n",
    "        \n",
    "        plt.tight_layout()\n",
    "        plt.savefig('study_4_results/scenario_analysis.png', dpi=300, bbox_inches='tight')\n",
    "        plt.show()\n",
    "    \n",
    "    return scenario_metrics\n",
    "\n",
    "# Run scenario difficulty analysis\n",
    "scenario_difficulty = analyze_scenario_difficulty(combined_behavioral_df, combined_stats_df)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 7. Comprehensive Report Generation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def generate_study4_comprehensive_report(\n",
    "    combined_behavioral_df, combined_stats_df, completion_rates, choice_diversity,\n",
    "    personality_behavior_correlations, model_similarities, temp_consistency,\n",
    "    scenario_difficulty, experiment_summary\n",
    "):\n",
    "    \"\"\"Generate a comprehensive analysis report for Study 4.\"\"\"\n",
    "    \n",
    "    report = []\n",
    "    report.append(\"# Study 4: Multi-Model Behavioral Validation Analysis Report\")\n",
    "    report.append(f\"Generated: {pd.Timestamp.now().strftime('%Y-%m-%d %H:%M:%S')}\")\n",
    "    report.append(\"\\n\" + \"=\"*80)\n",
    "    \n",
    "    # Executive Summary\n",
    "    report.append(\"\\n## Executive Summary\")\n",
    "    if experiment_summary:\n",
    "        report.append(f\"- **Study Focus**: Behavioral validation through moral reasoning and risk-taking scenarios\")\n",
    "        report.append(f\"- **Participants Analyzed**: {experiment_summary.get('total_participants', 'Unknown')}\")\n",
    "        report.append(f\"- **Models Tested**: {experiment_summary.get('models_tested', [])}\")\n",
    "        report.append(f\"- **Behavioral Domains**: Moral reasoning and risk-taking decision making\")\n",
    "        report.append(f\"- **Temperature Settings**: {experiment_summary.get('temperatures', [])}\")\n",
    "    \n",
    "    if not combined_behavioral_df.empty:\n",
    "        total_responses = len(combined_behavioral_df)\n",
    "        unique_scenarios = len(combined_behavioral_df['scenario_number'].unique())\n",
    "        report.append(f\"- **Total Behavioral Responses**: {total_responses}\")\n",
    "        report.append(f\"- **Unique Scenarios Tested**: {unique_scenarios}\")\n",
    "    \n",
    "    # Behavioral Pattern Analysis\n",
    "    report.append(\"\\n## Behavioral Pattern Analysis\")\n",
    "    \n",
    "    if completion_rates is not None:\n",
    "        report.append(\"\\n### Response Completion:\")\n",
    "        # Summarize completion rates\n",
    "        avg_completion = completion_rates.mean().iloc[0] if not completion_rates.empty else 0\n",
    "        report.append(f\"- **Average Response Rate**: {avg_completion:.1f} responses per model-scenario combination\")\n",
    "    \n",
    "    if choice_diversity is not None and not choice_diversity.empty:\n",
    "        report.append(\"\\n### Choice Diversity:\")\n",
    "        avg_diversity = choice_diversity['diversity_ratio'].mean()\n",
    "        report.append(f\"- **Average Choice Diversity**: {avg_diversity:.3f} (ratio of unique choices to total responses)\")\n",
    "        \n",
    "        # Find most and least diverse models\n",
    "        most_diverse = choice_diversity['diversity_ratio'].idxmax()[1]  # Get model name\n",
    "        least_diverse = choice_diversity['diversity_ratio'].idxmin()[1]\n",
    "        report.append(f\"- **Most Diverse Model**: {most_diverse}\")\n",
    "        report.append(f\"- **Most Consistent Model**: {least_diverse}\")\n",
    "    \n",
    "    # Personality-Behavior Relationships\n",
    "    if personality_behavior_correlations:\n",
    "        report.append(\"\\n## Personality-Behavior Relationships\")\n",
    "        \n",
    "        # Count significant correlations\n",
    "        total_correlations = 0\n",
    "        significant_correlations = 0\n",
    "        \n",
    "        for model_scenario, correlations in personality_behavior_correlations.items():\n",
    "            for trait_behavior, stats in correlations.items():\n",
    "                total_correlations += 1\n",
    "                if stats['p_value'] < 0.05:\n",
    "                    significant_correlations += 1\n",
    "        \n",
    "        if total_correlations > 0:\n",
    "            sig_rate = (significant_correlations / total_correlations) * 100\n",
    "            report.append(f\"\\n- **Significant Correlations Found**: {significant_correlations}/{total_correlations} ({sig_rate:.1f}%)\")\n",
    "            \n",
    "            # Find strongest correlations\n",
    "            strongest_corrs = []\n",
    "            for model_scenario, correlations in personality_behavior_correlations.items():\n",
    "                for trait_behavior, stats in correlations.items():\n",
    "                    if stats['p_value'] < 0.05:\n",
    "                        strongest_corrs.append((trait_behavior, stats['correlation'], model_scenario))\n",
    "            \n",
    "            if strongest_corrs:\n",
    "                strongest_corrs.sort(key=lambda x: abs(x[1]), reverse=True)\n",
    "                report.append(\"\\n### Strongest Personality-Behavior Relationships:\")\n",
    "                for trait_behavior, corr, model_scenario in strongest_corrs[:5]:\n",
    "                    report.append(f\"- **{trait_behavior}** in {model_scenario}: r = {corr:.3f}\")\n",
    "    \n",
    "    # Cross-Model Consistency\n",
    "    report.append(\"\\n## Cross-Model Consistency\")\n",
    "    \n",
    "    if model_similarities:\n",
    "        report.append(\"\\n### Inter-Model Agreement:\")\n",
    "        for scenario_type, stats in model_similarities.items():\n",
    "            mean_agreement = stats['mean_similarity']\n",
    "            std_agreement = stats['std_similarity']\n",
    "            report.append(f\"- **{scenario_type.title()} Scenarios**: {mean_agreement:.3f} (±{std_agreement:.3f}) agreement rate\")\n",
    "    \n",
    "    if temp_consistency:\n",
    "        report.append(\"\\n### Temperature Consistency:\")\n",
    "        temp_means = [stats['mean_consistency'] for stats in temp_consistency.values()]\n",
    "        if temp_means:\n",
    "            overall_temp_consistency = np.mean(temp_means)\n",
    "            report.append(f\"- **Overall Temperature Consistency**: {overall_temp_consistency:.3f}\")\n",
    "            \n",
    "            # Most and least consistent models\n",
    "            best_temp_model = max(temp_consistency.keys(), key=lambda x: temp_consistency[x]['mean_consistency'])\n",
    "            worst_temp_model = min(temp_consistency.keys(), key=lambda x: temp_consistency[x]['mean_consistency'])\n",
    "            report.append(f\"- **Most Temperature-Consistent**: {best_temp_model}\")\n",
    "            report.append(f\"- **Least Temperature-Consistent**: {worst_temp_model}\")\n",
    "    \n",
    "    # Scenario Analysis\n",
    "    if scenario_difficulty:\n",
    "        report.append(\"\\n## Scenario Analysis\")\n",
    "        \n",
    "        for scenario_type, scenarios in scenario_difficulty.items():\n",
    "            if scenarios:\n",
    "                report.append(f\"\\n### {scenario_type.title()} Scenarios:\")\n",
    "                \n",
    "                # Find most challenging scenarios\n",
    "                if 'choice_diversity' in next(iter(scenarios.values())):\n",
    "                    diversity_ranking = sorted(scenarios.items(), \n",
    "                                             key=lambda x: x[1]['choice_diversity'], reverse=True)\n",
    "                    \n",
    "                    most_challenging = diversity_ranking[0]\n",
    "                    least_challenging = diversity_ranking[-1]\n",
    "                    \n",
    "                    report.append(f\"- **Most Challenging**: Scenario {most_challenging[0]} \"\n",
    "                                f\"({most_challenging[1]['choice_diversity']:.3f} diversity)\")\n",
    "                    report.append(f\"- **Least Challenging**: Scenario {least_challenging[0]} \"\n",
    "                                f\"({least_challenging[1]['choice_diversity']:.3f} diversity)\")\n",
    "    \n",
    "    # Key Findings\n",
    "    report.append(\"\\n## Key Findings\")\n",
    "    \n",
    "    findings = []\n",
    "    \n",
    "    # Model performance\n",
    "    if model_similarities:\n",
    "        all_agreements = [stats['mean_similarity'] for stats in model_similarities.values()]\n",
    "        if all_agreements:\n",
    "            overall_agreement = np.mean(all_agreements)\n",
    "            findings.append(f\"**Model Consistency**: Average inter-model agreement of {overall_agreement:.3f}\")\n",
    "    \n",
    "    # Behavioral patterns\n",
    "    if choice_diversity is not None and not choice_diversity.empty:\n",
    "        avg_diversity = choice_diversity['diversity_ratio'].mean()\n",
    "        findings.append(f\"**Behavioral Diversity**: Average choice diversity of {avg_diversity:.3f} across all scenarios\")\n",
    "    \n",
    "    # Personality relationships\n",
    "    if personality_behavior_correlations:\n",
    "        sig_count = sum(1 for correlations in personality_behavior_correlations.values() \n",
    "                       for stats in correlations.values() if stats['p_value'] < 0.05)\n",
    "        if sig_count > 0:\n",
    "            findings.append(f\"**Personality Effects**: {sig_count} significant personality-behavior correlations identified\")\n",
    "    \n",
    "    for finding in findings:\n",
    "        report.append(f\"\\n- {finding}\")\n",
    "    \n",
    "    # Recommendations\n",
    "    report.append(\"\\n## Recommendations\")\n",
    "    \n",
    "    recommendations = [\n",
    "        \"**Behavioral Assessment**: Use scenarios with moderate choice diversity for reliable personality assessment\",\n",
    "        \"**Model Selection**: Consider ensemble approaches for more robust behavioral predictions\",\n",
    "        \"**Temperature Settings**: Use temperature = 0.0 for consistent behavioral assessment applications\",\n",
    "        \"**Scenario Design**: Focus on scenarios that show consistent personality-behavior relationships\",\n",
    "        \"**Future Research**: Investigate cultural and contextual factors in moral and risk-taking behaviors\"\n",
    "    ]\n",
    "    \n",
    "    for rec in recommendations:\n",
    "        report.append(f\"\\n- {rec}\")\n",
    "    \n",
    "    # Technical Notes\n",
    "    report.append(\"\\n## Technical Notes\")\n",
    "    technical_notes = [\n",
    "        \"Behavioral analysis based on moral reasoning and risk-taking scenarios\",\n",
    "        \"Agreement rates calculated using choice overlap between models\",\n",
    "        \"Correlations computed using Pearson's correlation coefficient\",\n",
    "        \"Statistical significance testing performed where applicable\",\n",
    "        \"Choice diversity calculated as ratio of unique choices to total responses\"\n",
    "    ]\n",
    "    \n",
    "    for note in technical_notes:\n",
    "        report.append(f\"- {note}\")\n",
    "    \n",
    "    return \"\\n\".join(report)\n",
    "\n",
    "# Generate comprehensive report\n",
    "study4_report = generate_study4_comprehensive_report(\n",
    "    combined_behavioral_df, combined_stats_df, completion_rates, choice_diversity,\n",
    "    personality_behavior_correlations, model_similarities, temp_consistency,\n",
    "    scenario_difficulty, experiment_summary\n",
    ")\n",
    "\n",
    "print(study4_report)\n",
    "\n",
    "# Save report to file\n",
    "with open('study_4_results/comprehensive_analysis_report.md', 'w') as f:\n",
    "    f.write(study4_report)\n",
    "\n",
    "print(\"\\n\" + \"=\"*80)\n",
    "print(\"STUDY 4 ANALYSIS COMPLETE\")\n",
    "print(\"Report saved to: study_4_results/comprehensive_analysis_report.md\")\n",
    "print(\"=\"*80)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 8. Export Results for Further Analysis"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def export_study4_results(\n",
    "    combined_behavioral_df, combined_stats_df, completion_rates, choice_diversity,\n",
    "    personality_behavior_correlations, scenario_difficulty\n",
    "):\n",
    "    \"\"\"Export all Study 4 analysis results in formats suitable for further research.\"\"\"\n",
    "    \n",
    "    output_dir = Path('study_4_results')\n",
    "    output_dir.mkdir(exist_ok=True)\n",
    "    \n",
    "    print(\"Exporting Study 4 analysis results...\")\n",
    "    \n",
    "    # 1. Export main behavioral results\n",
    "    if not combined_behavioral_df.empty:\n",
    "        combined_behavioral_df.to_csv(output_dir / 'behavioral_responses_combined.csv', index=False)\n",
    "        print(f\"✓ Combined behavioral responses: {len(combined_behavioral_df)} rows\")\n",
    "        \n",
    "        # Separate by scenario type\n",
    "        for scenario_type in combined_behavioral_df['scenario_type'].unique():\n",
    "            scenario_data = combined_behavioral_df[combined_behavioral_df['scenario_type'] == scenario_type]\n",
    "            scenario_data.to_csv(output_dir / f'behavioral_responses_{scenario_type}.csv', index=False)\n",
    "            print(f\"✓ {scenario_type.title()} responses: {len(scenario_data)} rows\")\n",
    "    \n",
    "    if not combined_stats_df.empty:\n",
    "        combined_stats_df.to_csv(output_dir / 'scenario_statistics.csv', index=False)\n",
    "        print(f\"✓ Scenario statistics: {len(combined_stats_df)} rows\")\n",
    "    \n",
    "    # 2. Export completion rates\n",
    "    if completion_rates is not None and not completion_rates.empty:\n",
    "        completion_rates.to_csv(output_dir / 'response_completion_rates.csv')\n",
    "        print(f\"✓ Completion rates: {completion_rates.shape}\")\n",
    "    \n",
    "    # 3. Export choice diversity metrics\n",
    "    if choice_diversity is not None and not choice_diversity.empty:\n",
    "        choice_diversity.to_csv(output_dir / 'choice_diversity_metrics.csv')\n",
    "        print(f\"✓ Choice diversity metrics: {choice_diversity.shape}\")\n",
    "    \n",
    "    # 4. Export personality-behavior correlations\n",
    "    if personality_behavior_correlations:\n",
    "        correlation_flat = []\n",
    "        for model_scenario, correlations in personality_behavior_correlations.items():\n",
    "            for trait_behavior, stats in correlations.items():\n",
    "                parts = model_scenario.split('_')\n",
    "                model = parts[0]\n",
    "                scenario_type = parts[1]\n",
    "                \n",
    "                trait_parts = trait_behavior.split('_')\n",
    "                trait = '_'.join(trait_parts[:-1])\n",
    "                behavior = trait_parts[-1]\n",
    "                \n",
    "                correlation_flat.append({\n",
    "                    'model': model,\n",
    "                    'scenario_type': scenario_type,\n",
    "                    'personality_trait': trait,\n",
    "                    'behavioral_measure': behavior,\n",
    "                    'correlation': stats['correlation'],\n",
    "                    'p_value': stats['p_value'],\n",
    "                    'n_participants': stats['n_participants'],\n",
    "                    'significant': stats['p_value'] < 0.05\n",
    "                })\n",
    "        \n",
    "        correlation_df = pd.DataFrame(correlation_flat)\n",
    "        correlation_df.to_csv(output_dir / 'personality_behavior_correlations.csv', index=False)\n",
    "        print(f\"✓ Personality-behavior correlations: {len(correlation_df)} rows\")\n",
    "    \n",
    "    # 5. Export scenario difficulty analysis\n",
    "    if scenario_difficulty:\n",
    "        scenario_flat = []\n",
    "        for scenario_type, scenarios in scenario_difficulty.items():\n",
    "            for scenario_num, metrics in scenarios.items():\n",
    "                record = {\n",
    "                    'scenario_type': scenario_type,\n",
    "                    'scenario_number': scenario_num\n",
    "                }\n",
    "                record.update(metrics)\n",
    "                scenario_flat.append(record)\n",
    "        \n",
    "        scenario_df = pd.DataFrame(scenario_flat)\n",
    "        scenario_df.to_csv(output_dir / 'scenario_difficulty_metrics.csv', index=False)\n",
    "        print(f\"✓ Scenario difficulty metrics: {len(scenario_df)} rows\")\n",
    "    \n",
    "    # 6. Create analysis summary\n",
    "    summary_stats = {\n",
    "        'analysis_timestamp': pd.Timestamp.now().isoformat(),\n",
    "        'study': 'Study 4 - Behavioral Validation',\n",
    "        'n_participants': len(combined_behavioral_df['participant_id'].unique()) if not combined_behavioral_df.empty else 0,\n",
    "        'n_models': len(combined_behavioral_df['model'].unique()) if not combined_behavioral_df.empty else 0,\n",
    "        'n_temperatures': len(combined_behavioral_df['temperature'].unique()) if not combined_behavioral_df.empty else 0,\n",
    "        'n_scenario_types': len(combined_behavioral_df['scenario_type'].unique()) if not combined_behavioral_df.empty else 0,\n",
    "        'n_total_scenarios': len(combined_behavioral_df['scenario_number'].unique()) if not combined_behavioral_df.empty else 0,\n",
    "        'n_total_responses': len(combined_behavioral_df) if not combined_behavioral_df.empty else 0\n",
    "    }\n",
    "    \n",
    "    with open(output_dir / 'analysis_metadata.json', 'w') as f:\n",
    "        json.dump(summary_stats, f, indent=2)\n",
    "    print(f\"✓ Analysis metadata saved\")\n",
    "    \n",
    "    # 7. Create R-ready format\n",
    "    if not combined_behavioral_df.empty:\n",
    "        # Long format for R analysis\n",
    "        r_format = combined_behavioral_df.copy()\n",
    "        r_format.to_csv(output_dir / 'behavioral_responses_r_format.csv', index=False)\n",
    "        print(f\"✓ R-ready format: {len(r_format)} rows\")\n",
    "    \n",
    "    print(f\"\\n📁 All Study 4 results exported to: {output_dir}/\")\n",
    "    print(\"Files available for further analysis:\")\n",
    "    for file in sorted(output_dir.glob('*.csv')):\n",
    "        print(f\"  - {file.name}\")\n",
    "    for file in sorted(output_dir.glob('*.json')):\n",
    "        print(f\"  - {file.name}\")\n",
    "    for file in sorted(output_dir.glob('*.png')):\n",
    "        print(f\"  - {file.name}\")\n",
    "\n",
    "# Export all results\n",
    "export_study4_results(\n",
    "    combined_behavioral_df, combined_stats_df, completion_rates, choice_diversity,\n",
    "    personality_behavior_correlations, scenario_difficulty\n",
    ")\n",
    "\n",
    "print(\"\\n🎉 STUDY 4 MULTI-MODEL ANALYSIS COMPLETE! 🎉\")\n",
    "print(\"\\nNext steps:\")\n",
    "print(\"1. Review the comprehensive report: study_4_results/comprehensive_analysis_report.md\")\n",
    "print(\"2. Examine visualizations in study_4_results/\")\n",
    "print(\"3. Use exported CSV files for statistical analysis in R or Python\")\n",
    "print(\"4. Compare with Studies 2 and 3 using similar analysis frameworks\")\n",
    "print(\"5. Conduct advanced behavioral modeling and personality-behavior relationship analysis\")"
   ]
  }
 ],
 "metadata": {
  "kernelnel": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}