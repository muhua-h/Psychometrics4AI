{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Multi-Model Facet-Level Personality Simulation - Study 3\n",
    "\n",
    "This notebook refactors the original Study 3 facet-level parameter extraction and simulation to work with multiple LLM models using the unified portal.py interface.\n",
    "\n",
    "## Models to Test\n",
    "- GPT-4\n",
    "- GPT-4o  \n",
    "- Llama-3.3-70B-Instruct\n",
    "- DeepSeek-V3\n",
    "\n",
    "## Study 3 Components\n",
    "1. **Facet-Level Parameter Extraction**: Extract personality parameters at the facet level\n",
    "2. **BFI-2 to Mini-Marker Simulation**: Similar to Study 2 but with facet-level focus\n",
    "3. **Statistical Modeling**: Advanced factor analysis and parameter validation\n",
    "\n",
    "## Data Flow\n",
    "1. Load and preprocess BFI-2 data\n",
    "2. Extract facet-level personality parameters\n",
    "3. Run simulations across multiple models\n",
    "4. Validate personality structure with factor analysis"
   ]
  },
  {
   "cell_type": "code",
   "metadata": {},
   "outputs": [],
   "source": "import pandas as pd\nimport numpy as np\nimport sys\nfrom pathlib import Path\n\n# Add shared modules to path\nsys.path.append('../shared')\n\nfrom simulation_utils import (\n    SimulationConfig, \n    run_bfi_to_minimarker_simulation,\n    run_batch_simulation,\n    retry_failed_participants,\n    save_simulation_results\n)\nfrom schema_bfi2 import expanded_scale\nfrom mini_marker_prompt import get_prompt",
   "execution_count": null
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Data Loading and Preprocessing"
   ]
  },
  {
   "cell_type": "code",
   "metadata": {},
   "outputs": [],
   "source": "# Load the BFI-2 dataset (same as Study 2)\ndata_path = Path('../../raw_data/Soto_data.xlsx')\nif not data_path.exists():\n    print(f\"Data file not found at {data_path}\")\n    print(\"Please ensure the raw_data/Soto_data.xlsx file exists in the project root\")\n    raise FileNotFoundError(f\"Data file not found: {data_path}\")\n\ndata = pd.read_excel(data_path, sheet_name='data')\nprint(f\"Loaded data shape: {data.shape}\")\n\n# Generate column names\ntda_columns = [f\"tda{i}\" for i in range(1, 41)]\nsbfi_columns = [f\"bfi{i}\" for i in range(1, 61)]\nselected_columns = tda_columns + sbfi_columns\n\n# Clean data\nprint(f\"Original data shape: {data.shape}\")\ndata = data.dropna(subset=selected_columns)\nprint(f\"Data shape after removing missing values: {data.shape}\")",
   "execution_count": null
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Apply reverse coding (same as Study 2)\n",
    "reverse_coding_map = {\n",
    "    'bfi1': 'bfi1', 'bfi2': 'bfi2', 'bfi3': 'bfi3R', 'bfi4': 'bfi4R', 'bfi5': 'bfi5R',\n",
    "    'bfi6': 'bfi6', 'bfi7': 'bfi7', 'bfi8': 'bfi8R', 'bfi9': 'bfi9R', 'bfi10': 'bfi10',\n",
    "    'bfi11': 'bfi11R', 'bfi12': 'bfi12R', 'bfi13': 'bfi13', 'bfi14': 'bfi14', 'bfi15': 'bfi15',\n",
    "    'bfi16': 'bfi16R', 'bfi17': 'bfi17R', 'bfi18': 'bfi18', 'bfi19': 'bfi19', 'bfi20': 'bfi20',\n",
    "    'bfi21': 'bfi21', 'bfi22': 'bfi22R', 'bfi23': 'bfi23R', 'bfi24': 'bfi24R', 'bfi25': 'bfi25R',\n",
    "    'bfi26': 'bfi26R', 'bfi27': 'bfi27', 'bfi28': 'bfi28R', 'bfi29': 'bfi29R', 'bfi30': 'bfi30R',\n",
    "    'bfi31': 'bfi31R', 'bfi32': 'bfi32', 'bfi33': 'bfi33', 'bfi34': 'bfi34', 'bfi35': 'bfi35',\n",
    "    'bfi36': 'bfi36R', 'bfi37': 'bfi37R', 'bfi38': 'bfi38', 'bfi39': 'bfi39', 'bfi40': 'bfi40',\n",
    "    'bfi41': 'bfi41', 'bfi42': 'bfi42R', 'bfi43': 'bfi43', 'bfi44': 'bfi44R', 'bfi45': 'bfi45R',\n",
    "    'bfi46': 'bfi46', 'bfi47': 'bfi47R', 'bfi48': 'bfi48R', 'bfi49': 'bfi49R', 'bfi50': 'bfi50R',\n",
    "    'bfi51': 'bfi51R', 'bfi52': 'bfi52', 'bfi53': 'bfi53', 'bfi54': 'bfi54', 'bfi55': 'bfi55R',\n",
    "    'bfi56': 'bfi56', 'bfi57': 'bfi57', 'bfi58': 'bfi58R', 'bfi59': 'bfi59', 'bfi60': 'bfi60'\n",
    "}\n",
    "\n",
    "for key, value in reverse_coding_map.items():\n",
    "    if value.endswith('R'):\n",
    "        data[key] = 6 - data[key]\n",
    "\n",
    "print(\"Reverse coding applied successfully\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Facet-Level Parameter Extraction\n",
    "\n",
    "Study 3 focuses on extracting and analyzing personality at the facet level rather than just the domain level."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# BFI-2 Facet Structure (12 facets across 5 domains)\n",
    "# Based on the BFI-2 structure from Soto & John (2017)\n",
    "\n",
    "facet_structure = {\n",
    "    # Extraversion facets\n",
    "    'Sociability': ['bfi1', 'bfi6', 'bfi11', 'bfi16', 'bfi21', 'bfi26'],\n",
    "    'Assertiveness': ['bfi31', 'bfi36', 'bfi41', 'bfi46', 'bfi51', 'bfi56'],\n",
    "    \n",
    "    # Agreeableness facets  \n",
    "    'Compassion': ['bfi2', 'bfi7', 'bfi12', 'bfi17', 'bfi22', 'bfi27'],\n",
    "    'Respectfulness': ['bfi32', 'bfi37', 'bfi42', 'bfi47', 'bfi52', 'bfi57'],\n",
    "    \n",
    "    # Conscientiousness facets\n",
    "    'Organization': ['bfi3', 'bfi8', 'bfi13', 'bfi18', 'bfi23', 'bfi28'],\n",
    "    'Productiveness': ['bfi33', 'bfi38', 'bfi43', 'bfi48', 'bfi53', 'bfi58'],\n",
    "    \n",
    "    # Negative Emotionality facets\n",
    "    'Anxiety': ['bfi4', 'bfi9', 'bfi14', 'bfi19', 'bfi24', 'bfi29'],\n",
    "    'Depression': ['bfi34', 'bfi39', 'bfi44', 'bfi49', 'bfi54', 'bfi59'],\n",
    "    \n",
    "    # Open-Mindedness facets\n",
    "    'Intellectual_Curiosity': ['bfi5', 'bfi10', 'bfi15', 'bfi20', 'bfi25', 'bfi30'],\n",
    "    'Aesthetic_Sensitivity': ['bfi35', 'bfi40', 'bfi45', 'bfi50', 'bfi55', 'bfi60']\n",
    "}\n",
    "\n",
    "print(f\"Facet structure defined with {len(facet_structure)} facets\")\n",
    "for facet, items in facet_structure.items():\n",
    "    print(f\"{facet}: {len(items)} items\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Calculate facet-level scores\n",
    "facet_scores = pd.DataFrame(index=data.index)\n",
    "\n",
    "for facet_name, items in facet_structure.items():\n",
    "    # Calculate mean score for each facet\n",
    "    facet_scores[facet_name] = data[items].mean(axis=1)\n",
    "    \n",
    "# Calculate domain-level scores\n",
    "domain_scores = pd.DataFrame(index=data.index)\n",
    "domain_scores['Extraversion'] = facet_scores[['Sociability', 'Assertiveness']].mean(axis=1)\n",
    "domain_scores['Agreeableness'] = facet_scores[['Compassion', 'Respectfulness']].mean(axis=1)\n",
    "domain_scores['Conscientiousness'] = facet_scores[['Organization', 'Productiveness']].mean(axis=1)\n",
    "domain_scores['Negative_Emotionality'] = facet_scores[['Anxiety', 'Depression']].mean(axis=1)\n",
    "domain_scores['Open_Mindedness'] = facet_scores[['Intellectual_Curiosity', 'Aesthetic_Sensitivity']].mean(axis=1)\n",
    "\n",
    "print(\"Facet and domain scores calculated\")\n",
    "print(f\"Facet scores shape: {facet_scores.shape}\")\n",
    "print(f\"Domain scores shape: {domain_scores.shape}\")\n",
    "\n",
    "# Combine with original data\n",
    "data_with_facets = pd.concat([data, facet_scores, domain_scores], axis=1)\n",
    "print(f\"Combined data shape: {data_with_facets.shape}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Create Expanded Format Personality Descriptions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Map numeric values to expanded format descriptions (same as Study 2)\n",
    "def map_values(row):\n",
    "    mapped_row = row.copy()\n",
    "    for key in expanded_scale:\n",
    "        if pd.notna(row[key]):\n",
    "            index = int(row[key]) - 1\n",
    "            mapped_row[key] = expanded_scale[key][index]\n",
    "    return mapped_row\n",
    "\n",
    "# Apply mapping to BFI columns\n",
    "mapped_data = data_with_facets[sbfi_columns].apply(map_values, axis=1)\n",
    "\n",
    "# Create combined BFI-2 description\n",
    "mapped_data['combined_bfi2'] = mapped_data[[f'bfi{i}' for i in range(1, 61)]].apply(\n",
    "    lambda row: ' '.join(row), axis=1\n",
    ")\n",
    "\n",
    "# Add to main dataset\n",
    "data_with_facets['combined_bfi2'] = mapped_data['combined_bfi2']\n",
    "\n",
    "print(\"Expanded format personality descriptions created\")\n",
    "print(f\"Sample description length: {len(data_with_facets.iloc[0]['combined_bfi2'])} characters\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Multi-Model Simulation Configuration"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Configuration for different models and temperatures\n",
    "models_to_test = ['gpt-4', 'gpt-4o', 'llama', 'deepseek']\n",
    "temperatures = [0.0, 1.0]  # Test both deterministic and stochastic responses\n",
    "batch_size = 20  # Conservative batch size for stability\n",
    "\n",
    "# Prepare participant data\n",
    "participants_data = data_with_facets.to_dict('records')\n",
    "print(f\"Prepared {len(participants_data)} participants for simulation\")\n",
    "\n",
    "# Results storage\n",
    "expanded_format_results = {}\n",
    "likert_format_results = {}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Expanded Format Simulations (BFI-2 to Mini-Marker)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"Starting Expanded Format BFI-2 to Mini-Marker Simulations\")\n",
    "print(\"=\" * 60)\n",
    "\n",
    "for model in models_to_test:\n",
    "    for temperature in temperatures:\n",
    "        print(f\"\\nRunning expanded format simulation: {model} with temperature {temperature}\")\n",
    "        \n",
    "        config = SimulationConfig(\n",
    "            model=model,\n",
    "            temperature=temperature,\n",
    "            batch_size=batch_size,\n",
    "            max_workers=8\n",
    "        )\n",
    "        \n",
    "        try:\n",
    "            results = run_bfi_to_minimarker_simulation(\n",
    "                participants_data=participants_data,\n",
    "                config=config,\n",
    "                output_dir=\"study_3_results/expanded_format\"\n",
    "            )\n",
    "            \n",
    "            # Store results\n",
    "            key = f\"{model}_temp{temperature}\"\n",
    "            expanded_format_results[key] = results\n",
    "            \n",
    "            # Check for failures\n",
    "            failed_count = sum(1 for r in results if isinstance(r, dict) and 'error' in r)\n",
    "            success_rate = ((len(results) - failed_count) / len(results)) * 100\n",
    "            \n",
    "            print(f\"Completed: {len(results)} participants, {failed_count} failed, {success_rate:.1f}% success rate\")\n",
    "            \n",
    "        except Exception as e:\n",
    "            print(f\"Error in expanded format simulation {model} temp {temperature}: {str(e)}\")\n",
    "            expanded_format_results[f\"{model}_temp{temperature}\"] = {\"error\": str(e)}\n",
    "\n",
    "print(f\"\\nExpanded format simulations completed. Results: {list(expanded_format_results.keys())}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Likert Format Simulations (Alternative Format)\n",
    "\n",
    "Study 3 also tests a more traditional Likert-scale format for comparison."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create Likert-style personality descriptions using raw scores\n",
    "def create_likert_description(row):\n",
    "    \"\"\"Create a personality description using Likert-style language.\"\"\"\n",
    "    descriptions = []\n",
    "    \n",
    "    # Use domain scores for more concise descriptions\n",
    "    domains = {\n",
    "        'Extraversion': 'outgoing and sociable',\n",
    "        'Agreeableness': 'compassionate and cooperative', \n",
    "        'Conscientiousness': 'organized and responsible',\n",
    "        'Negative_Emotionality': 'emotionally reactive',\n",
    "        'Open_Mindedness': 'curious and creative'\n",
    "    }\n",
    "    \n",
    "    for domain, description in domains.items():\n",
    "        score = row[domain]\n",
    "        if score >= 4.5:\n",
    "            level = \"very\"\n",
    "        elif score >= 3.5:\n",
    "            level = \"quite\"\n",
    "        elif score >= 2.5:\n",
    "            level = \"moderately\"\n",
    "        elif score >= 1.5:\n",
    "            level = \"somewhat\"\n",
    "        else:\n",
    "            level = \"not very\"\n",
    "            \n",
    "        descriptions.append(f\"I am {level} {description}\")\n",
    "    \n",
    "    return '. '.join(descriptions) + '.'\n",
    "\n",
    "# Add Likert-style descriptions\n",
    "data_with_facets['likert_description'] = data_with_facets.apply(create_likert_description, axis=1)\n",
    "\n",
    "print(\"Likert-style personality descriptions created\")\n",
    "print(\"Sample description:\")\n",
    "print(data_with_facets.iloc[0]['likert_description'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Update participants data with Likert descriptions\n",
    "participants_data_likert = data_with_facets.to_dict('records')\n",
    "\n",
    "print(\"Starting Likert Format BFI-2 to Mini-Marker Simulations\")\n",
    "print(\"=\" * 60)\n",
    "\n",
    "# Create a custom prompt generator for Likert format\n",
    "def likert_prompt_generator(personality_description):\n",
    "    \"\"\"Generate prompts using Likert-style personality descriptions.\"\"\"\n",
    "    from mini_marker_prompt import get_prompt\n",
    "    return get_prompt(personality_description)\n",
    "\n",
    "for model in models_to_test:\n",
    "    for temperature in temperatures:\n",
    "        print(f\"\\nRunning Likert format simulation: {model} with temperature {temperature}\")\n",
    "        \n",
    "        config = SimulationConfig(\n",
    "            model=model,\n",
    "            temperature=temperature,\n",
    "            batch_size=batch_size,\n",
    "            max_workers=8\n",
    "        )\n",
    "        \n",
    "        try:\n",
    "            results = run_batch_simulation(\n",
    "                participants_data=participants_data_likert,\n",
    "                prompt_generator=likert_prompt_generator,\n",
    "                config=config,\n",
    "                personality_key='likert_description',\n",
    "                output_dir=\"study_3_results/likert_format\",\n",
    "                output_filename=\"bfi_to_minimarker_likert\"\n",
    "            )\n",
    "            \n",
    "            # Store results\n",
    "            key = f\"{model}_temp{temperature}\"\n",
    "            likert_format_results[key] = results\n",
    "            \n",
    "            # Check for failures\n",
    "            failed_count = sum(1 for r in results if isinstance(r, dict) and 'error' in r)\n",
    "            success_rate = ((len(results) - failed_count) / len(results)) * 100\n",
    "            \n",
    "            print(f\"Completed: {len(results)} participants, {failed_count} failed, {success_rate:.1f}% success rate\")\n",
    "            \n",
    "        except Exception as e:\n",
    "            print(f\"Error in Likert format simulation {model} temp {temperature}: {str(e)}\")\n",
    "            likert_format_results[f\"{model}_temp{temperature}\"] = {\"error\": str(e)}\n",
    "\n",
    "print(f\"\\nLikert format simulations completed. Results: {list(likert_format_results.keys())}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Retry Failed Participants"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Retry failed expanded format participants\n",
    "print(\"Retrying failed expanded format participants...\")\n",
    "\n",
    "for key, results in expanded_format_results.items():\n",
    "    if isinstance(results, list):\n",
    "        failed_count = sum(1 for r in results if isinstance(r, dict) and 'error' in r)\n",
    "        if failed_count > 0:\n",
    "            print(f\"Retrying {failed_count} failed participants for expanded {key}\")\n",
    "            \n",
    "            model = key.split('_temp')[0]\n",
    "            temperature = float(key.split('_temp')[1])\n",
    "            \n",
    "            config = SimulationConfig(\n",
    "                model=model,\n",
    "                temperature=temperature,\n",
    "                batch_size=batch_size\n",
    "            )\n",
    "            \n",
    "            from mini_marker_prompt import get_prompt\n",
    "            updated_results = retry_failed_participants(\n",
    "                results=results,\n",
    "                participants_data=participants_data,\n",
    "                prompt_generator=get_prompt,\n",
    "                config=config,\n",
    "                personality_key='combined_bfi2'\n",
    "            )\n",
    "            \n",
    "            expanded_format_results[key] = updated_results\n",
    "            save_simulation_results(updated_results, \"study_3_results/expanded_format\", \"bfi_to_minimarker_retried\", config)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Retry failed Likert format participants\n",
    "print(\"Retrying failed Likert format participants...\")\n",
    "\n",
    "for key, results in likert_format_results.items():\n",
    "    if isinstance(results, list):\n",
    "        failed_count = sum(1 for r in results if isinstance(r, dict) and 'error' in r)\n",
    "        if failed_count > 0:\n",
    "            print(f\"Retrying {failed_count} failed participants for Likert {key}\")\n",
    "            \n",
    "            model = key.split('_temp')[0]\n",
    "            temperature = float(key.split('_temp')[1])\n",
    "            \n",
    "            config = SimulationConfig(\n",
    "                model=model,\n",
    "                temperature=temperature,\n",
    "                batch_size=batch_size\n",
    "            )\n",
    "            \n",
    "            updated_results = retry_failed_participants(\n",
    "                results=results,\n",
    "                participants_data=participants_data_likert,\n",
    "                prompt_generator=likert_prompt_generator,\n",
    "                config=config,\n",
    "                personality_key='likert_description'\n",
    "            )\n",
    "            \n",
    "            likert_format_results[key] = updated_results\n",
    "            save_simulation_results(updated_results, \"study_3_results/likert_format\", \"bfi_to_minimarker_likert_retried\", config)\n",
    "\n",
    "print(\"Retry process completed\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Results Summary and Comparison"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Analyze expanded format results\n",
    "print(\"Expanded Format Simulation Results:\")\n",
    "print(\"=\" * 50)\n",
    "\n",
    "for key, results in expanded_format_results.items():\n",
    "    if isinstance(results, list):\n",
    "        total = len(results)\n",
    "        successful = sum(1 for r in results if not (isinstance(r, dict) and 'error' in r))\n",
    "        failed = total - successful\n",
    "        success_rate = (successful / total) * 100\n",
    "        \n",
    "        print(f\"{key}: Total={total}, Successful={successful}, Failed={failed}, Success Rate={success_rate:.1f}%\")\n",
    "    else:\n",
    "        print(f\"{key}: FAILED - {results.get('error', 'Unknown error')}\")\n",
    "\n",
    "print(\"\\nLikert Format Simulation Results:\")\n",
    "print(\"=\" * 50)\n",
    "\n",
    "for key, results in likert_format_results.items():\n",
    "    if isinstance(results, list):\n",
    "        total = len(results)\n",
    "        successful = sum(1 for r in results if not (isinstance(r, dict) and 'error' in r))\n",
    "        failed = total - successful\n",
    "        success_rate = (successful / total) * 100\n",
    "        \n",
    "        print(f\"{key}: Total={total}, Successful={successful}, Failed={failed}, Success Rate={success_rate:.1f}%\")\n",
    "    else:\n",
    "        print(f\"{key}: FAILED - {results.get('error', 'Unknown error')}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Analyze correlations between facet scores and simulation outcomes\n",
    "def analyze_facet_correlations(results_dict, facet_scores, model_name):\n",
    "    \"\"\"Analyze correlations between facet scores and Mini-Marker responses.\"\"\"\n",
    "    correlations = {}\n",
    "    \n",
    "    for key, results in results_dict.items():\n",
    "        if isinstance(results, list) and model_name in key:\n",
    "            valid_results = [r for r in results if not (isinstance(r, dict) and 'error' in r)]\n",
    "            if len(valid_results) > 10:  # Minimum sample size\n",
    "                # Extract Mini-Marker trait scores\n",
    "                trait_scores = {}\n",
    "                for i, response in enumerate(valid_results):\n",
    "                    if isinstance(response, dict):\n",
    "                        for trait, score in response.items():\n",
    "                            if trait not in trait_scores:\n",
    "                                trait_scores[trait] = []\n",
    "                            try:\n",
    "                                trait_scores[trait].append(float(score))\n",
    "                            except (ValueError, TypeError):\n",
    "                                trait_scores[trait].append(np.nan)\n",
    "                \n",
    "                # Calculate correlations with facet scores\n",
    "                for trait, scores in trait_scores.items():\n",
    "                    if len(scores) == len(facet_scores):\n",
    "                        trait_df = pd.DataFrame({'trait_score': scores})\n",
    "                        combined_df = pd.concat([facet_scores.reset_index(drop=True), trait_df], axis=1)\n",
    "                        \n",
    "                        trait_correlations = {}\n",
    "                        for facet in facet_scores.columns:\n",
    "                            corr = combined_df[facet].corr(combined_df['trait_score'])\n",
    "                            if not np.isnan(corr):\n",
    "                                trait_correlations[facet] = corr\n",
    "                        \n",
    "                        if trait_correlations:\n",
    "                            correlations[f\"{key}_{trait}\"] = trait_correlations\n",
    "    \n",
    "    return correlations\n",
    "\n",
    "# Analyze correlations for one model as example\n",
    "sample_correlations = analyze_facet_correlations(expanded_format_results, facet_scores, 'gpt-4')\n",
    "\n",
    "if sample_correlations:\n",
    "    print(\"Sample Facet-Trait Correlations (GPT-4):\")\n",
    "    for key, corrs in list(sample_correlations.items())[:3]:  # Show first 3\n",
    "        print(f\"\\n{key}:\")\n",
    "        sorted_corrs = sorted(corrs.items(), key=lambda x: abs(x[1]), reverse=True)[:5]\n",
    "        for facet, corr in sorted_corrs:\n",
    "            print(f\"  {facet}: {corr:.3f}\")\nelse:\n    print(\"No valid correlations found - check data alignment\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": "# Save simulation results\nresults_dir = Path(\"study_3_results\")\nresults_dir.mkdir(exist_ok=True)\n\n# Save processed data with facet scores\ndata_with_facets.to_csv(results_dir / 'study3_data_with_facets.csv', index=False)\nfacet_scores.to_csv(results_dir / 'facet_scores.csv', index=False)\ndomain_scores.to_csv(results_dir / 'domain_scores.csv', index=False)\n\nprint(f\"Study 3 simulation completed. Results saved to {results_dir}/\")",
   "outputs": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}