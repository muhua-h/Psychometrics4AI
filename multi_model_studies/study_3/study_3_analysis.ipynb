{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Multi-Model Facet-Level Personality Analysis - Study 3\n",
    "\n",
    "This notebook analyzes the results from Study 3 facet-level parameter extraction and simulation across multiple LLM models.\n",
    "\n",
    "## Prerequisites\n",
    "- Run `study_3_multi_model_simulation.ipynb` first to generate simulation results\n",
    "- Results should be saved in `study_3_results/` directory\n",
    "\n",
    "## Analysis Overview\n",
    "1. **Data Loading**: Load saved simulation results, facet scores, and empirical data\n",
    "2. **Format Comparison**: Compare expanded vs. Likert format effectiveness\n",
    "3. **Facet-Level Analysis**: Analyze personality structure at the facet level\n",
    "4. **Cross-Model Validation**: Assess consistency across different LLMs\n",
    "5. **Factor Analysis**: Validate personality structure preservation\n",
    "6. **Empirical Validation**: Test against human personality data\n",
    "7. **Comprehensive Reporting**: Generate detailed analysis report\n",
    "8. **Export Results**: Save all analysis outputs for further research"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "from scipy.stats import pearsonr, spearmanr\n",
    "from scipy.cluster.hierarchy import dendrogram, linkage\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.decomposition import PCA, FactorAnalysis\n",
    "import json\n",
    "from pathlib import Path\n",
    "import warnings\n",
    "warnings.filterwarnings('ignore')\n",
    "\n",
    "# Set up plotting style\n",
    "plt.style.use('default')\n",
    "sns.set_palette(\"husl\")\n",
    "plt.rcParams['figure.figsize'] = (15, 10)\n",
    "plt.rcParams['font.size'] = 10\n",
    "\n",
    "print(\"Analysis environment setup complete\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1. Data Loading and Preparation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load simulation results\n",
    "results_dir = Path('study_3_results')\n",
    "if not results_dir.exists():\n",
    "    raise FileNotFoundError(\"Results directory not found. Please run study_3_multi_model_simulation.ipynb first.\")\n",
    "\n",
    "# Load expanded format results\n",
    "expanded_results = {}\n",
    "expanded_dir = results_dir / 'expanded_format'\n",
    "if expanded_dir.exists():\n",
    "    for json_file in expanded_dir.glob('bfi_to_minimarker_*.json'):\n",
    "        filename = json_file.stem\n",
    "        parts = filename.split('_')\n",
    "        model = parts[3]\n",
    "        temp_part = parts[4]\n",
    "        temp_value = parts[5]\n",
    "        temperature = f\"{temp_part.replace('temp', '')}.{temp_value}\"\n",
    "        \n",
    "        key = f\"{model}_temp{temperature}\"\n",
    "        \n",
    "        with open(json_file, 'r') as f:\n",
    "            expanded_results[key] = json.load(f)\n",
    "        \n",
    "        print(f\"Loaded expanded {key}: {len(expanded_results[key])} participants\")\n",
    "\n",
    "# Load Likert format results\n",
    "likert_results = {}\n",
    "likert_dir = results_dir / 'likert_format'\n",
    "if likert_dir.exists():\n",
    "    for json_file in likert_dir.glob('bfi_to_minimarker_likert_*.json'):\n",
    "        filename = json_file.stem\n",
    "        parts = filename.split('_')\n",
    "        model = parts[4]\n",
    "        temp_part = parts[5]\n",
    "        temp_value = parts[6]\n",
    "        temperature = f\"{temp_part.replace('temp', '')}.{temp_value}\"\n",
    "        \n",
    "        key = f\"{model}_temp{temperature}\"\n",
    "        \n",
    "        with open(json_file, 'r') as f:\n",
    "            likert_results[key] = json.load(f)\n",
    "        \n",
    "        print(f\"Loaded Likert {key}: {len(likert_results[key])} participants\")\n",
    "\n",
    "print(f\"\\nTotal expanded result sets: {len(expanded_results)}\")\n",
    "print(f\"Total Likert result sets: {len(likert_results)}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load processed data with facet scores\n",
    "data_with_facets_path = results_dir / 'study3_data_with_facets.csv'\n",
    "facet_scores_path = results_dir / 'facet_scores.csv'\n",
    "domain_scores_path = results_dir / 'domain_scores.csv'\n",
    "\n",
    "if data_with_facets_path.exists():\n",
    "    data_with_facets = pd.read_csv(data_with_facets_path)\n",
    "    print(f\"Loaded processed data: {data_with_facets.shape}\")\n",
    "else:\n",
    "    raise FileNotFoundError(\"Processed data not found. Please run simulation first.\")\n",
    "\n",
    "if facet_scores_path.exists():\n",
    "    facet_scores = pd.read_csv(facet_scores_path)\n",
    "    print(f\"Loaded facet scores: {facet_scores.shape}\")\n",
    "    print(f\"Facets: {list(facet_scores.columns)}\")\n",
    "\n",
    "if domain_scores_path.exists():\n",
    "    domain_scores = pd.read_csv(domain_scores_path)\n",
    "    print(f\"Loaded domain scores: {domain_scores.shape}\")\n",
    "    print(f\"Domains: {list(domain_scores.columns)}\")\n",
    "\n",
    "# Load experiment summary\n",
    "summary_path = results_dir / 'study3_experiment_summary.json'\n",
    "if summary_path.exists():\n",
    "    with open(summary_path, 'r') as f:\n",
    "        experiment_summary = json.load(f)\n",
    "    print(f\"\\nExperiment conducted: {experiment_summary['timestamp']}\")\n",
    "    print(f\"Models tested: {experiment_summary['models_tested']}\")\n",
    "    print(f\"Total participants: {experiment_summary['total_participants']}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load empirical data for validation\n",
    "empirical_data_path = Path('../../raw_data/Soto_data.xlsx')\n",
    "if empirical_data_path.exists():\n",
    "    empirical_data = pd.read_excel(empirical_data_path, sheet_name='data')\n",
    "    print(f\"Loaded empirical data: {empirical_data.shape}\")\n",
    "    \n",
    "    # Get TDA columns for validation\n",
    "    tda_columns = [f\"tda{i}\" for i in range(1, 41)]\n",
    "    available_tda = [col for col in tda_columns if col in empirical_data.columns]\n",
    "    print(f\"Available Mini-Marker traits for validation: {len(available_tda)}\")\n",
    "else:\n",
    "    print(\"Empirical data not found - some validation analyses will be limited\")\n",
    "    empirical_data = None\n",
    "    available_tda = []"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2. Data Processing and Structure"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def process_study3_results(results_dict, format_name):\n",
    "    \"\"\"\n",
    "    Process Study 3 simulation results into structured DataFrames.\n",
    "    \n",
    "    Returns:\n",
    "    - results_df: Long format DataFrame with all responses\n",
    "    - trait_stats: Summary statistics by trait and model\n",
    "    \"\"\"\n",
    "    \n",
    "    # Mini-Marker trait names in order\n",
    "    trait_names = [\n",
    "        'Bashful', 'Bold', 'Careless', 'Cold', 'Complex', 'Cooperative', 'Creative', 'Deep',\n",
    "        'Disorganized', 'Efficient', 'Energetic', 'Envious', 'Extraverted', 'Fretful', 'Harsh',\n",
    "        'Imaginative', 'Inefficient', 'Intellectual', 'Jealous', 'Kind', 'Moody', 'Organized',\n",
    "        'Philosophical', 'Practical', 'Quiet', 'Relaxed', 'Rude', 'Shy', 'Sloppy', 'Sympathetic',\n",
    "        'Systematic', 'Talkative', 'Temperamental', 'Touchy', 'Uncreative', 'Unenvious',\n",
    "        'Unintellectual', 'Unsympathetic', 'Warm', 'Withdrawn'\n",
    "    ]\n",
    "    \n",
    "    # Initialize storage\n",
    "    results_list = []\n",
    "    \n",
    "    # Process each model-temperature combination\n",
    "    for model_temp, results in results_dict.items():\n",
    "        if not isinstance(results, list):\n",
    "            print(f\"Skipping {model_temp}: {results}\")\n",
    "            continue\n",
    "            \n",
    "        model_name = model_temp.split('_temp')[0]\n",
    "        temperature = model_temp.split('_temp')[1]\n",
    "        \n",
    "        # Extract responses for each participant\n",
    "        for i, result in enumerate(results):\n",
    "            if isinstance(result, dict) and 'error' not in result:\n",
    "                # Convert to standard format\n",
    "                response_dict = {\n",
    "                    'participant_id': i, \n",
    "                    'model': model_name, \n",
    "                    'temperature': temperature,\n",
    "                    'format': format_name\n",
    "                }\n",
    "                \n",
    "                for trait in trait_names:\n",
    "                    if trait in result:\n",
    "                        try:\n",
    "                            value = float(result[trait])\n",
    "                            response_dict[trait] = value\n",
    "                        except (ValueError, TypeError):\n",
    "                            response_dict[trait] = np.nan\n",
    "                    else:\n",
    "                        response_dict[trait] = np.nan\n",
    "                \n",
    "                results_list.append(response_dict)\n",
    "    \n",
    "    # Create DataFrame\n",
    "    results_df = pd.DataFrame(results_list)\n",
    "    \n",
    "    # Calculate trait statistics\n",
    "    trait_stats = []\n",
    "    if not results_df.empty:\n",
    "        for trait in trait_names:\n",
    "            if trait in results_df.columns:\n",
    "                trait_data = results_df.groupby(['model', 'temperature', 'format'])[trait].agg([\n",
    "                    'count', 'mean', 'std', 'min', 'max'\n",
    "                ]).reset_index()\n",
    "                trait_data['trait'] = trait\n",
    "                trait_stats.append(trait_data)\n",
    "    \n",
    "    trait_stats_df = pd.concat(trait_stats, ignore_index=True) if trait_stats else pd.DataFrame()\n",
    "    \n",
    "    return results_df, trait_stats_df\n",
    "\n",
    "# Process both formats\n",
    "print(\"Processing expanded format results...\")\n",
    "expanded_df, expanded_stats = process_study3_results(expanded_results, 'expanded')\n",
    "\n",
    "print(\"Processing Likert format results...\")\n",
    "likert_df, likert_stats = process_study3_results(likert_results, 'likert')\n",
    "\n",
    "# Combine results\n",
    "combined_results_df = pd.concat([expanded_df, likert_df], ignore_index=True)\n",
    "combined_stats_df = pd.concat([expanded_stats, likert_stats], ignore_index=True)\n",
    "\n",
    "print(f\"\\nExpanded format results: {expanded_df.shape}\")\n",
    "print(f\"Likert format results: {likert_df.shape}\")\n",
    "print(f\"Combined results: {combined_results_df.shape}\")\n",
    "print(f\"Combined statistics: {combined_stats_df.shape}\")\n",
    "\n",
    "if not combined_results_df.empty:\n",
    "    print(f\"\\nAvailable models: {combined_results_df['model'].unique()}\")\n",
    "    print(f\"Available temperatures: {combined_results_df['temperature'].unique()}\")\n",
    "    print(f\"Available formats: {combined_results_df['format'].unique()}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3. Format Comparison Analysis"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def analyze_format_comparison(combined_results_df, combined_stats_df):\n",
    "    \"\"\"Compare expanded vs. Likert format effectiveness.\"\"\"\n",
    "    \n",
    "    if combined_results_df.empty:\n",
    "        print(\"No results to compare\")\n",
    "        return\n",
    "    \n",
    "    print(\"=== FORMAT COMPARISON ANALYSIS ===\")\n",
    "    \n",
    "    # 1. Response completion rates\n",
    "    print(\"\\n1. Response Completion Rates:\")\n",
    "    completion_rates = combined_results_df.groupby(['format', 'model', 'temperature']).agg({\n",
    "        'participant_id': 'count'\n",
    "    }).rename(columns={'participant_id': 'n_responses'})\n",
    "    print(completion_rates)\n",
    "    \n",
    "    # 2. Response quality (non-missing values)\n",
    "    print(\"\\n2. Response Quality (Average Non-Missing Traits):\")\n",
    "    trait_columns = [col for col in combined_results_df.columns \n",
    "                    if col not in ['participant_id', 'model', 'temperature', 'format']]\n",
    "    \n",
    "    quality_metrics = []\n",
    "    for format_name in combined_results_df['format'].unique():\n",
    "        format_data = combined_results_df[combined_results_df['format'] == format_name]\n",
    "        for model in format_data['model'].unique():\n",
    "            for temp in format_data['temperature'].unique():\n",
    "                subset = format_data[(format_data['model'] == model) & \n",
    "                                  (format_data['temperature'] == temp)]\n",
    "                if not subset.empty:\n",
    "                    # Calculate average non-missing traits per participant\n",
    "                    non_missing = subset[trait_columns].notna().sum(axis=1).mean()\n",
    "                    quality_metrics.append({\n",
    "                        'format': format_name,\n",
    "                        'model': model,\n",
    "                        'temperature': temp,\n",
    "                        'avg_non_missing_traits': non_missing,\n",
    "                        'completion_percentage': (non_missing / len(trait_columns)) * 100\n",
    "                    })\n",
    "    \n",
    "    quality_df = pd.DataFrame(quality_metrics)\n",
    "    if not quality_df.empty:\n",
    "        quality_summary = quality_df.groupby('format')[['avg_non_missing_traits', 'completion_percentage']].mean()\n",
    "        print(quality_summary.round(2))\n",
    "    \n",
    "    # 3. Response variability\n",
    "    print(\"\\n3. Response Variability by Format:\")\n",
    "    if not combined_stats_df.empty:\n",
    "        variability = combined_stats_df.groupby('format')['std'].mean()\n",
    "        print(variability.round(3))\n",
    "    \n",
    "    # 4. Create visualization\n",
    "    if not quality_df.empty:\n",
    "        fig, axes = plt.subplots(1, 2, figsize=(15, 6))\n",
    "        \n",
    "        # Completion rates by format\n",
    "        sns.boxplot(data=quality_df, x='format', y='completion_percentage', ax=axes[0])\n",
    "        axes[0].set_title('Response Completion by Format')\n",
    "        axes[0].set_ylabel('Completion Percentage')\n",
    "        \n",
    "        # Variability comparison\n",
    "        if not combined_stats_df.empty:\n",
    "            sns.boxplot(data=combined_stats_df, x='format', y='std', ax=axes[1])\n",
    "            axes[1].set_title('Response Variability by Format')\n",
    "            axes[1].set_ylabel('Standard Deviation')\n",
    "        \n",
    "        plt.tight_layout()\n",
    "        plt.savefig('study_3_results/format_comparison.png', dpi=300, bbox_inches='tight')\n",
    "        plt.show()\n",
    "    \n",
    "    return quality_df\n",
    "\n",
    "# Run format comparison\n",
    "format_quality = analyze_format_comparison(combined_results_df, combined_stats_df)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 4. Facet-Level Analysis"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def analyze_facet_level_performance(combined_results_df, facet_scores):\n",
    "    \"\"\"Analyze personality structure at the facet level.\"\"\"\n",
    "    \n",
    "    if combined_results_df.empty or facet_scores.empty:\n",
    "        print(\"Insufficient data for facet-level analysis\")\n",
    "        return\n",
    "    \n",
    "    print(\"=== FACET-LEVEL ANALYSIS ===\")\n",
    "    \n",
    "    # BFI-2 Facet structure\n",
    "    facet_structure = {\n",
    "        'Sociability': ['Talkative', 'Extraverted', 'Bold'],\n",
    "        'Assertiveness': ['Energetic', 'Efficient'],\n",
    "        'Compassion': ['Sympathetic', 'Warm', 'Kind'],\n",
    "        'Respectfulness': ['Cooperative'],\n",
    "        'Organization': ['Organized', 'Systematic'],\n",
    "        'Productiveness': ['Efficient'],\n",
    "        'Anxiety': ['Moody', 'Temperamental', 'Touchy'],\n",
    "        'Depression': ['Withdrawn', 'Quiet'],\n",
    "        'Intellectual_Curiosity': ['Intellectual', 'Philosophical', 'Complex'],\n",
    "        'Aesthetic_Sensitivity': ['Creative', 'Imaginative']\n",
    "    }\n",
    "    \n",
    "    # Calculate facet-level correlations for each model\n",
    "    facet_correlations = {}\n",
    "    \n",
    "    for model in combined_results_df['model'].unique():\n",
    "        for temp in combined_results_df['temperature'].unique():\n",
    "            for format_type in combined_results_df['format'].unique():\n",
    "                subset = combined_results_df[\n",
    "                    (combined_results_df['model'] == model) & \n",
    "                    (combined_results_df['temperature'] == temp) &\n",
    "                    (combined_results_df['format'] == format_type)\n",
    "                ]\n",
    "                \n",
    "                if subset.empty:\n",
    "                    continue\n",
    "                \n",
    "                key = f\"{model}_{temp}_{format_type}\"\n",
    "                facet_corr = {}\n",
    "                \n",
    "                # Calculate correlations between empirical facets and LLM responses\n",
    "                for facet_name in facet_scores.columns:\n",
    "                    if facet_name in facet_structure:\n",
    "                        related_traits = facet_structure[facet_name]\n",
    "                        trait_correlations = []\n",
    "                        \n",
    "                        for trait in related_traits:\n",
    "                            if trait in subset.columns:\n",
    "                                # Align data by participant_id\n",
    "                                n_participants = min(len(facet_scores), len(subset))\n",
    "                                empirical_vals = facet_scores[facet_name].iloc[:n_participants]\n",
    "                                llm_vals = subset[trait].iloc[:n_participants]\n",
    "                                \n",
    "                                # Remove NaN values\n",
    "                                valid_indices = ~(empirical_vals.isna() | llm_vals.isna())\n",
    "                                if valid_indices.sum() > 5:  # Minimum 5 valid pairs\n",
    "                                    corr, _ = pearsonr(empirical_vals[valid_indices], \n",
    "                                                     llm_vals[valid_indices])\n",
    "                                    if not np.isnan(corr):\n",
    "                                        trait_correlations.append(corr)\n",
    "                        \n",
    "                        if trait_correlations:\n",
    "                            facet_corr[facet_name] = np.mean(trait_correlations)\n",
    "                \n",
    "                if facet_corr:\n",
    "                    facet_correlations[key] = facet_corr\n",
    "    \n",
    "    # Display top facet correlations\n",
    "    if facet_correlations:\n",
    "        print(\"\\nTop Facet-Level Correlations:\")\n",
    "        for key, corrs in list(facet_correlations.items())[:3]:\n",
    "            print(f\"\\n{key}:\")\n",
    "            sorted_corrs = sorted(corrs.items(), key=lambda x: abs(x[1]), reverse=True)\n",
    "            for facet, corr in sorted_corrs[:5]:\n",
    "                print(f\"  {facet}: {corr:.3f}\")\n",
    "    \n",
    "    # Create facet correlation heatmap\n",
    "    if facet_correlations:\n",
    "        # Convert to DataFrame for visualization\n",
    "        all_facets = set()\n",
    "        for corrs in facet_correlations.values():\n",
    "            all_facets.update(corrs.keys())\n",
    "        \n",
    "        corr_matrix = pd.DataFrame(index=list(facet_correlations.keys()), \n",
    "                                  columns=sorted(all_facets))\n",
    "        \n",
    "        for model_key, corrs in facet_correlations.items():\n",
    "            for facet, corr in corrs.items():\n",
    "                corr_matrix.loc[model_key, facet] = corr\n",
    "        \n",
    "        # Plot heatmap\n",
    "        plt.figure(figsize=(12, 8))\n",
    "        sns.heatmap(corr_matrix.astype(float), \n",
    "                   annot=True, cmap='RdBu_r', center=0, \n",
    "                   fmt='.2f', cbar_kws={'label': 'Correlation'})\n",
    "        plt.title('Facet-Level Correlations Across Models and Formats')\n",
    "        plt.ylabel('Model_Temperature_Format')\n",
    "        plt.xlabel('BFI-2 Facets')\n",
    "        plt.xticks(rotation=45, ha='right')\n",
    "        plt.yticks(rotation=0)\n",
    "        plt.tight_layout()\n",
    "        plt.savefig('study_3_results/facet_correlations_heatmap.png', dpi=300, bbox_inches='tight')\n",
    "        plt.show()\n",
    "        \n",
    "        return corr_matrix\n",
    "    \n",
    "    return None\n",
    "\n",
    "# Run facet-level analysis\n",
    "facet_correlation_matrix = analyze_facet_level_performance(combined_results_df, facet_scores)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 5. Cross-Model Validation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def analyze_cross_model_consistency(combined_results_df):\n",
    "    \"\"\"Assess consistency across different LLMs.\"\"\"\n",
    "    \n",
    "    if combined_results_df.empty:\n",
    "        print(\"No results for cross-model analysis\")\n",
    "        return\n",
    "    \n",
    "    print(\"=== CROSS-MODEL CONSISTENCY ANALYSIS ===\")\n",
    "    \n",
    "    trait_names = [col for col in combined_results_df.columns \n",
    "                  if col not in ['participant_id', 'model', 'temperature', 'format']]\n",
    "    \n",
    "    # 1. Inter-model correlations by format\n",
    "    print(\"\\n1. Inter-Model Correlations by Format:\")\n",
    "    \n",
    "    format_correlations = {}\n",
    "    \n",
    "    for format_type in combined_results_df['format'].unique():\n",
    "        format_data = combined_results_df[combined_results_df['format'] == format_type]\n",
    "        \n",
    "        models = format_data['model'].unique()\n",
    "        temps = format_data['temperature'].unique()\n",
    "        \n",
    "        correlations = []\n",
    "        model_pairs = []\n",
    "        \n",
    "        for i, (model1, temp1) in enumerate([(m, t) for m in models for t in temps]):\n",
    "            subset1 = format_data[(format_data['model'] == model1) & \n",
    "                                (format_data['temperature'] == temp1)]\n",
    "            if subset1.empty:\n",
    "                continue\n",
    "                \n",
    "            for j, (model2, temp2) in enumerate([(m, t) for m in models for t in temps]):\n",
    "                if i >= j:\n",
    "                    continue\n",
    "                    \n",
    "                subset2 = format_data[(format_data['model'] == model2) & \n",
    "                                    (format_data['temperature'] == temp2)]\n",
    "                if subset2.empty:\n",
    "                    continue\n",
    "                \n",
    "                # Calculate average correlation across traits\n",
    "                trait_correlations = []\n",
    "                for trait in trait_names:\n",
    "                    if trait in subset1.columns and trait in subset2.columns:\n",
    "                        # Align by participant_id\n",
    "                        merged = pd.merge(subset1[['participant_id', trait]], \n",
    "                                        subset2[['participant_id', trait]], \n",
    "                                        on='participant_id', suffixes=('_1', '_2'))\n",
    "                        \n",
    "                        clean_data = merged.dropna()\n",
    "                        if len(clean_data) > 3:\n",
    "                            corr, _ = pearsonr(clean_data[f'{trait}_1'], clean_data[f'{trait}_2'])\n",
    "                            if not np.isnan(corr):\n",
    "                                trait_correlations.append(corr)\n",
    "                \n",
    "                if trait_correlations:\n",
    "                    avg_corr = np.mean(trait_correlations)\n",
    "                    correlations.append(avg_corr)\n",
    "                    model_pairs.append(f'{model1}_t{temp1} vs {model2}_t{temp2}')\n",
    "        \n",
    "        if correlations:\n",
    "            format_correlations[format_type] = {\n",
    "                'correlations': correlations,\n",
    "                'pairs': model_pairs,\n",
    "                'mean_correlation': np.mean(correlations),\n",
    "                'std_correlation': np.std(correlations)\n",
    "            }\n",
    "            \n",
    "            print(f\"\\n{format_type.upper()} Format:\")\n",
    "            print(f\"  Mean inter-model correlation: {np.mean(correlations):.3f}\")\n",
    "            print(f\"  Std inter-model correlation: {np.std(correlations):.3f}\")\n",
    "            print(f\"  Range: {np.min(correlations):.3f} to {np.max(correlations):.3f}\")\n",
    "    \n",
    "    # 2. Temperature consistency within models\n",
    "    print(\"\\n2. Temperature Consistency Within Models:\")\n",
    "    \n",
    "    temp_consistency = {}\n",
    "    \n",
    "    for format_type in combined_results_df['format'].unique():\n",
    "        format_data = combined_results_df[combined_results_df['format'] == format_type]\n",
    "        \n",
    "        for model in format_data['model'].unique():\n",
    "            temp0_data = format_data[(format_data['model'] == model) & \n",
    "                                   (format_data['temperature'] == '0.0')]\n",
    "            temp1_data = format_data[(format_data['model'] == model) & \n",
    "                                   (format_data['temperature'] == '1.0')]\n",
    "            \n",
    "            if not temp0_data.empty and not temp1_data.empty:\n",
    "                trait_consistencies = []\n",
    "                \n",
    "                for trait in trait_names:\n",
    "                    merged = pd.merge(temp0_data[['participant_id', trait]], \n",
    "                                    temp1_data[['participant_id', trait]], \n",
    "                                    on='participant_id', suffixes=('_t0', '_t1'))\n",
    "                    \n",
    "                    clean_data = merged.dropna()\n",
    "                    if len(clean_data) > 3:\n",
    "                        corr, _ = pearsonr(clean_data[f'{trait}_t0'], clean_data[f'{trait}_t1'])\n",
    "                        if not np.isnan(corr):\n",
    "                            trait_consistencies.append(corr)\n",
    "                \n",
    "                if trait_consistencies:\n",
    "                    temp_consistency[f\"{format_type}_{model}\"] = {\n",
    "                        'mean_consistency': np.mean(trait_consistencies),\n",
    "                        'std_consistency': np.std(trait_consistencies),\n",
    "                        'n_traits': len(trait_consistencies)\n",
    "                    }\n",
    "                    \n",
    "                    print(f\"  {format_type}_{model}: {np.mean(trait_consistencies):.3f} \"\n",
    "                         f\"(±{np.std(trait_consistencies):.3f})\")\n",
    "    \n",
    "    # 3. Visualization\n",
    "    if format_correlations:\n",
    "        fig, axes = plt.subplots(1, 2, figsize=(15, 6))\n",
    "        \n",
    "        # Inter-model correlations\n",
    "        format_names = list(format_correlations.keys())\n",
    "        mean_corrs = [format_correlations[f]['mean_correlation'] for f in format_names]\n",
    "        std_corrs = [format_correlations[f]['std_correlation'] for f in format_names]\n",
    "        \n",
    "        axes[0].bar(format_names, mean_corrs, yerr=std_corrs, capsize=5)\n",
    "        axes[0].set_title('Inter-Model Agreement by Format')\n",
    "        axes[0].set_ylabel('Mean Correlation')\n",
    "        axes[0].set_ylim(0, 1)\n",
    "        \n",
    "        # Temperature consistency\n",
    "        if temp_consistency:\n",
    "            models = list(temp_consistency.keys())\n",
    "            consistencies = [temp_consistency[m]['mean_consistency'] for m in models]\n",
    "            \n",
    "            axes[1].bar(range(len(models)), consistencies)\n",
    "            axes[1].set_title('Temperature Consistency')\n",
    "            axes[1].set_ylabel('Mean Correlation')\n",
    "            axes[1].set_xticks(range(len(models)))\n",
    "            axes[1].set_xticklabels(models, rotation=45, ha='right')\n",
    "            axes[1].set_ylim(0, 1)\n",
    "        \n",
    "        plt.tight_layout()\n",
    "        plt.savefig('study_3_results/cross_model_consistency.png', dpi=300, bbox_inches='tight')\n",
    "        plt.show()\n",
    "    \n",
    "    return format_correlations, temp_consistency\n",
    "\n",
    "# Run cross-model analysis\n",
    "format_consistency, temp_consistency = analyze_cross_model_consistency(combined_results_df)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 6. Empirical Validation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def validate_against_empirical_mini_marker(combined_results_df, empirical_data, available_tda):\n",
    "    \"\"\"Compare LLM responses with empirical human Mini-Marker data.\"\"\"\n",
    "    \n",
    "    if combined_results_df.empty or empirical_data is None or not available_tda:\n",
    "        print(\"Insufficient data for empirical validation\")\n",
    "        return None\n",
    "    \n",
    "    print(\"=== EMPIRICAL VALIDATION ANALYSIS ===\")\n",
    "    print(f\"Available empirical Mini-Marker traits: {len(available_tda)}\")\n",
    "    \n",
    "    # Mini-Marker trait names\n",
    "    trait_names = [\n",
    "        'Bashful', 'Bold', 'Careless', 'Cold', 'Complex', 'Cooperative', 'Creative', 'Deep',\n",
    "        'Disorganized', 'Efficient', 'Energetic', 'Envious', 'Extraverted', 'Fretful', 'Harsh',\n",
    "        'Imaginative', 'Inefficient', 'Intellectual', 'Jealous', 'Kind', 'Moody', 'Organized',\n",
    "        'Philosophical', 'Practical', 'Quiet', 'Relaxed', 'Rude', 'Shy', 'Sloppy', 'Sympathetic',\n",
    "        'Systematic', 'Talkative', 'Temperamental', 'Touchy', 'Uncreative', 'Unenvious',\n",
    "        'Unintellectual', 'Unsympathetic', 'Warm', 'Withdrawn'\n",
    "    ]\n",
    "    \n",
    "    # Clean empirical data\n",
    "    empirical_clean = empirical_data.dropna(subset=available_tda)\n",
    "    \n",
    "    validation_results = {}\n",
    "    \n",
    "    # For each model-format-temperature combination\n",
    "    for format_type in combined_results_df['format'].unique():\n",
    "        for model in combined_results_df['model'].unique():\n",
    "            for temp in combined_results_df['temperature'].unique():\n",
    "                subset = combined_results_df[\n",
    "                    (combined_results_df['format'] == format_type) &\n",
    "                    (combined_results_df['model'] == model) & \n",
    "                    (combined_results_df['temperature'] == temp)\n",
    "                ]\n",
    "                \n",
    "                if subset.empty:\n",
    "                    continue\n",
    "                \n",
    "                key = f'{format_type}_{model}_temp{temp}'\n",
    "                trait_correlations = []\n",
    "                trait_details = {}\n",
    "                \n",
    "                # Compare each trait\n",
    "                for i, trait in enumerate(trait_names[:len(available_tda)]):\n",
    "                    if trait in subset.columns:\n",
    "                        # Get LLM responses\n",
    "                        llm_responses = subset[['participant_id', trait]].dropna()\n",
    "                        \n",
    "                        # Get corresponding empirical data\n",
    "                        empirical_values = []\n",
    "                        llm_values = []\n",
    "                        \n",
    "                        for _, row in llm_responses.iterrows():\n",
    "                            participant_id = int(row['participant_id'])\n",
    "                            if participant_id < len(empirical_clean):\n",
    "                                emp_value = empirical_clean.iloc[participant_id][available_tda[i]]\n",
    "                                if pd.notna(emp_value) and pd.notna(row[trait]):\n",
    "                                    empirical_values.append(emp_value)\n",
    "                                    llm_values.append(row[trait])\n",
    "                        \n",
    "                        # Calculate correlation if we have enough data\n",
    "                        if len(empirical_values) > 5:\n",
    "                            corr, p_value = pearsonr(empirical_values, llm_values)\n",
    "                            \n",
    "                            if not np.isnan(corr):\n",
    "                                trait_correlations.append(corr)\n",
    "                                trait_details[trait] = {\n",
    "                                    'correlation': corr,\n",
    "                                    'p_value': p_value,\n",
    "                                    'n_participants': len(empirical_values),\n",
    "                                    'empirical_mean': np.mean(empirical_values),\n",
    "                                    'llm_mean': np.mean(llm_values),\n",
    "                                    'empirical_std': np.std(empirical_values),\n",
    "                                    'llm_std': np.std(llm_values)\n",
    "                                }\n",
    "                \n",
    "                # Store validation results\n",
    "                if trait_correlations:\n",
    "                    validation_results[key] = {\n",
    "                        'mean_correlation': np.mean(trait_correlations),\n",
    "                        'median_correlation': np.median(trait_correlations),\n",
    "                        'std_correlation': np.std(trait_correlations),\n",
    "                        'n_traits': len(trait_correlations),\n",
    "                        'trait_details': trait_details\n",
    "                    }\n",
    "    \n",
    "    # Display results\n",
    "    if validation_results:\n",
    "        print(\"\\nEmpirical Validation Results:\")\n",
    "        \n",
    "        validation_df = pd.DataFrame({k: {\n",
    "            'mean_corr': v['mean_correlation'],\n",
    "            'median_corr': v['median_correlation'],\n",
    "            'std_corr': v['std_correlation'],\n",
    "            'n_traits': v['n_traits']\n",
    "        } for k, v in validation_results.items()}).T\n",
    "        \n",
    "        print(validation_df.round(3))\n",
    "        \n",
    "        # Find best performing combinations\n",
    "        best_overall = validation_df['mean_corr'].idxmax()\n",
    "        print(f\"\\nBest overall performance: {best_overall} (r = {validation_df.loc[best_overall, 'mean_corr']:.3f})\")\n",
    "        \n",
    "        # Compare formats\n",
    "        format_performance = validation_df.reset_index()\n",
    "        format_performance['format'] = format_performance['index'].str.split('_').str[0]\n",
    "        format_comparison = format_performance.groupby('format')['mean_corr'].agg(['mean', 'std'])\n",
    "        print(\"\\nFormat Comparison:\")\n",
    "        print(format_comparison.round(3))\n",
    "        \n",
    "        # Visualization\n",
    "        fig, axes = plt.subplots(1, 2, figsize=(15, 6))\n",
    "        \n",
    "        # Performance by model-format\n",
    "        validation_df.reset_index(inplace=True)\n",
    "        validation_df['format'] = validation_df['index'].str.split('_').str[0]\n",
    "        validation_df['model'] = validation_df['index'].str.split('_').str[1]\n",
    "        \n",
    "        sns.boxplot(data=validation_df, x='format', y='mean_corr', ax=axes[0])\n",
    "        axes[0].set_title('Empirical Validation by Format')\n",
    "        axes[0].set_ylabel('Mean Correlation with Human Data')\n",
    "        \n",
    "        sns.boxplot(data=validation_df, x='model', y='mean_corr', ax=axes[1])\n",
    "        axes[1].set_title('Empirical Validation by Model')\n",
    "        axes[1].set_ylabel('Mean Correlation with Human Data')\n",
    "        axes[1].tick_params(axis='x', rotation=45)\n",
    "        \n",
    "        plt.tight_layout()\n",
    "        plt.savefig('study_3_results/empirical_validation.png', dpi=300, bbox_inches='tight')\n",
    "        plt.show()\n",
    "        \n",
    "        return validation_results, validation_df\n",
    "    \n",
    "    return None, None\n",
    "\n",
    "# Run empirical validation\n",
    "validation_results, validation_summary = validate_against_empirical_mini_marker(\n",
    "    combined_results_df, empirical_data, available_tda\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 7. Comprehensive Report Generation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def generate_study3_comprehensive_report(\n",
    "    combined_results_df, combined_stats_df, format_quality, \n",
    "    facet_correlation_matrix, format_consistency, temp_consistency,\n",
    "    validation_results, experiment_summary\n",
    "):\n",
    "    \"\"\"Generate a comprehensive analysis report for Study 3.\"\"\"\n",
    "    \n",
    "    report = []\n",
    "    report.append(\"# Study 3: Multi-Model Facet-Level Personality Analysis Report\")\n",
    "    report.append(f\"Generated: {pd.Timestamp.now().strftime('%Y-%m-%d %H:%M:%S')}\")\n",
    "    report.append(\"\\n\" + \"=\"*80)\n",
    "    \n",
    "    # Executive Summary\n",
    "    report.append(\"\\n## Executive Summary\")\n",
    "    if experiment_summary:\n",
    "        report.append(f\"- **Study Focus**: Facet-level personality parameter extraction and validation\")\n",
    "        report.append(f\"- **Participants Analyzed**: {experiment_summary.get('total_participants', 'Unknown')}\")\n",
    "        report.append(f\"- **Models Tested**: {experiment_summary.get('models_tested', [])}\")\n",
    "        report.append(f\"- **Formats Compared**: Expanded BFI-2 descriptions vs. Likert-style descriptions\")\n",
    "        report.append(f\"- **Temperature Settings**: {experiment_summary.get('temperatures', [])}\")\n",
    "    \n",
    "    if not combined_results_df.empty:\n",
    "        total_combinations = len(combined_results_df.groupby(['model', 'temperature', 'format']))\n",
    "        report.append(f\"- **Total Model-Format Combinations**: {total_combinations}\")\n",
    "    \n",
    "    # Format Comparison Results\n",
    "    report.append(\"\\n## Format Comparison: Expanded vs. Likert\")\n",
    "    \n",
    "    if format_quality is not None and not format_quality.empty:\n",
    "        format_summary = format_quality.groupby('format')[['completion_percentage']].mean()\n",
    "        report.append(\"\\n### Response Quality by Format:\")\n",
    "        for format_name, stats in format_summary.iterrows():\n",
    "            report.append(f\"- **{format_name.title()} Format**: {stats['completion_percentage']:.1f}% average completion\")\n",
    "        \n",
    "        best_format = format_summary['completion_percentage'].idxmax()\n",
    "        report.append(f\"\\n**Best Performing Format**: {best_format.title()} (higher completion rate)\")\n",
    "    \n",
    "    # Facet-Level Insights\n",
    "    report.append(\"\\n## Facet-Level Analysis\")\n",
    "    \n",
    "    if facet_correlation_matrix is not None and not facet_correlation_matrix.empty:\n",
    "        # Find best facet correlations\n",
    "        mean_facet_corrs = facet_correlation_matrix.mean(axis=0, skipna=True).sort_values(ascending=False)\n",
    "        \n",
    "        report.append(\"\\n### Most Reliably Captured Facets:\")\n",
    "        for facet, corr in mean_facet_corrs.head(5).items():\n",
    "            if not pd.isna(corr):\n",
    "                report.append(f\"- **{facet}**: Average correlation = {corr:.3f}\")\n",
    "        \n",
    "        report.append(\"\\n### Most Variable Facets:\")\n",
    "        for facet, corr in mean_facet_corrs.tail(3).items():\n",
    "            if not pd.isna(corr):\n",
    "                report.append(f\"- **{facet}**: Average correlation = {corr:.3f}\")\n",
    "    \n",
    "    # Cross-Model Consistency\n",
    "    report.append(\"\\n## Cross-Model Consistency\")\n",
    "    \n",
    "    if format_consistency:\n",
    "        report.append(\"\\n### Inter-Model Agreement by Format:\")\n",
    "        for format_type, stats in format_consistency.items():\n",
    "            mean_corr = stats['mean_correlation']\n",
    "            std_corr = stats['std_correlation']\n",
    "            report.append(f\"- **{format_type.title()} Format**: r = {mean_corr:.3f} (±{std_corr:.3f})\")\n",
    "    \n",
    "    if temp_consistency:\n",
    "        report.append(\"\\n### Temperature Consistency:\")\n",
    "        for model_format, stats in temp_consistency.items():\n",
    "            mean_cons = stats['mean_consistency']\n",
    "            report.append(f\"- **{model_format}**: r = {mean_cons:.3f}\")\n",
    "    \n",
    "    # Empirical Validation\n",
    "    if validation_results:\n",
    "        report.append(\"\\n## Empirical Validation Results\")\n",
    "        \n",
    "        # Find best performers\n",
    "        best_combination = max(validation_results.keys(), \n",
    "                             key=lambda x: validation_results[x]['mean_correlation'])\n",
    "        best_performance = validation_results[best_combination]['mean_correlation']\n",
    "        \n",
    "        report.append(f\"\\n**Best Empirical Match**: {best_combination} (r = {best_performance:.3f})\")\n",
    "        \n",
    "        # Format comparison\n",
    "        expanded_results = {k: v for k, v in validation_results.items() if k.startswith('expanded')}\n",
    "        likert_results = {k: v for k, v in validation_results.items() if k.startswith('likert')}\n",
    "        \n",
    "        if expanded_results and likert_results:\n",
    "            expanded_mean = np.mean([v['mean_correlation'] for v in expanded_results.values()])\n",
    "            likert_mean = np.mean([v['mean_correlation'] for v in likert_results.values()])\n",
    "            \n",
    "            report.append(f\"\\n**Format Performance Comparison**:\")\n",
    "            report.append(f\"- Expanded Format: r = {expanded_mean:.3f}\")\n",
    "            report.append(f\"- Likert Format: r = {likert_mean:.3f}\")\n",
    "            \n",
    "            if expanded_mean > likert_mean:\n",
    "                report.append(f\"- **Winner**: Expanded format (+{expanded_mean - likert_mean:.3f})\")\n",
    "            else:\n",
    "                report.append(f\"- **Winner**: Likert format (+{likert_mean - expanded_mean:.3f})\")\n",
    "    \n",
    "    # Key Findings\n",
    "    report.append(\"\\n## Key Findings\")\n",
    "    \n",
    "    findings = []\n",
    "    \n",
    "    # Format effectiveness\n",
    "    if format_quality is not None and not format_quality.empty:\n",
    "        format_means = format_quality.groupby('format')['completion_percentage'].mean()\n",
    "        if len(format_means) > 1:\n",
    "            best_format = format_means.idxmax()\n",
    "            findings.append(f\"**Format Effectiveness**: {best_format.title()} format shows superior response completion\")\n",
    "    \n",
    "    # Model consistency\n",
    "    if format_consistency:\n",
    "        all_corrs = [stats['mean_correlation'] for stats in format_consistency.values()]\n",
    "        if all_corrs:\n",
    "            overall_consistency = np.mean(all_corrs)\n",
    "            findings.append(f\"**Model Consistency**: Average inter-model agreement of r = {overall_consistency:.3f}\")\n",
    "    \n",
    "    # Empirical validation\n",
    "    if validation_results:\n",
    "        all_emp_corrs = [v['mean_correlation'] for v in validation_results.values()]\n",
    "        if all_emp_corrs:\n",
    "            overall_empirical = np.mean(all_emp_corrs)\n",
    "            findings.append(f\"**Empirical Validity**: Average correlation with human data of r = {overall_empirical:.3f}\")\n",
    "    \n",
    "    for finding in findings:\n",
    "        report.append(f\"\\n- {finding}\")\n",
    "    \n",
    "    # Recommendations\n",
    "    report.append(\"\\n## Recommendations\")\n",
    "    \n",
    "    recommendations = [\n",
    "        \"**Personality Assessment**: Use expanded format descriptions for more comprehensive personality capture\",\n",
    "        \"**Model Selection**: Consider ensemble approaches combining multiple models for enhanced reliability\",\n",
    "        \"**Temperature Settings**: Use temperature = 0.0 for consistent personality assessment applications\",\n",
    "        \"**Facet-Level Analysis**: Focus on consistently captured facets for reliable personality insights\",\n",
    "        \"**Future Research**: Investigate cultural and demographic factors in personality simulation\"\n",
    "    ]\n",
    "    \n",
    "    for rec in recommendations:\n",
    "        report.append(f\"\\n- {rec}\")\n",
    "    \n",
    "    # Technical Notes\n",
    "    report.append(\"\\n## Technical Notes\")\n",
    "    technical_notes = [\n",
    "        \"Facet-level analysis based on BFI-2 10-facet structure\",\n",
    "        \"Correlations computed using Pearson's correlation coefficient\",\n",
    "        \"Statistical significance testing performed where applicable\",\n",
    "        \"Missing data handled through pairwise deletion\",\n",
    "        \"Empirical validation against Mini-Marker 40-item assessment\"\n",
    "    ]\n",
    "    \n",
    "    for note in technical_notes:\n",
    "        report.append(f\"- {note}\")\n",
    "    \n",
    "    return \"\\n\".join(report)\n",
    "\n",
    "# Generate comprehensive report\n",
    "study3_report = generate_study3_comprehensive_report(\n",
    "    combined_results_df, combined_stats_df, format_quality,\n",
    "    facet_correlation_matrix, format_consistency, temp_consistency,\n",
    "    validation_results, experiment_summary\n",
    ")\n",
    "\n",
    "print(study3_report)\n",
    "\n",
    "# Save report to file\n",
    "with open('study_3_results/comprehensive_analysis_report.md', 'w') as f:\n",
    "    f.write(study3_report)\n",
    "\n",
    "print(\"\\n\" + \"=\"*80)\n",
    "print(\"STUDY 3 ANALYSIS COMPLETE\")\n",
    "print(\"Report saved to: study_3_results/comprehensive_analysis_report.md\")\n",
    "print(\"=\"*80)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 8. Export Results for Further Analysis"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def export_study3_results(\n",
    "    combined_results_df, combined_stats_df, format_quality,\n",
    "    facet_correlation_matrix, validation_results\n",
    "):\n",
    "    \"\"\"Export all Study 3 analysis results in formats suitable for further research.\"\"\"\n",
    "    \n",
    "    output_dir = Path('study_3_results')\n",
    "    output_dir.mkdir(exist_ok=True)\n",
    "    \n",
    "    print(\"Exporting Study 3 analysis results...\")\n",
    "    \n",
    "    # 1. Export main results DataFrames\n",
    "    if not combined_results_df.empty:\n",
    "        combined_results_df.to_csv(output_dir / 'results_combined_format.csv', index=False)\n",
    "        print(f\"✓ Combined results: {len(combined_results_df)} rows\")\n",
    "        \n",
    "        # Separate by format\n",
    "        for format_type in combined_results_df['format'].unique():\n",
    "            format_data = combined_results_df[combined_results_df['format'] == format_type]\n",
    "            format_data.to_csv(output_dir / f'results_{format_type}_format.csv', index=False)\n",
    "            print(f\"✓ {format_type.title()} format results: {len(format_data)} rows\")\n",
    "    \n",
    "    if not combined_stats_df.empty:\n",
    "        combined_stats_df.to_csv(output_dir / 'trait_statistics_combined.csv', index=False)\n",
    "        print(f\"✓ Combined trait statistics: {len(combined_stats_df)} rows\")\n",
    "    \n",
    "    # 2. Export format comparison results\n",
    "    if format_quality is not None and not format_quality.empty:\n",
    "        format_quality.to_csv(output_dir / 'format_comparison_metrics.csv', index=False)\n",
    "        print(f\"✓ Format comparison metrics: {len(format_quality)} rows\")\n",
    "    \n",
    "    # 3. Export facet correlation matrix\n",
    "    if facet_correlation_matrix is not None and not facet_correlation_matrix.empty:\n",
    "        facet_correlation_matrix.to_csv(output_dir / 'facet_correlations_matrix.csv')\n",
    "        print(f\"✓ Facet correlation matrix: {facet_correlation_matrix.shape}\")\n",
    "    \n",
    "    # 4. Export validation results\n",
    "    if validation_results:\n",
    "        validation_flat = []\n",
    "        for combination, validation in validation_results.items():\n",
    "            base_record = {\n",
    "                'combination': combination,\n",
    "                'format': combination.split('_')[0],\n",
    "                'model': combination.split('_')[1],\n",
    "                'temperature': combination.split('_')[2],\n",
    "                'mean_correlation': validation['mean_correlation'],\n",
    "                'median_correlation': validation['median_correlation'],\n",
    "                'std_correlation': validation['std_correlation'],\n",
    "                'n_traits': validation['n_traits']\n",
    "            }\n",
    "            \n",
    "            # Add trait-level details\n",
    "            for trait, details in validation['trait_details'].items():\n",
    "                record = base_record.copy()\n",
    "                record.update({\n",
    "                    'trait': trait,\n",
    "                    'trait_correlation': details['correlation'],\n",
    "                    'trait_p_value': details['p_value'],\n",
    "                    'trait_n_participants': details['n_participants'],\n",
    "                    'empirical_mean': details['empirical_mean'],\n",
    "                    'llm_mean': details['llm_mean'],\n",
    "                    'empirical_std': details['empirical_std'],\n",
    "                    'llm_std': details['llm_std']\n",
    "                })\n",
    "                validation_flat.append(record)\n",
    "        \n",
    "        validation_df = pd.DataFrame(validation_flat)\n",
    "        validation_df.to_csv(output_dir / 'empirical_validation_details.csv', index=False)\n",
    "        print(f\"✓ Validation details: {len(validation_df)} rows\")\n",
    "    \n",
    "    # 5. Create analysis summary\n",
    "    summary_stats = {\n",
    "        'analysis_timestamp': pd.Timestamp.now().isoformat(),\n",
    "        'study': 'Study 3 - Facet-Level Parameter Extraction',\n",
    "        'n_participants': len(combined_results_df['participant_id'].unique()) if not combined_results_df.empty else 0,\n",
    "        'n_models': len(combined_results_df['model'].unique()) if not combined_results_df.empty else 0,\n",
    "        'n_temperatures': len(combined_results_df['temperature'].unique()) if not combined_results_df.empty else 0,\n",
    "        'n_formats': len(combined_results_df['format'].unique()) if not combined_results_df.empty else 0,\n",
    "        'n_traits': len([col for col in combined_results_df.columns if col not in ['participant_id', 'model', 'temperature', 'format']]) if not combined_results_df.empty else 0\n",
    "    }\n",
    "    \n",
    "    with open(output_dir / 'analysis_metadata.json', 'w') as f:\n",
    "        json.dump(summary_stats, f, indent=2)\n",
    "    print(f\"✓ Analysis metadata saved\")\n",
    "    \n",
    "    # 6. Create R-ready format\n",
    "    if not combined_results_df.empty:\n",
    "        trait_columns = [col for col in combined_results_df.columns \n",
    "                        if col not in ['participant_id', 'model', 'temperature', 'format']]\n",
    "        r_format = combined_results_df.melt(\n",
    "            id_vars=['participant_id', 'model', 'temperature', 'format'],\n",
    "            value_vars=trait_columns,\n",
    "            var_name='trait', value_name='response'\n",
    "        )\n",
    "        r_format.to_csv(output_dir / 'results_r_format.csv', index=False)\n",
    "        print(f\"✓ R-ready format: {len(r_format)} rows\")\n",
    "    \n",
    "    print(f\"\\n📁 All Study 3 results exported to: {output_dir}/\")\n",
    "    print(\"Files available for further analysis:\")\n",
    "    for file in sorted(output_dir.glob('*.csv')):\n",
    "        print(f\"  - {file.name}\")\n",
    "    for file in sorted(output_dir.glob('*.json')):\n",
    "        print(f\"  - {file.name}\")\n",
    "    for file in sorted(output_dir.glob('*.png')):\n",
    "        print(f\"  - {file.name}\")\n",
    "\n",
    "# Export all results\n",
    "export_study3_results(\n",
    "    combined_results_df, combined_stats_df, format_quality,\n",
    "    facet_correlation_matrix, validation_results\n",
    ")\n",
    "\n",
    "print(\"\\n🎉 STUDY 3 MULTI-MODEL ANALYSIS COMPLETE! 🎉\")\n",
    "print(\"\\nNext steps:\")\n",
    "print(\"1. Review the comprehensive report: study_3_results/comprehensive_analysis_report.md\")\n",
    "print(\"2. Examine visualizations in study_3_results/\")\n",
    "print(\"3. Use exported CSV files for statistical analysis in R or Python\")\n",
    "print(\"4. Compare with Studies 2 and 4 using similar analysis frameworks\")\n",
    "print(\"5. Conduct factor analysis using R scripts for personality structure validation\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}