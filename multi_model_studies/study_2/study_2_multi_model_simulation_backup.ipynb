{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Multi-Model Personality Simulation - Study 2\n",
    "\n",
    "This notebook refactors the original Study 2 BFI-2 to Mini-Marker simulation to work with multiple LLM models using the unified portal.py interface.\n",
    "\n",
    "## Models to Test\n",
    "- GPT-4\n",
    "- GPT-4o  \n",
    "- Llama-3.3-70B-Instruct\n",
    "- DeepSeek-V3\n",
    "\n",
    "## Data Flow\n",
    "1. Load and preprocess Soto BFI-2 data\n",
    "2. Apply reverse coding to personality items\n",
    "3. Map numeric responses to expanded format descriptions\n",
    "4. Generate personality simulation prompts\n",
    "5. Run simulations across multiple models\n",
    "6. Save results for analysis"
   ]
  },
  {
   "cell_type": "code",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-06-23T14:11:41.237201Z",
     "start_time": "2025-06-23T14:11:38.974257Z"
    }
   },
   "source": [
    "import pandas as pd\n",
    "import sys\n",
    "from pathlib import Path\n",
    "\n",
    "# Add shared modules to path\n",
    "sys.path.append('../shared')\n",
    "\n",
    "from simulation_utils import (\n",
    "    SimulationConfig, \n",
    "    run_bfi_to_minimarker_simulation,\n",
    "    retry_failed_participants\n",
    ")\n",
    "from schema_bfi2 import expanded_scale\n",
    "from mini_marker_prompt import get_prompt"
   ],
   "outputs": [],
   "execution_count": 1
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Data Loading and Preprocessing"
   ]
  },
  {
   "cell_type": "code",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-06-23T14:11:47.271225Z",
     "start_time": "2025-06-23T14:11:44.880666Z"
    }
   },
   "source": "# Load the Soto BFI-2 dataset\ndata_path = Path('../../raw_data/Soto_data.xlsx')\nif not data_path.exists():\n    print(f\"Data file not found at {data_path}\")\n    print(\"Please ensure the raw_data/Soto_data.xlsx file exists in the project root\")\n    raise FileNotFoundError(f\"Data file not found: {data_path}\")\n\ndata = pd.read_excel(data_path, sheet_name='data')\nprint(f\"Loaded data shape: {data.shape}\")\ndata.head()",
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loaded data shape: (470, 704)\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "   case_id   age sex  ethnicity  rel_acquaintance  rel_friend  rel_roommate  \\\n",
       "0        1  27.0   M        2.0               NaN         NaN           NaN   \n",
       "1        2  26.0   M        3.0               NaN         NaN           NaN   \n",
       "2        3  24.0   F        4.0               NaN         NaN           NaN   \n",
       "3        4  33.0   M        3.0               NaN         1.0           NaN   \n",
       "4        5  23.0   F        5.0               NaN         NaN           NaN   \n",
       "\n",
       "   rel_boygirlfriend  rel_relative  rel_other  ... tneo_n3_dep  tneo_n4_sel  \\\n",
       "0                NaN           NaN        NaN  ...   51.250000    40.181818   \n",
       "1                NaN           NaN        NaN  ...   69.632353    60.636364   \n",
       "2                NaN           NaN        NaN  ...   60.441176    74.272727   \n",
       "3                NaN           NaN        NaN  ...   67.794118    58.363636   \n",
       "4                NaN           NaN        NaN  ...   62.279412    67.454545   \n",
       "\n",
       "   tneo_n5_imp  tneo_n6_vul  tneo_o1_fan  tneo_o2_aes  tneo_o3_fee  \\\n",
       "0    64.000000    55.102041    46.639344    46.969697         66.7   \n",
       "1    66.272727    65.306122    54.836066    56.439394         51.7   \n",
       "2    54.909091    65.306122    75.327869    56.439394         56.7   \n",
       "3    64.000000    52.551020    54.836066    50.757576         36.7   \n",
       "4    41.272727    60.204082    50.737705    48.863636         49.2   \n",
       "\n",
       "   tneo_o4_act  tneo_o5_ide  tneo_o6_val  \n",
       "0    57.065217    41.984127    58.039216  \n",
       "1    51.630435    51.904762    45.784314  \n",
       "2    40.760870    51.904762    58.039216  \n",
       "3    65.217391    63.809524    58.039216  \n",
       "4    46.195652    38.015873    38.431373  \n",
       "\n",
       "[5 rows x 704 columns]"
      ],
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>case_id</th>\n",
       "      <th>age</th>\n",
       "      <th>sex</th>\n",
       "      <th>ethnicity</th>\n",
       "      <th>rel_acquaintance</th>\n",
       "      <th>rel_friend</th>\n",
       "      <th>rel_roommate</th>\n",
       "      <th>rel_boygirlfriend</th>\n",
       "      <th>rel_relative</th>\n",
       "      <th>rel_other</th>\n",
       "      <th>...</th>\n",
       "      <th>tneo_n3_dep</th>\n",
       "      <th>tneo_n4_sel</th>\n",
       "      <th>tneo_n5_imp</th>\n",
       "      <th>tneo_n6_vul</th>\n",
       "      <th>tneo_o1_fan</th>\n",
       "      <th>tneo_o2_aes</th>\n",
       "      <th>tneo_o3_fee</th>\n",
       "      <th>tneo_o4_act</th>\n",
       "      <th>tneo_o5_ide</th>\n",
       "      <th>tneo_o6_val</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>1</td>\n",
       "      <td>27.0</td>\n",
       "      <td>M</td>\n",
       "      <td>2.0</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>...</td>\n",
       "      <td>51.250000</td>\n",
       "      <td>40.181818</td>\n",
       "      <td>64.000000</td>\n",
       "      <td>55.102041</td>\n",
       "      <td>46.639344</td>\n",
       "      <td>46.969697</td>\n",
       "      <td>66.7</td>\n",
       "      <td>57.065217</td>\n",
       "      <td>41.984127</td>\n",
       "      <td>58.039216</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>2</td>\n",
       "      <td>26.0</td>\n",
       "      <td>M</td>\n",
       "      <td>3.0</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>...</td>\n",
       "      <td>69.632353</td>\n",
       "      <td>60.636364</td>\n",
       "      <td>66.272727</td>\n",
       "      <td>65.306122</td>\n",
       "      <td>54.836066</td>\n",
       "      <td>56.439394</td>\n",
       "      <td>51.7</td>\n",
       "      <td>51.630435</td>\n",
       "      <td>51.904762</td>\n",
       "      <td>45.784314</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>3</td>\n",
       "      <td>24.0</td>\n",
       "      <td>F</td>\n",
       "      <td>4.0</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>...</td>\n",
       "      <td>60.441176</td>\n",
       "      <td>74.272727</td>\n",
       "      <td>54.909091</td>\n",
       "      <td>65.306122</td>\n",
       "      <td>75.327869</td>\n",
       "      <td>56.439394</td>\n",
       "      <td>56.7</td>\n",
       "      <td>40.760870</td>\n",
       "      <td>51.904762</td>\n",
       "      <td>58.039216</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>4</td>\n",
       "      <td>33.0</td>\n",
       "      <td>M</td>\n",
       "      <td>3.0</td>\n",
       "      <td>NaN</td>\n",
       "      <td>1.0</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>...</td>\n",
       "      <td>67.794118</td>\n",
       "      <td>58.363636</td>\n",
       "      <td>64.000000</td>\n",
       "      <td>52.551020</td>\n",
       "      <td>54.836066</td>\n",
       "      <td>50.757576</td>\n",
       "      <td>36.7</td>\n",
       "      <td>65.217391</td>\n",
       "      <td>63.809524</td>\n",
       "      <td>58.039216</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>5</td>\n",
       "      <td>23.0</td>\n",
       "      <td>F</td>\n",
       "      <td>5.0</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>...</td>\n",
       "      <td>62.279412</td>\n",
       "      <td>67.454545</td>\n",
       "      <td>41.272727</td>\n",
       "      <td>60.204082</td>\n",
       "      <td>50.737705</td>\n",
       "      <td>48.863636</td>\n",
       "      <td>49.2</td>\n",
       "      <td>46.195652</td>\n",
       "      <td>38.015873</td>\n",
       "      <td>38.431373</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>5 rows × 704 columns</p>\n",
       "</div>"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "execution_count": 2
  },
  {
   "cell_type": "code",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-06-23T14:11:55.823421Z",
     "start_time": "2025-06-23T14:11:55.817581Z"
    }
   },
   "source": [
    "# Generate column names for TDA (Mini-Marker) and BFI-2 items\n",
    "tda_columns = [f\"tda{i}\" for i in range(1, 41)]\n",
    "sbfi_columns = [f\"bfi{i}\" for i in range(1, 61)]\n",
    "selected_columns = tda_columns + sbfi_columns\n",
    "\n",
    "print(f\"Original data shape: {data.shape}\")\n",
    "\n",
    "# Remove rows with missing values in the selected columns\n",
    "data = data.dropna(subset=selected_columns)\n",
    "print(f\"Data shape after removing missing values: {data.shape}\")"
   ],
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Original data shape: (470, 704)\n",
      "Data shape after removing missing values: (438, 704)\n"
     ]
    }
   ],
   "execution_count": 3
  },
  {
   "cell_type": "code",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-06-23T14:12:00.966345Z",
     "start_time": "2025-06-23T14:12:00.955768Z"
    }
   },
   "source": [
    "# Reverse coding map for BFI-2 items\n",
    "reverse_coding_map = {\n",
    "    'bfi1': 'bfi1', 'bfi2': 'bfi2', 'bfi3': 'bfi3R', 'bfi4': 'bfi4R', 'bfi5': 'bfi5R',\n",
    "    'bfi6': 'bfi6', 'bfi7': 'bfi7', 'bfi8': 'bfi8R', 'bfi9': 'bfi9R', 'bfi10': 'bfi10',\n",
    "    'bfi11': 'bfi11R', 'bfi12': 'bfi12R', 'bfi13': 'bfi13', 'bfi14': 'bfi14', 'bfi15': 'bfi15',\n",
    "    'bfi16': 'bfi16R', 'bfi17': 'bfi17R', 'bfi18': 'bfi18', 'bfi19': 'bfi19', 'bfi20': 'bfi20',\n",
    "    'bfi21': 'bfi21', 'bfi22': 'bfi22R', 'bfi23': 'bfi23R', 'bfi24': 'bfi24R', 'bfi25': 'bfi25R',\n",
    "    'bfi26': 'bfi26R', 'bfi27': 'bfi27', 'bfi28': 'bfi28R', 'bfi29': 'bfi29R', 'bfi30': 'bfi30R',\n",
    "    'bfi31': 'bfi31R', 'bfi32': 'bfi32', 'bfi33': 'bfi33', 'bfi34': 'bfi34', 'bfi35': 'bfi35',\n",
    "    'bfi36': 'bfi36R', 'bfi37': 'bfi37R', 'bfi38': 'bfi38', 'bfi39': 'bfi39', 'bfi40': 'bfi40',\n",
    "    'bfi41': 'bfi41', 'bfi42': 'bfi42R', 'bfi43': 'bfi43', 'bfi44': 'bfi44R', 'bfi45': 'bfi45R',\n",
    "    'bfi46': 'bfi46', 'bfi47': 'bfi47R', 'bfi48': 'bfi48R', 'bfi49': 'bfi49R', 'bfi50': 'bfi50R',\n",
    "    'bfi51': 'bfi51R', 'bfi52': 'bfi52', 'bfi53': 'bfi53', 'bfi54': 'bfi54', 'bfi55': 'bfi55R',\n",
    "    'bfi56': 'bfi56', 'bfi57': 'bfi57', 'bfi58': 'bfi58R', 'bfi59': 'bfi59', 'bfi60': 'bfi60'\n",
    "}\n",
    "\n",
    "# Apply reverse coding\n",
    "for key, value in reverse_coding_map.items():\n",
    "    if value.endswith('R'):  # Reverse coded\n",
    "        data[key] = 6 - data[key]\n",
    "    # else: keep original value\n",
    "\n",
    "print(\"Reverse coding applied successfully\")"
   ],
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Reverse coding applied successfully\n"
     ]
    }
   ],
   "execution_count": 4
  },
  {
   "cell_type": "code",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-06-23T14:12:04.350709Z",
     "start_time": "2025-06-23T14:12:04.142373Z"
    }
   },
   "source": [
    "# Map numeric values to expanded format descriptions\n",
    "def map_values(row):\n",
    "    mapped_row = row.copy()\n",
    "    for key in expanded_scale:\n",
    "        if pd.notna(row[key]):  # Check if the value is not NaN\n",
    "            index = int(row[key]) - 1  # Convert to 0-index\n",
    "            mapped_row[key] = expanded_scale[key][index]  # Replace with corresponding string\n",
    "    return mapped_row\n",
    "\n",
    "# Apply mapping to BFI columns\n",
    "mapped_data = data[sbfi_columns].apply(map_values, axis=1)\n",
    "\n",
    "# Create combined BFI-2 description\n",
    "mapped_data['combined_bfi2'] = mapped_data[[f'bfi{i}' for i in range(1, 61)]].apply(\n",
    "    lambda row: ' '.join(row), axis=1\n",
    ")\n",
    "\n",
    "# Add combined description to original data\n",
    "data['combined_bfi2'] = mapped_data['combined_bfi2']\n",
    "\n",
    "print(\"Personality descriptions created successfully\")\n",
    "print(f\"Final data shape: {data.shape}\")"
   ],
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Personality descriptions created successfully\n",
      "Final data shape: (438, 705)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/var/folders/zh/79y5ykz51rxcnjchzb73nfxw0000gn/T/ipykernel_33913/3440998859.py:7: FutureWarning: Setting an item of incompatible dtype is deprecated and will raise in a future error of pandas. Value 'I am fairly organized.' has dtype incompatible with int64, please explicitly cast to a compatible dtype first.\n",
      "  mapped_row[key] = expanded_scale[key][index]  # Replace with corresponding string\n",
      "/var/folders/zh/79y5ykz51rxcnjchzb73nfxw0000gn/T/ipykernel_33913/3440998859.py:7: FutureWarning: Setting an item of incompatible dtype is deprecated and will raise in a future error of pandas. Value 'I am fairly disorganized.' has dtype incompatible with int64, please explicitly cast to a compatible dtype first.\n",
      "  mapped_row[key] = expanded_scale[key][index]  # Replace with corresponding string\n",
      "/var/folders/zh/79y5ykz51rxcnjchzb73nfxw0000gn/T/ipykernel_33913/3440998859.py:7: FutureWarning: Setting an item of incompatible dtype is deprecated and will raise in a future error of pandas. Value 'I am very organized.' has dtype incompatible with int64, please explicitly cast to a compatible dtype first.\n",
      "  mapped_row[key] = expanded_scale[key][index]  # Replace with corresponding string\n",
      "/var/folders/zh/79y5ykz51rxcnjchzb73nfxw0000gn/T/ipykernel_33913/3440998859.py:7: FutureWarning: Setting an item of incompatible dtype is deprecated and will raise in a future error of pandas. Value 'I am neither particularly organized nor disorganized.' has dtype incompatible with int64, please explicitly cast to a compatible dtype first.\n",
      "  mapped_row[key] = expanded_scale[key][index]  # Replace with corresponding string\n",
      "/var/folders/zh/79y5ykz51rxcnjchzb73nfxw0000gn/T/ipykernel_33913/3440998859.py:7: FutureWarning: Setting an item of incompatible dtype is deprecated and will raise in a future error of pandas. Value 'I am very disorganized.' has dtype incompatible with int64, please explicitly cast to a compatible dtype first.\n",
      "  mapped_row[key] = expanded_scale[key][index]  # Replace with corresponding string\n"
     ]
    }
   ],
   "execution_count": 5
  },
  {
   "cell_type": "code",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-06-23T14:12:32.109366Z",
     "start_time": "2025-06-23T14:12:32.105494Z"
    }
   },
   "source": [
    "# Preview a personality description\n",
    "print(\"Sample personality description:\")\n",
    "print(data.iloc[0]['combined_bfi2'][:500] + \"...\")"
   ],
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Sample personality description:\n",
      "I am very outgoing, sociable. I am very compassionate almost always soft-hearted. I am fairly organized. I am somewhat relaxed handle stress somewhat well. I have some artistic interests. I am quite assertive. I am very respectful almost always treat others with respect. I am often lazy. I stay very optimistic after experiencing a setback. I am curious about few things. I often feel excited or eager. I rarely find fault with others. I am very dependable steady. I am fairly moody often have up an...\n"
     ]
    }
   ],
   "execution_count": 6
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Multi-Model Simulation Configuration"
   ]
  },
  {
   "cell_type": "code",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-06-23T14:13:50.148786Z",
     "start_time": "2025-06-23T14:13:50.065233Z"
    }
   },
   "source": [
    "# Configuration for different models and temperatures\n",
    "models_to_test = ['gpt-4', 'gpt-4o', 'llama', 'deepseek']\n",
    "temperatures = [0.0, 1.0]  # Test both deterministic and stochastic responses\n",
    "batch_size = 25  # Smaller batch size for stability across different APIs\n",
    "\n",
    "# Create participant data list from DataFrame\n",
    "participants_data = data.to_dict('records')\n",
    "print(f\"Prepared {len(participants_data)} participants for simulation\")\n",
    "\n",
    "\n",
    "## temp for testing \n",
    "models_to_test = ['deepseek']\n",
    "## take the first 30 participants for testing\n",
    "participants_data = participants_data[:30]"
   ],
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Prepared 438 participants for simulation\n"
     ]
    }
   ],
   "execution_count": 8
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Run Simulations for All Models"
   ]
  },
  {
   "cell_type": "code",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-06-23T14:15:28.111416Z",
     "start_time": "2025-06-23T14:13:53.305878Z"
    }
   },
   "source": [
    "# Run simulations for all model-temperature combinations\n",
    "all_results = {}\n",
    "\n",
    "for model in models_to_test:\n",
    "    for temperature in temperatures:\n",
    "        print(f\"\\n{'='*60}\")\n",
    "        print(f\"Starting simulation: {model} with temperature {temperature}\")\n",
    "        print(f\"{'='*60}\")\n",
    "        \n",
    "        config = SimulationConfig(\n",
    "            model=model,\n",
    "            temperature=temperature,\n",
    "            batch_size=batch_size,\n",
    "            max_workers=10\n",
    "        )\n",
    "        \n",
    "        try:\n",
    "            results = run_bfi_to_minimarker_simulation(\n",
    "                participants_data=participants_data,\n",
    "                config=config,\n",
    "                output_dir=\"study_2_results\"\n",
    "            )\n",
    "            \n",
    "            # Store results\n",
    "            key = f\"{model}_temp{temperature}\"\n",
    "            all_results[key] = results\n",
    "            \n",
    "            # Check for any failed participants\n",
    "            failed_count = sum(1 for r in results if isinstance(r, dict) and 'error' in r)\n",
    "            if failed_count > 0:\n",
    "                print(f\"Warning: {failed_count} participants failed. Consider retrying.\")\n",
    "                \n",
    "            print(f\"Completed simulation: {model} with temperature {temperature}\")\n",
    "            \n",
    "        except Exception as e:\n",
    "            print(f\"Error in simulation {model} temp {temperature}: {str(e)}\")\n",
    "            all_results[f\"{model}_temp{temperature}\"] = {\"error\": str(e)}\n",
    "\n",
    "print(f\"\\nCompleted all simulations. Results keys: {list(all_results.keys())}\")"
   ],
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "============================================================\n",
      "Starting simulation: deepseek with temperature 0.0\n",
      "============================================================\n",
      "Starting simulation for 30 participants using deepseek\n",
      "Temperature: 0.0, Batch size: 25\n",
      "Processing participants 0 to 24\n",
      "Completed batch 0 to 24\n",
      "Processing participants 25 to 29\n",
      "Completed batch 25 to 29\n",
      "Results saved to study_2_results/bfi_to_minimarker_deepseek_temp0_0.json\n",
      "Completed simulation: deepseek with temperature 0.0\n",
      "\n",
      "============================================================\n",
      "Starting simulation: deepseek with temperature 1.0\n",
      "============================================================\n",
      "Starting simulation for 30 participants using deepseek\n",
      "Temperature: 1.0, Batch size: 25\n",
      "Processing participants 0 to 24\n",
      "Completed batch 0 to 24\n",
      "Processing participants 25 to 29\n",
      "Completed batch 25 to 29\n",
      "Results saved to study_2_results/bfi_to_minimarker_deepseek_temp1_0.json\n",
      "Completed simulation: deepseek with temperature 1.0\n",
      "\n",
      "Completed all simulations. Results keys: ['deepseek_temp0.0', 'deepseek_temp1.0']\n"
     ]
    }
   ],
   "execution_count": 9
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Retry Failed Participants (if any)"
   ]
  },
  {
   "cell_type": "code",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-06-23T14:15:52.437156Z",
     "start_time": "2025-06-23T14:15:52.433069Z"
    }
   },
   "source": "# Retry any failed participants\nfor key, results in all_results.items():\n    if isinstance(results, list):\n        failed_count = sum(1 for r in results if isinstance(r, dict) and 'error' in r)\n        if failed_count > 0:\n            print(f\"Retrying {failed_count} failed participants for {key}\")\n            \n            # Extract model and temperature from key\n            model = key.split('_temp')[0]\n            temperature = float(key.split('_temp')[1])\n            \n            config = SimulationConfig(\n                model=model,\n                temperature=temperature,\n                batch_size=batch_size\n            )\n            \n            updated_results = retry_failed_participants(\n                results=results,\n                participants_data=participants_data,\n                prompt_generator=get_prompt,  # Use imported get_prompt function\n                config=config,\n                personality_key='combined_bfi2'\n            )\n            \n            all_results[key] = updated_results\n            \n            # Save updated results\n            from simulation_utils import save_simulation_results\n            save_simulation_results(updated_results, \"study_2_results\", \"bfi_to_minimarker_retried\", config)\n\nprint(\"Retry process completed\")",
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Retry process completed\n"
     ]
    }
   ],
   "execution_count": 11
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Results Summary"
   ]
  },
  {
   "cell_type": "code",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-06-23T14:16:05.860157Z",
     "start_time": "2025-06-23T14:16:05.855753Z"
    }
   },
   "source": [
    "# Analyze results summary\n",
    "print(\"Simulation Results Summary:\")\n",
    "print(\"=\" * 50)\n",
    "\n",
    "for key, results in all_results.items():\n",
    "    if isinstance(results, list):\n",
    "        total_participants = len(results)\n",
    "        successful = sum(1 for r in results if not (isinstance(r, dict) and 'error' in r))\n",
    "        failed = total_participants - successful\n",
    "        success_rate = (successful / total_participants) * 100\n",
    "        \n",
    "        print(f\"{key}:\")\n",
    "        print(f\"  Total: {total_participants}, Successful: {successful}, Failed: {failed}\")\n",
    "        print(f\"  Success Rate: {success_rate:.1f}%\")\n",
    "        print()\n",
    "    else:\n",
    "        print(f\"{key}: FAILED - {results.get('error', 'Unknown error')}\")\n",
    "        print()"
   ],
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Simulation Results Summary:\n",
      "==================================================\n",
      "deepseek_temp0.0:\n",
      "  Total: 30, Successful: 30, Failed: 0\n",
      "  Success Rate: 100.0%\n",
      "\n",
      "deepseek_temp1.0:\n",
      "  Total: 30, Successful: 30, Failed: 0\n",
      "  Success Rate: 100.0%\n",
      "\n"
     ]
    }
   ],
   "execution_count": 12
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Save Preprocessed Data"
   ]
  },
  {
   "cell_type": "code",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-06-23T14:16:14.518181Z",
     "start_time": "2025-06-23T14:16:14.395356Z"
    }
   },
   "source": [
    "# Save the preprocessed data for reference\n",
    "output_path = Path('study_2_results')\n",
    "output_path.mkdir(exist_ok=True)\n",
    "\n",
    "data.to_csv(output_path / 'study2_preprocessed_data.csv', index=False)\n",
    "print(f\"Preprocessed data saved to {output_path / 'study2_preprocessed_data.csv'}\")"
   ],
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Preprocessed data saved to study_2_results/study2_preprocessed_data.csv\n"
     ]
    }
   ],
   "execution_count": 13
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": "# Multi-Model Results Analysis\n\n## Overview\nThis section provides comprehensive analysis tools for comparing results across multiple LLM models. The analysis includes:\n\n1. **Data Processing**: Convert JSON results to structured DataFrames\n2. **Descriptive Statistics**: Compare response patterns across models and temperatures\n3. **Correlation Analysis**: Assess agreement between models and with empirical data\n4. **Visualization**: Create comprehensive plots for model comparison\n5. **Reliability Assessment**: Evaluate consistency within and across models\n6. **Psychometric Validation**: Test personality structure preservation\n\n## Usage\n- Run all previous cells to generate simulation results\n- Results should be available in `all_results` dictionary\n- Empirical Mini-Marker data available in original dataset (TDA columns)"
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-06-26T09:03:51.176361Z",
     "start_time": "2025-06-26T09:03:50.796058Z"
    }
   },
   "cell_type": "code",
   "source": [
    "## 7. Export Results for Further Analysis\n",
    "\n",
    "def export_results_for_analysis(results_df, summary_df, trait_stats_df, \n",
    "                               corr_matrix, validation_results):\n",
    "    \"\"\"Export all analysis results in formats suitable for further research.\"\"\"\n",
    "    \n",
    "    output_dir = Path('study_2_results')\n",
    "    output_dir.mkdir(exist_ok=True)\n",
    "    \n",
    "    print(\"Exporting analysis results...\")\n",
    "    \n",
    "    # 1. Export main results DataFrames\n",
    "    if not results_df.empty:\n",
    "        results_df.to_csv(output_dir / 'results_long_format.csv', index=False)\n",
    "        print(f\"✓ Long format results: {len(results_df)} rows\")\n",
    "    \n",
    "    if not summary_df.empty:\n",
    "        summary_df.to_csv(output_dir / 'results_wide_format.csv', index=False)\n",
    "        print(f\"✓ Wide format results: {len(summary_df)} rows\")\n",
    "    \n",
    "    if not trait_stats_df.empty:\n",
    "        trait_stats_df.to_csv(output_dir / 'trait_statistics.csv', index=False)\n",
    "        print(f\"✓ Trait statistics: {len(trait_stats_df)} rows\")\n",
    "    \n",
    "    # 2. Export correlation matrices\n",
    "    if corr_matrix is not None and not corr_matrix.empty:\n",
    "        corr_matrix.to_csv(output_dir / 'inter_model_correlations.csv')\n",
    "        print(f\"✓ Correlation matrix: {corr_matrix.shape}\")\n",
    "    \n",
    "    # 3. Export validation results\n",
    "    if validation_results:\n",
    "        # Flatten validation results for CSV export\n",
    "        validation_flat = []\n",
    "        for model_key, validation in validation_results.items():\n",
    "            base_record = {\n",
    "                'model_temp': model_key,\n",
    "                'mean_correlation': validation['mean_correlation'],\n",
    "                'median_correlation': validation['median_correlation'],\n",
    "                'std_correlation': validation['std_correlation'],\n",
    "                'n_traits': validation['n_traits']\n",
    "            }\n",
    "            \n",
    "            # Add trait-level details\n",
    "            for trait, details in validation['trait_details'].items():\n",
    "                record = base_record.copy()\n",
    "                record.update({\n",
    "                    'trait': trait,\n",
    "                    'trait_correlation': details['correlation'],\n",
    "                    'trait_p_value': details['p_value'],\n",
    "                    'trait_n_participants': details['n_participants'],\n",
    "                    'empirical_mean': details['empirical_mean'],\n",
    "                    'llm_mean': details['llm_mean'],\n",
    "                    'empirical_std': details['empirical_std'],\n",
    "                    'llm_std': details['llm_std']\n",
    "                })\n",
    "                validation_flat.append(record)\n",
    "        \n",
    "        validation_df = pd.DataFrame(validation_flat)\n",
    "        validation_df.to_csv(output_dir / 'empirical_validation_details.csv', index=False)\n",
    "        print(f\"✓ Validation details: {len(validation_df)} rows\")\n",
    "    \n",
    "    # 4. Create analysis summary for R/other tools\n",
    "    import json\n",
    "    summary_stats = {\n",
    "        'analysis_timestamp': pd.Timestamp.now().isoformat(),\n",
    "        'n_participants': len(summary_df) if not summary_df.empty else 0,\n",
    "        'n_models': len(results_df['model'].unique()) if not results_df.empty else 0,\n",
    "        'n_temperatures': len(results_df['temperature'].unique()) if not results_df.empty else 0,\n",
    "        'n_traits': len([col for col in results_df.columns if col not in ['participant_id', 'model', 'temperature']]) if not results_df.empty else 0\n",
    "    }\n",
    "    \n",
    "    with open(output_dir / 'analysis_metadata.json', 'w') as f:\n",
    "        json.dump(summary_stats, f, indent=2)\n",
    "    print(f\"✓ Analysis metadata saved\")\n",
    "    \n",
    "    # 5. Create R-ready format\n",
    "    if not results_df.empty:\n",
    "        # Reshape for R analysis\n",
    "        r_format = results_df.melt(id_vars=['participant_id', 'model', 'temperature'],\n",
    "                                  var_name='trait', value_name='response')\n",
    "        r_format.to_csv(output_dir / 'results_r_format.csv', index=False)\n",
    "        print(f\"✓ R-ready format: {len(r_format)} rows\")\n",
    "    \n",
    "    print(f\"\\n📁 All results exported to: {output_dir}/\")\n",
    "    print(\"Files available for further analysis:\")\n",
    "    for file in sorted(output_dir.glob('*.csv')):\n",
    "        print(f\"  - {file.name}\")\n",
    "    for file in sorted(output_dir.glob('*.json')):\n",
    "        print(f\"  - {file.name}\")\n",
    "\n",
    "# Export all results\n",
    "export_results_for_analysis(results_df, summary_df, trait_stats_df, \n",
    "                           corr_matrix, validation_results)\n",
    "\n",
    "print(\"\\n🎉 MULTI-MODEL ANALYSIS COMPLETE! 🎉\")\n",
    "print(\"\\nNext steps:\")\n",
    "print(\"1. Review the comprehensive report: study_2_results/comprehensive_analysis_report.md\")\n",
    "print(\"2. Examine visualizations: study_2_results/multi_model_analysis.png\")\n",
    "print(\"3. Use exported CSV files for statistical analysis in R or Python\")\n",
    "print(\"4. Scale up to full dataset with all models when satisfied with results\")\n",
    "print(\"5. Compare with Studies 3 and 4 using the same analysis framework\")"
   ],
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'results_df' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001B[0;31m---------------------------------------------------------------------------\u001B[0m",
      "\u001B[0;31mNameError\u001B[0m                                 Traceback (most recent call last)",
      "Cell \u001B[0;32mIn[2], line 92\u001B[0m\n\u001B[1;32m     89\u001B[0m         \u001B[38;5;28mprint\u001B[39m(\u001B[38;5;124mf\u001B[39m\u001B[38;5;124m\"\u001B[39m\u001B[38;5;124m  - \u001B[39m\u001B[38;5;132;01m{\u001B[39;00mfile\u001B[38;5;241m.\u001B[39mname\u001B[38;5;132;01m}\u001B[39;00m\u001B[38;5;124m\"\u001B[39m)\n\u001B[1;32m     91\u001B[0m \u001B[38;5;66;03m# Export all results\u001B[39;00m\n\u001B[0;32m---> 92\u001B[0m export_results_for_analysis(\u001B[43mresults_df\u001B[49m, summary_df, trait_stats_df, \n\u001B[1;32m     93\u001B[0m                            corr_matrix, validation_results)\n\u001B[1;32m     95\u001B[0m \u001B[38;5;28mprint\u001B[39m(\u001B[38;5;124m\"\u001B[39m\u001B[38;5;130;01m\\n\u001B[39;00m\u001B[38;5;124m🎉 MULTI-MODEL ANALYSIS COMPLETE! 🎉\u001B[39m\u001B[38;5;124m\"\u001B[39m)\n\u001B[1;32m     96\u001B[0m \u001B[38;5;28mprint\u001B[39m(\u001B[38;5;124m\"\u001B[39m\u001B[38;5;130;01m\\n\u001B[39;00m\u001B[38;5;124mNext steps:\u001B[39m\u001B[38;5;124m\"\u001B[39m)\n",
      "\u001B[0;31mNameError\u001B[0m: name 'results_df' is not defined"
     ]
    }
   ],
   "execution_count": 2
  },
  {
   "metadata": {},
   "cell_type": "code",
   "outputs": [],
   "execution_count": null,
   "source": [
    "## 6. Comprehensive Report Generation\n",
    "\n",
    "def generate_comprehensive_report(all_results, results_df, trait_stats_df, \n",
    "                                 corr_matrix, temp_consistency, trait_agreement,\n",
    "                                 validation_results, participants_data):\n",
    "    \\\"\\\"\\\"Generate a comprehensive analysis report.\\\"\\\"\\\"\n",
    "    \n",
    "    report = []\\n    report.append(\\\"# Multi-Model Personality Simulation Analysis Report\\\")\\n    report.append(f\\\"Generated: {pd.Timestamp.now().strftime('%Y-%m-%d %H:%M:%S')}\\\")\\n    report.append(\\\"\\\\n\\\" + \\\"=\\\"*60)\\n    \\n    # Executive Summary\\n    report.append(\\\"\\\\n## Executive Summary\\\")\\n    report.append(f\\\"- **Participants Analyzed**: {len(participants_data)}\\\")\\n    report.append(f\\\"- **Models Tested**: {list(results_df['model'].unique()) if not results_df.empty else 'None'}\\\")\\n    report.append(f\\\"- **Temperature Settings**: {list(results_df['temperature'].unique()) if not results_df.empty else 'None'}\\\")\\n    \\n    if not results_df.empty:\\n        total_responses = len(results_df)\\n        successful_responses = len(results_df[results_df.notna().any(axis=1)])\\n        success_rate = (successful_responses / total_responses) * 100 if total_responses > 0 else 0\\n        report.append(f\\\"- **Overall Success Rate**: {success_rate:.1f}%\\\")\\n    \\n    # Model Performance Summary\\n    if not trait_stats_df.empty:\\n        report.append(\\\"\\\\n## Model Performance Summary\\\")\\n        \\n        performance_summary = trait_stats_df.groupby(['model', 'temperature']).agg({\\n            'count': 'mean',\\n            'mean': ['mean', 'std'],\\n            'std': 'mean'\\n        }).round(3)\\n        \\n        report.append(\\\"\\\\n### Response Completeness and Quality\\\")\\n        for (model, temp), group in trait_stats_df.groupby(['model', 'temperature']):\\n            avg_completeness = group['count'].mean() / len(participants_data)\\n            avg_response = group['mean'].mean()\\n            avg_variability = group['std'].mean()\\n            \\n            report.append(f\\\"- **{model} (temp={temp})**: {avg_completeness:.2f} completeness, \\\"\\n                         f\\\"{avg_response:.2f} avg response, {avg_variability:.2f} variability\\\")\\n    \\n    # Temperature Consistency\\n    if temp_consistency is not None and not temp_consistency.empty:\\n        report.append(\\\"\\\\n### Temperature Consistency\\\")\\n        for model, stats in temp_consistency.iterrows():\\n            report.append(f\\\"- **{model}**: r = {stats['mean_consistency']:.3f} \\\"\\n                         f\\\"(±{stats['std_consistency']:.3f}, n={stats['n_traits']})\\\")\\n    \\n    # Inter-Model Agreement\\n    if corr_matrix is not None and not corr_matrix.empty:\\n        report.append(\\\"\\\\n### Inter-Model Agreement\\\")\\n        \\n        # Calculate average correlations (excluding diagonal)\\n        avg_correlations = []\\n        for i, col1 in enumerate(corr_matrix.columns):\\n            for j, col2 in enumerate(corr_matrix.columns):\\n                if i != j and not pd.isna(corr_matrix.loc[col1, col2]):\\n                    avg_correlations.append(corr_matrix.loc[col1, col2])\\n        \\n        if avg_correlations:\\n            overall_agreement = np.mean(avg_correlations)\\n            report.append(f\\\"- **Overall Inter-Model Agreement**: r = {overall_agreement:.3f}\\\")\\n            \\n            # Find best and worst agreements\\n            max_corr = np.max(avg_correlations)\\n            min_corr = np.min(avg_correlations)\\n            report.append(f\\\"- **Range**: {min_corr:.3f} to {max_corr:.3f}\\\")\\n    \\n    # Empirical Validation\\n    if validation_results:\\n        report.append(\\\"\\\\n## Empirical Validation Results\\\")\\n        \\n        best_model = None\\n        best_correlation = -1\\n        \\n        for model_key, validation in validation_results.items():\\n            mean_corr = validation['mean_correlation']\\n            n_traits = validation['n_traits']\\n            \\n            report.append(f\\\"- **{model_key}**: r = {mean_corr:.3f} \\\"\\n                         f\\\"(across {n_traits} traits)\\\")\\n            \\n            if mean_corr > best_correlation:\\n                best_correlation = mean_corr\\n                best_model = model_key\\n        \\n        if best_model:\\n            report.append(f\\\"\\\\n**Best Empirical Match**: {best_model} (r = {best_correlation:.3f})\\\")\\n    \\n    # Trait-Level Insights\\n    if trait_agreement is not None and not trait_agreement.empty:\\n        report.append(\\\"\\\\n## Trait-Level Analysis\\\")\\n        \\n        # Most reliable traits\\n        top_traits = trait_agreement.sort_values('mean_agreement', ascending=False).head(5)\\n        report.append(\\\"\\\\n### Most Reliable Traits (High Inter-Model Agreement):\\\")\\n        for trait, stats in top_traits.iterrows():\\n            report.append(f\\\"- **{trait}**: r = {stats['mean_agreement']:.3f} \\\"\\n                         f\\\"(±{stats['std_agreement']:.3f})\\\")\\n        \\n        # Most variable traits\\n        bottom_traits = trait_agreement.sort_values('mean_agreement', ascending=True).head(5)\\n        report.append(\\\"\\\\n### Most Variable Traits (Low Inter-Model Agreement):\\\")\\n        for trait, stats in bottom_traits.iterrows():\\n            report.append(f\\\"- **{trait}**: r = {stats['mean_agreement']:.3f} \\\"\\n                         f\\\"(±{stats['std_agreement']:.3f})\\\")\\n    \\n    # Recommendations\\n    report.append(\\\"\\\\n## Recommendations\\\")\\n    \\n    if not results_df.empty:\\n        models = results_df['model'].unique()\\n        if len(models) > 1:\\n            report.append(\\\"\\\\n### Model Selection:\\\")\\n            \\n            # Based on empirical validation\\n            if validation_results:\\n                best_empirical = max(validation_results.keys(), \\n                                   key=lambda x: validation_results[x]['mean_correlation'])\\n                report.append(f\\\"- For **empirical accuracy**: Use {best_empirical}\\\")\\n            \\n            # Based on consistency\\n            if temp_consistency is not None and not temp_consistency.empty:\\n                most_consistent = temp_consistency['mean_consistency'].idxmax()\\n                report.append(f\\\"- For **consistency**: Use {most_consistent}\\\")\\n        \\n        # Temperature recommendations\\n        if len(results_df['temperature'].unique()) > 1:\\n            report.append(\\\"\\\\n### Temperature Settings:\\\")\\n            report.append(\\\"- Use **temperature = 0.0** for deterministic, consistent responses\\\")\\n            report.append(\\\"- Use **temperature = 1.0** for diverse, creative responses\\\")\\n            report.append(\\\"- Consider ensemble approaches combining both temperatures\\\")\\n    \\n    # Technical Notes\\n    report.append(\\\"\\\\n## Technical Notes\\\")\\n    report.append(\\\"- All correlations computed using Pearson's correlation coefficient\\\")\\n    report.append(\\\"- Statistical significance testing performed where applicable\\\")\\n    report.append(\\\"- Missing data handled through pairwise deletion\\\")\\n    report.append(\\\"- Results based on Mini-Marker 40-item personality assessment\\\")\\n    \\n    # Limitations\\n    report.append(\\\"\\\\n## Limitations\\\")\\n    report.append(f\\\"- Analysis based on {len(participants_data)} participants (subset for testing)\\\")\\n    report.append(\\\"- Results may vary with different personality descriptions\\\")\\n    report.append(\\\"- Model performance may depend on prompt engineering\\\")\\n    report.append(\\\"- Cross-cultural validity not assessed\\\")\\n    \\n    return \\\"\\\\n\\\".join(report)\\n\\n# Generate comprehensive report\\nfinal_report = generate_comprehensive_report(\\n    all_results, results_df, trait_stats_df, \\n    corr_matrix, temp_consistency, trait_agreement,\\n    validation_results, participants_data\\n)\\n\\nprint(final_report)\\n\\n# Save report to file\\nwith open('study_2_results/comprehensive_analysis_report.md', 'w') as f:\\n    f.write(final_report)\\n\\nprint(\\\"\\\\n\\\" + \\\"=\\\"*60)\\nprint(\\\"ANALYSIS COMPLETE\\\")\\nprint(\\\"Report saved to: study_2_results/comprehensive_analysis_report.md\\\")\\nprint(\\\"Visualizations saved to: study_2_results/multi_model_analysis.png\\\")\\nprint(\\\"=\\\"*60)"
   ]
  },
  {
   "cell_type": "code",
   "source": "## 5. Empirical Validation Against Human Data\n\ndef validate_against_empirical_data(results_df, data, participants_data):\n    \\\"\\\"\\\"Compare LLM responses with empirical human Mini-Marker data.\\\"\\\"\\\"\n    \n    if results_df.empty:\n        print(\"No LLM results for empirical validation\")\n        return None\n    \n    # TDA columns represent empirical Mini-Marker responses (1-9 scale)\n    tda_columns = [f\"tda{i}\" for i in range(1, 41)]\n    \n    # Check if we have empirical data\n    available_tda = [col for col in tda_columns if col in data.columns]\n    if not available_tda:\n        print(\"No empirical TDA (Mini-Marker) data available for validation\")\n        return None\n    \n    print(f\"=== EMPIRICAL VALIDATION ===\")\n    print(f\"Available empirical data: {len(available_tda)} Mini-Marker traits\")\n    \n    # Mini-Marker trait names corresponding to TDA columns\n    trait_names = [\n        'Bashful', 'Bold', 'Careless', 'Cold', 'Complex', 'Cooperative', 'Creative', 'Deep',\n        'Disorganized', 'Efficient', 'Energetic', 'Envious', 'Extraverted', 'Fretful', 'Harsh',\n        'Imaginative', 'Inefficient', 'Intellectual', 'Jealous', 'Kind', 'Moody', 'Organized',\n        'Philosophical', 'Practical', 'Quiet', 'Relaxed', 'Rude', 'Shy', 'Sloppy', 'Sympathetic',\n        'Systematic', 'Talkative', 'Temperamental', 'Touchy', 'Uncreative', 'Unenvious',\n        'Unintellectual', 'Unsympathetic', 'Warm', 'Withdrawn'\n    ]\n    \n    # Get empirical data for participants used in simulation\n    empirical_subset = data.iloc[:len(participants_data)][available_tda]\n    \n    validation_results = {}\n    \n    # For each model-temperature combination\n    for model in results_df['model'].unique():\n        for temp in results_df['temperature'].unique():\n            subset = results_df[(results_df['model'] == model) & (results_df['temperature'] == temp)]\n            \n            if subset.empty:\n                continue\n                \n            key = f'{model}_temp{temp}'\n            trait_correlations = []\n            trait_details = {}\n            \n            # Compare each trait\n            for i, trait in enumerate(trait_names[:len(available_tda)]):\n                if trait in subset.columns:\n                    # Get LLM responses\n                    llm_responses = subset[['participant_id', trait]].dropna()\n                    \n                    # Get corresponding empirical data\n                    empirical_values = []\n                    llm_values = []\n                    \n                    for _, row in llm_responses.iterrows():\n                        participant_id = int(row['participant_id'])\n                        if participant_id < len(empirical_subset):\n                            emp_value = empirical_subset.iloc[participant_id, i]\n                            if pd.notna(emp_value) and pd.notna(row[trait]):\n                                empirical_values.append(emp_value)\n                                llm_values.append(row[trait])\n                    \n                    # Calculate correlation if we have enough data\n                    if len(empirical_values) > 2:\n                        corr, p_value = pearsonr(empirical_values, llm_values)\n                        \n                        if not np.isnan(corr):\n                            trait_correlations.append(corr)\n                            trait_details[trait] = {\n                                'correlation': corr,\n                                'p_value': p_value,\n                                'n_participants': len(empirical_values),\n                                'empirical_mean': np.mean(empirical_values),\n                                'llm_mean': np.mean(llm_values),\n                                'empirical_std': np.std(empirical_values),\n                                'llm_std': np.std(llm_values)\n                            }\n            \n            # Store validation results\n            if trait_correlations:\n                validation_results[key] = {\n                    'mean_correlation': np.mean(trait_correlations),\n                    'median_correlation': np.median(trait_correlations),\n                    'std_correlation': np.std(trait_correlations),\n                    'n_traits': len(trait_correlations),\n                    'trait_details': trait_details\n                }\n    \n    # Display results\n    validation_df = pd.DataFrame({k: {\n        'mean_corr': v['mean_correlation'],\n        'median_corr': v['median_correlation'],\n        'std_corr': v['std_correlation'],\n        'n_traits': v['n_traits']\n    } for k, v in validation_results.items()}).T\n    \n    if not validation_df.empty:\n        print(\"\\\\nValidation against empirical data:\")\n        print(validation_df.round(3))\n        \n        # Find best performing model\n        best_model = validation_df['mean_corr'].idxmax()\n        print(f\"\\\\nBest performing model: {best_model} (r = {validation_df.loc[best_model, 'mean_corr']:.3f})\\\")\\n        \n        # Show trait-level details for best model\n        if best_model in validation_results:\n            print(f\\\"\\\\nTrait-level correlations for {best_model}:\\\")\\n            trait_details = validation_results[best_model]['trait_details']\\n            trait_corr_df = pd.DataFrame({\\n                trait: {\\n                    'correlation': details['correlation'],\\n                    'p_value': details['p_value'],\\n                    'n_participants': details['n_participants']\\n                } for trait, details in trait_details.items()\\n            }).T\\n            \\n            # Sort by correlation strength\\n            trait_corr_sorted = trait_corr_df.sort_values('correlation', ascending=False)\\n            print(trait_corr_sorted.head(10).round(3))\\n    \\n    return validation_results, validation_df\\n\\n# Run empirical validation\\nvalidation_results, validation_summary = validate_against_empirical_data(results_df, data, participants_data)",
   "metadata": {},
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "code",
   "source": "## 4. Correlation Analysis and Model Agreement\n\ndef analyze_model_correlations(results_df, summary_df):\n    \\\"\\\"\\\"Comprehensive correlation analysis between models and with empirical data.\\\"\\\"\\\"\n    \n    if results_df.empty:\n        print(\"No results for correlation analysis\")\n        return None\n    \n    trait_names = [col for col in results_df.columns if col not in ['participant_id', 'model', 'temperature']]\n    \n    print(\"=== MODEL CORRELATION ANALYSIS ===\")\n    \n    # 1. Inter-model correlations\n    model_combinations = []\n    correlations_matrix = {}\n    \n    models = results_df['model'].unique()\n    temps = results_df['temperature'].unique()\n    \n    for model1 in models:\n        for temp1 in temps:\n            key1 = f'{model1}_temp{temp1}'\n            subset1 = results_df[(results_df['model'] == model1) & (results_df['temperature'] == temp1)]\n            \n            if subset1.empty:\n                continue\n                \n            correlations_matrix[key1] = {}\n            \n            for model2 in models:\n                for temp2 in temps:\n                    key2 = f'{model2}_temp{temp2}'\n                    subset2 = results_df[(results_df['model'] == model2) & (results_df['temperature'] == temp2)]\n                    \n                    if subset2.empty:\n                        correlations_matrix[key1][key2] = np.nan\n                        continue\n                    \n                    # Calculate correlations for each trait\n                    trait_correlations = []\n                    for trait in trait_names:\n                        if trait in subset1.columns and trait in subset2.columns:\n                            # Merge on participant_id to align responses\n                            merged = pd.merge(subset1[['participant_id', trait]], \n                                            subset2[['participant_id', trait]], \n                                            on='participant_id', suffixes=('_1', '_2'))\n                            \n                            if len(merged) > 1:\n                                # Remove rows with NaN values\n                                clean_data = merged.dropna()\n                                if len(clean_data) > 1:\n                                    corr, _ = pearsonr(clean_data[f'{trait}_1'], clean_data[f'{trait}_2'])\n                                    if not np.isnan(corr):\n                                        trait_correlations.append(corr)\n                    \n                    # Average correlation across traits\n                    if trait_correlations:\n                        avg_correlation = np.mean(trait_correlations)\n                        correlations_matrix[key1][key2] = avg_correlation\n                    else:\n                        correlations_matrix[key1][key2] = np.nan\n    \n    # Convert to DataFrame for visualization\n    corr_df = pd.DataFrame(correlations_matrix)\n    \n    print(f\"Inter-model correlation matrix ({len(corr_df)}x{len(corr_df.columns)}):\")\n    print(corr_df.round(3))\n    \n    # 2. Temperature consistency within models\n    print(\"\\\\n=== TEMPERATURE CONSISTENCY ===\")\n    temp_consistency = {}\n    \n    for model in models:\n        temp0_data = results_df[(results_df['model'] == model) & (results_df['temperature'] == '0.0')]\n        temp1_data = results_df[(results_df['model'] == model) & (results_df['temperature'] == '1.0')]\n        \n        if not temp0_data.empty and not temp1_data.empty:\n            trait_consistencies = []\n            \n            for trait in trait_names:\n                merged = pd.merge(temp0_data[['participant_id', trait]], \n                                temp1_data[['participant_id', trait]], \n                                on='participant_id', suffixes=('_t0', '_t1'))\n                \n                clean_data = merged.dropna()\n                if len(clean_data) > 1:\n                    corr, _ = pearsonr(clean_data[f'{trait}_t0'], clean_data[f'{trait}_t1'])\n                    if not np.isnan(corr):\n                        trait_consistencies.append(corr)\n            \n            if trait_consistencies:\n                temp_consistency[model] = {\n                    'mean_consistency': np.mean(trait_consistencies),\n                    'std_consistency': np.std(trait_consistencies),\n                    'n_traits': len(trait_consistencies)\n                }\n    \n    consistency_df = pd.DataFrame(temp_consistency).T\n    if not consistency_df.empty:\n        print(\"Temperature consistency by model:\")\n        print(consistency_df.round(3))\n    \n    # 3. Trait-level analysis\n    print(\"\\\\n=== TRAIT-LEVEL AGREEMENT ===\")\n    trait_agreement = {}\n    \n    for trait in trait_names[:10]:  # Analyze first 10 traits\n        trait_corrs = []\n        \n        # Get all model-temperature combinations for this trait\n        trait_data = {}\n        for model in models:\n            for temp in temps:\n                subset = results_df[(results_df['model'] == model) & (results_df['temperature'] == temp)]\n                if not subset.empty and trait in subset.columns:\n                    trait_data[f'{model}_t{temp}'] = subset[['participant_id', trait]].dropna()\n        \n        # Calculate pairwise correlations\n        combinations = list(trait_data.keys())\n        for i in range(len(combinations)):\n            for j in range(i+1, len(combinations)):\n                data1 = trait_data[combinations[i]]\n                data2 = trait_data[combinations[j]]\n                \n                merged = pd.merge(data1, data2, on='participant_id', suffixes=('_1', '_2'))\n                if len(merged) > 1:\n                    corr, _ = pearsonr(merged[f'{trait}_1'], merged[f'{trait}_2'])\n                    if not np.isnan(corr):\n                        trait_corrs.append(corr)\n        \n        if trait_corrs:\n            trait_agreement[trait] = {\n                'mean_agreement': np.mean(trait_corrs),\n                'std_agreement': np.std(trait_corrs),\n                'n_comparisons': len(trait_corrs)\n            }\n    \n    trait_agreement_df = pd.DataFrame(trait_agreement).T\n    if not trait_agreement_df.empty:\n        print(\"Top traits by inter-model agreement:\")\n        sorted_traits = trait_agreement_df.sort_values('mean_agreement', ascending=False)\n        print(sorted_traits.head().round(3))\n    \n    return corr_df, consistency_df, trait_agreement_df\n\n# Run correlation analysis\ncorr_matrix, temp_consistency, trait_agreement = analyze_model_correlations(results_df, summary_df)",
   "metadata": {},
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "code",
   "source": "## 3. Visualization Suite\n\ndef create_comprehensive_plots(results_df, trait_stats_df, summary_df):\n    \\\"\\\"\\\"Create a comprehensive set of visualization plots.\\\"\\\"\\\"\n    \n    if results_df.empty:\n        print(\"No data to plot\")\n        return\n    \n    # Set up the plotting layout\n    fig = plt.figure(figsize=(20, 16))\n    \n    # Get trait names for plotting\n    trait_names = [col for col in results_df.columns if col not in ['participant_id', 'model', 'temperature']]\n    \n    # 1. Response Distribution Comparison\n    plt.subplot(3, 3, 1)\n    if not results_df.empty:\n        for model in results_df['model'].unique():\n            for temp in results_df['temperature'].unique():\n                subset = results_df[(results_df['model'] == model) & (results_df['temperature'] == temp)]\n                if not subset.empty:\n                    # Calculate mean response across all traits for each participant\n                    trait_means = subset[trait_names].mean(axis=1)\n                    plt.hist(trait_means, alpha=0.6, bins=20, \n                            label=f'{model}_temp{temp}', density=True)\n    \n    plt.xlabel('Mean Trait Response')\n    plt.ylabel('Density')\n    plt.title('Distribution of Mean Responses by Model')\n    plt.legend()\n    plt.grid(True, alpha=0.3)\n    \n    # 2. Response Range Comparison\n    plt.subplot(3, 3, 2)\n    if not trait_stats_df.empty:\n        range_data = []\n        labels = []\n        for model in trait_stats_df['model'].unique():\n            for temp in trait_stats_df['temperature'].unique():\n                subset = trait_stats_df[(trait_stats_df['model'] == model) & \n                                      (trait_stats_df['temperature'] == temp)]\n                if not subset.empty:\n                    ranges = subset['max'] - subset['min']\n                    range_data.append(ranges)\n                    labels.append(f'{model}\\\\ntemp{temp}')\n        \n        if range_data:\n            plt.boxplot(range_data, labels=labels)\n            plt.ylabel('Response Range (Max - Min)')\n            plt.title('Response Range by Model')\n            plt.xticks(rotation=45)\n    \n    # 3. Model Agreement Heatmap\n    plt.subplot(3, 3, 3)\n    if len(results_df['model'].unique()) > 1:\n        # Calculate correlations between models\n        correlations = []\n        model_pairs = []\n        \n        models = results_df['model'].unique()\n        temps = results_df['temperature'].unique()\n        \n        for i, (model1, temp1) in enumerate([(m, t) for m in models for t in temps]):\n            subset1 = results_df[(results_df['model'] == model1) & (results_df['temperature'] == temp1)]\n            if subset1.empty:\n                continue\n                \n            for j, (model2, temp2) in enumerate([(m, t) for m in models for t in temps]):\n                if i >= j:\n                    continue\n                    \n                subset2 = results_df[(results_df['model'] == model2) & (results_df['temperature'] == temp2)]\n                if subset2.empty:\n                    continue\n                \n                # Calculate correlation across all traits\n                corr_values = []\n                for trait in trait_names:\n                    if trait in subset1.columns and trait in subset2.columns:\n                        values1 = subset1[trait].dropna()\n                        values2 = subset2[trait].dropna()\n                        if len(values1) > 0 and len(values2) > 0:\n                            # Align by participant_id\n                            merged = pd.merge(subset1[['participant_id', trait]], \n                                            subset2[['participant_id', trait]], \n                                            on='participant_id', suffixes=('_1', '_2'))\n                            if len(merged) > 1:\n                                corr, _ = pearsonr(merged[f'{trait}_1'], merged[f'{trait}_2'])\n                                if not np.isnan(corr):\n                                    corr_values.append(corr)\n                \n                if corr_values:\n                    avg_corr = np.mean(corr_values)\n                    correlations.append(avg_corr)\n                    model_pairs.append(f'{model1}_t{temp1} vs\\\\n{model2}_t{temp2}')\n        \n        if correlations:\n            y_pos = np.arange(len(model_pairs))\n            plt.barh(y_pos, correlations)\n            plt.yticks(y_pos, model_pairs)\n            plt.xlabel('Average Correlation')\n            plt.title('Inter-Model Agreement')\n    else:\n        plt.text(0.5, 0.5, 'Need multiple models\\\\nfor comparison', \n                ha='center', va='center', transform=plt.gca().transAxes)\n        plt.title('Inter-Model Agreement')\n    \n    # 4. Trait Profile Comparison\n    plt.subplot(3, 3, 4)\n    if not trait_stats_df.empty and len(trait_names) > 5:\n        # Select top 10 most variable traits\n        trait_vars = trait_stats_df.groupby('trait')['std'].mean().sort_values(ascending=False).head(10)\n        selected_traits = trait_vars.index.tolist()\n        \n        for model in trait_stats_df['model'].unique():\n            for temp in trait_stats_df['temperature'].unique():\n                subset = trait_stats_df[(trait_stats_df['model'] == model) & \n                                      (trait_stats_df['temperature'] == temp)]\n                trait_means = []\n                for trait in selected_traits:\n                    trait_data = subset[subset['trait'] == trait]\n                    if not trait_data.empty:\n                        trait_means.append(trait_data['mean'].iloc[0])\n                    else:\n                        trait_means.append(np.nan)\n                \n                plt.plot(range(len(selected_traits)), trait_means, \n                        marker='o', label=f'{model}_temp{temp}')\n        \n        plt.xticks(range(len(selected_traits)), selected_traits, rotation=45, ha='right')\n        plt.ylabel('Mean Rating')\n        plt.title('Trait Profiles (Top 10 Variable Traits)')\n        plt.legend()\n        plt.grid(True, alpha=0.3)\n    \n    # 5. Response Consistency (Temperature Effect)\n    plt.subplot(3, 3, 5)\n    if len(results_df['temperature'].unique()) > 1:\n        consistency_data = []\n        models = []\n        \n        for model in results_df['model'].unique():\n            temp0_data = results_df[(results_df['model'] == model) & (results_df['temperature'] == '0.0')]\n            temp1_data = results_df[(results_df['model'] == model) & (results_df['temperature'] == '1.0')]\n            \n            if not temp0_data.empty and not temp1_data.empty:\n                # Calculate consistency across traits\n                trait_consistencies = []\n                for trait in trait_names:\n                    if trait in temp0_data.columns and trait in temp1_data.columns:\n                        merged = pd.merge(temp0_data[['participant_id', trait]], \n                                        temp1_data[['participant_id', trait]], \n                                        on='participant_id', suffixes=('_t0', '_t1'))\n                        if len(merged) > 1:\n                            corr, _ = pearsonr(merged[f'{trait}_t0'], merged[f'{trait}_t1'])\n                            if not np.isnan(corr):\n                                trait_consistencies.append(corr)\n                \n                if trait_consistencies:\n                    avg_consistency = np.mean(trait_consistencies)\n                    consistency_data.append(avg_consistency)\n                    models.append(model)\n        \n        if consistency_data:\n            plt.bar(models, consistency_data)\n            plt.ylabel('Temperature Consistency (r)')\n            plt.title('Model Consistency Across Temperatures')\n            plt.ylim(0, 1)\n    else:\n        plt.text(0.5, 0.5, 'Need multiple temperatures\\\\nfor comparison', \n                ha='center', va='center', transform=plt.gca().transAxes)\n        plt.title('Temperature Consistency')\n    \n    # 6. Score Distribution by Model\n    plt.subplot(3, 3, 6)\n    if not results_df.empty:\n        all_scores = []\n        model_labels = []\n        \n        for model in results_df['model'].unique():\n            for temp in results_df['temperature'].unique():\n                subset = results_df[(results_df['model'] == model) & (results_df['temperature'] == temp)]\n                if not subset.empty:\n                    # Flatten all trait scores\n                    scores = subset[trait_names].values.flatten()\n                    scores = scores[~np.isnan(scores)]\n                    if len(scores) > 0:\n                        all_scores.append(scores)\n                        model_labels.append(f'{model}_t{temp}')\n        \n        if all_scores:\n            plt.boxplot(all_scores, labels=model_labels)\n            plt.ylabel('Response Values')\n            plt.title('Score Distributions by Model')\n            plt.xticks(rotation=45)\n    \n    # 7. Trait Reliability Heatmap\n    plt.subplot(3, 3, 7)\n    if not trait_stats_df.empty and len(trait_names) > 10:\n        # Create reliability matrix (coefficient of variation)\n        reliability_matrix = []\n        model_temp_combos = []\n        \n        for model in trait_stats_df['model'].unique():\n            for temp in trait_stats_df['temperature'].unique():\n                subset = trait_stats_df[(trait_stats_df['model'] == model) & \n                                      (trait_stats_df['temperature'] == temp)]\n                if not subset.empty:\n                    cv_values = []\n                    for trait in trait_names[:15]:  # Limit to first 15 traits for readability\n                        trait_data = subset[subset['trait'] == trait]\n                        if not trait_data.empty and trait_data['mean'].iloc[0] != 0:\n                            cv = trait_data['std'].iloc[0] / trait_data['mean'].iloc[0]\n                            cv_values.append(cv)\n                        else:\n                            cv_values.append(np.nan)\n                    \n                    reliability_matrix.append(cv_values)\n                    model_temp_combos.append(f'{model}_t{temp}')\n        \n        if reliability_matrix:\n            reliability_array = np.array(reliability_matrix)\n            im = plt.imshow(reliability_array, cmap='viridis', aspect='auto')\n            plt.colorbar(im, label='Coefficient of Variation')\n            plt.yticks(range(len(model_temp_combos)), model_temp_combos)\n            plt.xticks(range(min(15, len(trait_names))), trait_names[:15], rotation=90)\n            plt.title('Trait Reliability by Model')\n    \n    # 8. Model Performance Summary\n    plt.subplot(3, 3, 8)\n    if not trait_stats_df.empty:\n        # Calculate performance metrics\n        performance_metrics = []\n        model_names = []\n        \n        for model in trait_stats_df['model'].unique():\n            for temp in trait_stats_df['temperature'].unique():\n                subset = trait_stats_df[(trait_stats_df['model'] == model) & \n                                      (trait_stats_df['temperature'] == temp)]\n                if not subset.empty:\n                    # Use response completeness as performance metric\n                    completeness = subset['count'].mean() / len(participants_data)\n                    avg_range = (subset['max'] - subset['min']).mean()\n                    \n                    performance_metrics.append([completeness, avg_range])\n                    model_names.append(f'{model}_t{temp}')\n        \n        if performance_metrics:\n            metrics_array = np.array(performance_metrics)\n            x = np.arange(len(model_names))\n            width = 0.35\n            \n            plt.bar(x - width/2, metrics_array[:, 0], width, label='Completeness', alpha=0.8)\n            plt.bar(x + width/2, metrics_array[:, 1]/9, width, label='Range (scaled)', alpha=0.8)  # Scale range to 0-1\n            \n            plt.xlabel('Model')\n            plt.ylabel('Score')\n            plt.title('Model Performance Metrics')\n            plt.xticks(x, model_names, rotation=45)\n            plt.legend()\n    \n    # 9. Response Time Trend (if applicable)\n    plt.subplot(3, 3, 9)\n    plt.text(0.5, 0.5, 'Response patterns\\\\nacross participants', \n            ha='center', va='center', transform=plt.gca().transAxes,\n            fontsize=12, bbox=dict(boxstyle='round', facecolor='lightgray'))\n    plt.title('Future: Response Patterns')\n    \n    plt.tight_layout()\n    plt.savefig('study_2_results/multi_model_analysis.png', dpi=300, bbox_inches='tight')\n    plt.show()\n\n# Create the comprehensive visualization\ncreate_comprehensive_plots(results_df, trait_stats_df, summary_df)",
   "metadata": {},
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "code",
   "source": "## 2. Descriptive Statistics and Model Comparison\n\ndef create_descriptive_analysis(results_df, trait_stats_df):\n    \\\"\\\"\\\"Create comprehensive descriptive statistics for model comparison.\\\"\\\"\\\"\n    \n    if results_df.empty:\n        print(\"No results to analyze\")\n        return\n    \n    # Overall statistics by model and temperature\n    print(\"=== SIMULATION OVERVIEW ===\")\n    model_summary = results_df.groupby(['model', 'temperature']).agg({\n        'participant_id': 'count'\n    }).rename(columns={'participant_id': 'n_participants'})\n    print(model_summary)\n    \n    # Trait-level statistics\n    if not trait_stats_df.empty:\n        print(\"\\\\n=== TRAIT STATISTICS BY MODEL ===\")\n        \n        # Calculate overall means by model-temperature\n        overall_means = trait_stats_df.groupby(['model', 'temperature'])['mean'].agg(['mean', 'std']).round(3)\n        overall_means.columns = ['avg_trait_mean', 'std_trait_mean']\n        print(\"Average trait ratings:\")\n        print(overall_means)\n        \n        # Response range analysis\n        print(\"\\\\n=== RESPONSE RANGE ANALYSIS ===\")\n        range_stats = trait_stats_df.copy()\n        range_stats['range'] = range_stats['max'] - range_stats['min']\n        range_summary = range_stats.groupby(['model', 'temperature'])['range'].agg(['mean', 'std']).round(3)\n        range_summary.columns = ['avg_range', 'std_range']\n        print(\"Response range statistics:\")\n        print(range_summary)\n        \n        # Most variable traits\n        print(\"\\\\n=== MOST VARIABLE TRAITS ===\")\n        trait_variability = trait_stats_df.groupby('trait')['std'].mean().sort_values(ascending=False)\n        print(\"Traits with highest variability across models:\")\n        print(trait_variability.head(10).round(3))\n    \n    return model_summary, overall_means if not trait_stats_df.empty else None\n\n# Run descriptive analysis\nmodel_summary, overall_means = create_descriptive_analysis(results_df, trait_stats_df)",
   "metadata": {},
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "code",
   "source": "## 1. Data Processing and Structure\n\ndef process_simulation_results(all_results, participants_data):\n    \"\"\"\n    Process simulation results into structured DataFrames for analysis.\n    \n    Returns:\n    - results_df: Long format DataFrame with all responses\n    - summary_df: Wide format DataFrame with model comparisons\n    - trait_stats: Summary statistics by trait and model\n    \"\"\"\n    \n    # Mini-Marker trait names in order\n    trait_names = [\n        'Bashful', 'Bold', 'Careless', 'Cold', 'Complex', 'Cooperative', 'Creative', 'Deep',\n        'Disorganized', 'Efficient', 'Energetic', 'Envious', 'Extraverted', 'Fretful', 'Harsh',\n        'Imaginative', 'Inefficient', 'Intellectual', 'Jealous', 'Kind', 'Moody', 'Organized',\n        'Philosophical', 'Practical', 'Quiet', 'Relaxed', 'Rude', 'Shy', 'Sloppy', 'Sympathetic',\n        'Systematic', 'Talkative', 'Temperamental', 'Touchy', 'Uncreative', 'Unenvious',\n        'Unintellectual', 'Unsympathetic', 'Warm', 'Withdrawn'\n    ]\n    \n    # Initialize storage\n    results_list = []\n    summary_data = {'participant_id': range(len(participants_data))}\n    \n    # Process each model-temperature combination\n    for model_temp, results in all_results.items():\n        if not isinstance(results, list):\n            print(f\"Skipping {model_temp}: {results}\")\n            continue\n            \n        model_name = model_temp.split('_temp')[0]\n        temperature = model_temp.split('_temp')[1]\n        \n        # Extract responses for each participant\n        model_responses = []\n        for i, result in enumerate(results):\n            if isinstance(result, dict) and 'error' not in result:\n                # Convert to standard format and ensure all traits are present\n                response_dict = {'participant_id': i, 'model': model_name, 'temperature': temperature}\n                \n                for trait in trait_names:\n                    if trait in result:\n                        try:\n                            value = float(result[trait])\n                            response_dict[trait] = value\n                        except (ValueError, TypeError):\n                            response_dict[trait] = np.nan\n                    else:\n                        response_dict[trait] = np.nan\n                \n                results_list.append(response_dict)\n                model_responses.append([response_dict.get(trait, np.nan) for trait in trait_names])\n            else:\n                # Handle failed responses\n                model_responses.append([np.nan] * len(trait_names))\n        \n        # Add to summary DataFrame\n        model_responses_array = np.array(model_responses)\n        for j, trait in enumerate(trait_names):\n            summary_data[f'{model_name}_temp{temperature}_{trait}'] = model_responses_array[:, j]\n    \n    # Create DataFrames\n    results_df = pd.DataFrame(results_list)\n    summary_df = pd.DataFrame(summary_data)\n    \n    # Calculate trait statistics\n    trait_stats = []\n    if not results_df.empty:\n        for trait in trait_names:\n            if trait in results_df.columns:\n                trait_data = results_df.groupby(['model', 'temperature'])[trait].agg([\n                    'count', 'mean', 'std', 'min', 'max'\n                ]).reset_index()\n                trait_data['trait'] = trait\n                trait_stats.append(trait_data)\n    \n    trait_stats_df = pd.concat(trait_stats, ignore_index=True) if trait_stats else pd.DataFrame()\n    \n    return results_df, summary_df, trait_stats_df\n\n# Process the results\nprint(\"Processing simulation results...\")\nresults_df, summary_df, trait_stats_df = process_simulation_results(all_results, participants_data)\n\nprint(f\"Results DataFrame shape: {results_df.shape}\")\nprint(f\"Summary DataFrame shape: {summary_df.shape}\")\nprint(f\"Trait statistics shape: {trait_stats_df.shape}\")\n\nif not results_df.empty:\n    print(f\"\\\\nAvailable models: {results_df['model'].unique()}\")\n    print(f\"Available temperatures: {results_df['temperature'].unique()}\")\nelse:\n    print(\"No valid results to process\")",
   "metadata": {},
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "code",
   "source": "import numpy as np\nimport matplotlib.pyplot as plt\nimport seaborn as sns\nfrom scipy.stats import pearsonr, spearmanr\nfrom scipy.cluster.hierarchy import dendrogram, linkage\nfrom sklearn.preprocessing import StandardScaler\nimport warnings\nwarnings.filterwarnings('ignore')\n\n# Set up plotting style\nplt.style.use('default')\nsns.set_palette(\"husl\")\nplt.rcParams['figure.figsize'] = (12, 8)\nplt.rcParams['font.size'] = 10",
   "metadata": {},
   "outputs": [],
   "execution_count": null
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
