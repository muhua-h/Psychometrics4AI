{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Multi-Model Personality Simulation - Study 2\n",
    "\n",
    "This notebook refactors the original Study 2 BFI-2 to Mini-Marker simulation to work with multiple LLM models using the unified portal.py interface.\n",
    "\n",
    "## Models to Test\n",
    "- GPT-4\n",
    "- GPT-4o  \n",
    "- Llama-3.3-70B-Instruct\n",
    "- DeepSeek-V3\n",
    "\n",
    "## Data Flow\n",
    "1. Load and preprocess Soto BFI-2 data\n",
    "2. Apply reverse coding to personality items\n",
    "3. Map numeric responses to expanded format descriptions\n",
    "4. Generate personality simulation prompts\n",
    "5. Run simulations across multiple models\n",
    "6. Save results for analysis\n",
    "\n",
    "## Next Steps\n",
    "After running this notebook, use `study_2_analysis.ipynb` for comprehensive analysis of the results."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-06-26T12:18:40.488298Z",
     "start_time": "2025-06-26T12:18:39.151180Z"
    }
   },
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import sys\n",
    "from pathlib import Path\n",
    "\n",
    "# Add shared modules to path\n",
    "sys.path.append('../shared')\n",
    "\n",
    "from simulation_utils import (\n",
    "    SimulationConfig, \n",
    "    run_bfi_to_minimarker_simulation,\n",
    "    retry_failed_participants\n",
    ")\n",
    "from schema_bfi2 import expanded_scale\n",
    "from mini_marker_prompt import get_prompt"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Data Loading and Preprocessing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-06-26T12:18:42.883719Z",
     "start_time": "2025-06-26T12:18:40.494250Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loaded data shape: (470, 704)\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>case_id</th>\n",
       "      <th>age</th>\n",
       "      <th>sex</th>\n",
       "      <th>ethnicity</th>\n",
       "      <th>rel_acquaintance</th>\n",
       "      <th>rel_friend</th>\n",
       "      <th>rel_roommate</th>\n",
       "      <th>rel_boygirlfriend</th>\n",
       "      <th>rel_relative</th>\n",
       "      <th>rel_other</th>\n",
       "      <th>...</th>\n",
       "      <th>tneo_n3_dep</th>\n",
       "      <th>tneo_n4_sel</th>\n",
       "      <th>tneo_n5_imp</th>\n",
       "      <th>tneo_n6_vul</th>\n",
       "      <th>tneo_o1_fan</th>\n",
       "      <th>tneo_o2_aes</th>\n",
       "      <th>tneo_o3_fee</th>\n",
       "      <th>tneo_o4_act</th>\n",
       "      <th>tneo_o5_ide</th>\n",
       "      <th>tneo_o6_val</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>1</td>\n",
       "      <td>27.0</td>\n",
       "      <td>M</td>\n",
       "      <td>2.0</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>...</td>\n",
       "      <td>51.250000</td>\n",
       "      <td>40.181818</td>\n",
       "      <td>64.000000</td>\n",
       "      <td>55.102041</td>\n",
       "      <td>46.639344</td>\n",
       "      <td>46.969697</td>\n",
       "      <td>66.7</td>\n",
       "      <td>57.065217</td>\n",
       "      <td>41.984127</td>\n",
       "      <td>58.039216</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>2</td>\n",
       "      <td>26.0</td>\n",
       "      <td>M</td>\n",
       "      <td>3.0</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>...</td>\n",
       "      <td>69.632353</td>\n",
       "      <td>60.636364</td>\n",
       "      <td>66.272727</td>\n",
       "      <td>65.306122</td>\n",
       "      <td>54.836066</td>\n",
       "      <td>56.439394</td>\n",
       "      <td>51.7</td>\n",
       "      <td>51.630435</td>\n",
       "      <td>51.904762</td>\n",
       "      <td>45.784314</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>3</td>\n",
       "      <td>24.0</td>\n",
       "      <td>F</td>\n",
       "      <td>4.0</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>...</td>\n",
       "      <td>60.441176</td>\n",
       "      <td>74.272727</td>\n",
       "      <td>54.909091</td>\n",
       "      <td>65.306122</td>\n",
       "      <td>75.327869</td>\n",
       "      <td>56.439394</td>\n",
       "      <td>56.7</td>\n",
       "      <td>40.760870</td>\n",
       "      <td>51.904762</td>\n",
       "      <td>58.039216</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>4</td>\n",
       "      <td>33.0</td>\n",
       "      <td>M</td>\n",
       "      <td>3.0</td>\n",
       "      <td>NaN</td>\n",
       "      <td>1.0</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>...</td>\n",
       "      <td>67.794118</td>\n",
       "      <td>58.363636</td>\n",
       "      <td>64.000000</td>\n",
       "      <td>52.551020</td>\n",
       "      <td>54.836066</td>\n",
       "      <td>50.757576</td>\n",
       "      <td>36.7</td>\n",
       "      <td>65.217391</td>\n",
       "      <td>63.809524</td>\n",
       "      <td>58.039216</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>5</td>\n",
       "      <td>23.0</td>\n",
       "      <td>F</td>\n",
       "      <td>5.0</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>...</td>\n",
       "      <td>62.279412</td>\n",
       "      <td>67.454545</td>\n",
       "      <td>41.272727</td>\n",
       "      <td>60.204082</td>\n",
       "      <td>50.737705</td>\n",
       "      <td>48.863636</td>\n",
       "      <td>49.2</td>\n",
       "      <td>46.195652</td>\n",
       "      <td>38.015873</td>\n",
       "      <td>38.431373</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>5 rows Ã— 704 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "   case_id   age sex  ethnicity  rel_acquaintance  rel_friend  rel_roommate  \\\n",
       "0        1  27.0   M        2.0               NaN         NaN           NaN   \n",
       "1        2  26.0   M        3.0               NaN         NaN           NaN   \n",
       "2        3  24.0   F        4.0               NaN         NaN           NaN   \n",
       "3        4  33.0   M        3.0               NaN         1.0           NaN   \n",
       "4        5  23.0   F        5.0               NaN         NaN           NaN   \n",
       "\n",
       "   rel_boygirlfriend  rel_relative  rel_other  ... tneo_n3_dep  tneo_n4_sel  \\\n",
       "0                NaN           NaN        NaN  ...   51.250000    40.181818   \n",
       "1                NaN           NaN        NaN  ...   69.632353    60.636364   \n",
       "2                NaN           NaN        NaN  ...   60.441176    74.272727   \n",
       "3                NaN           NaN        NaN  ...   67.794118    58.363636   \n",
       "4                NaN           NaN        NaN  ...   62.279412    67.454545   \n",
       "\n",
       "   tneo_n5_imp  tneo_n6_vul  tneo_o1_fan  tneo_o2_aes  tneo_o3_fee  \\\n",
       "0    64.000000    55.102041    46.639344    46.969697         66.7   \n",
       "1    66.272727    65.306122    54.836066    56.439394         51.7   \n",
       "2    54.909091    65.306122    75.327869    56.439394         56.7   \n",
       "3    64.000000    52.551020    54.836066    50.757576         36.7   \n",
       "4    41.272727    60.204082    50.737705    48.863636         49.2   \n",
       "\n",
       "   tneo_o4_act  tneo_o5_ide  tneo_o6_val  \n",
       "0    57.065217    41.984127    58.039216  \n",
       "1    51.630435    51.904762    45.784314  \n",
       "2    40.760870    51.904762    58.039216  \n",
       "3    65.217391    63.809524    58.039216  \n",
       "4    46.195652    38.015873    38.431373  \n",
       "\n",
       "[5 rows x 704 columns]"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Load the Soto BFI-2 dataset\n",
    "data_path = Path('../../raw_data/Soto_data.xlsx')\n",
    "if not data_path.exists():\n",
    "    print(f\"Data file not found at {data_path}\")\n",
    "    print(\"Please ensure the raw_data/Soto_data.xlsx file exists in the project root\")\n",
    "    raise FileNotFoundError(f\"Data file not found: {data_path}\")\n",
    "\n",
    "data = pd.read_excel(data_path, sheet_name='data')\n",
    "print(f\"Loaded data shape: {data.shape}\")\n",
    "data.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-06-26T12:18:43.073948Z",
     "start_time": "2025-06-26T12:18:43.069903Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Original data shape: (470, 704)\n",
      "Data shape after removing missing values: (438, 704)\n"
     ]
    }
   ],
   "source": [
    "# Generate column names for TDA (Mini-Marker) and BFI-2 items\n",
    "tda_columns = [f\"tda{i}\" for i in range(1, 41)]\n",
    "sbfi_columns = [f\"bfi{i}\" for i in range(1, 61)]\n",
    "selected_columns = tda_columns + sbfi_columns\n",
    "\n",
    "print(f\"Original data shape: {data.shape}\")\n",
    "\n",
    "# Remove rows with missing values in the selected columns\n",
    "data = data.dropna(subset=selected_columns)\n",
    "print(f\"Data shape after removing missing values: {data.shape}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-06-26T12:18:43.091483Z",
     "start_time": "2025-06-26T12:18:43.081449Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Reverse coding applied successfully\n"
     ]
    }
   ],
   "source": [
    "# Reverse coding map for BFI-2 items\n",
    "reverse_coding_map = {\n",
    "    'bfi1': 'bfi1', 'bfi2': 'bfi2', 'bfi3': 'bfi3R', 'bfi4': 'bfi4R', 'bfi5': 'bfi5R',\n",
    "    'bfi6': 'bfi6', 'bfi7': 'bfi7', 'bfi8': 'bfi8R', 'bfi9': 'bfi9R', 'bfi10': 'bfi10',\n",
    "    'bfi11': 'bfi11R', 'bfi12': 'bfi12R', 'bfi13': 'bfi13', 'bfi14': 'bfi14', 'bfi15': 'bfi15',\n",
    "    'bfi16': 'bfi16R', 'bfi17': 'bfi17R', 'bfi18': 'bfi18', 'bfi19': 'bfi19', 'bfi20': 'bfi20',\n",
    "    'bfi21': 'bfi21', 'bfi22': 'bfi22R', 'bfi23': 'bfi23R', 'bfi24': 'bfi24R', 'bfi25': 'bfi25R',\n",
    "    'bfi26': 'bfi26R', 'bfi27': 'bfi27', 'bfi28': 'bfi28R', 'bfi29': 'bfi29R', 'bfi30': 'bfi30R',\n",
    "    'bfi31': 'bfi31R', 'bfi32': 'bfi32', 'bfi33': 'bfi33', 'bfi34': 'bfi34', 'bfi35': 'bfi35',\n",
    "    'bfi36': 'bfi36R', 'bfi37': 'bfi37R', 'bfi38': 'bfi38', 'bfi39': 'bfi39', 'bfi40': 'bfi40',\n",
    "    'bfi41': 'bfi41', 'bfi42': 'bfi42R', 'bfi43': 'bfi43', 'bfi44': 'bfi44R', 'bfi45': 'bfi45R',\n",
    "    'bfi46': 'bfi46', 'bfi47': 'bfi47R', 'bfi48': 'bfi48R', 'bfi49': 'bfi49R', 'bfi50': 'bfi50R',\n",
    "    'bfi51': 'bfi51R', 'bfi52': 'bfi52', 'bfi53': 'bfi53', 'bfi54': 'bfi54', 'bfi55': 'bfi55R',\n",
    "    'bfi56': 'bfi56', 'bfi57': 'bfi57', 'bfi58': 'bfi58R', 'bfi59': 'bfi59', 'bfi60': 'bfi60'\n",
    "}\n",
    "\n",
    "# Apply reverse coding\n",
    "for key, value in reverse_coding_map.items():\n",
    "    if value.endswith('R'):  # Reverse coded\n",
    "        data[key] = 6 - data[key]\n",
    "    # else: keep original value\n",
    "\n",
    "print(\"Reverse coding applied successfully\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-06-26T12:18:43.311991Z",
     "start_time": "2025-06-26T12:18:43.099507Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Personality descriptions created successfully\n",
      "Final data shape: (438, 705)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/var/folders/zh/79y5ykz51rxcnjchzb73nfxw0000gn/T/ipykernel_14304/3440998859.py:7: FutureWarning: Setting an item of incompatible dtype is deprecated and will raise in a future error of pandas. Value 'I am fairly organized.' has dtype incompatible with int64, please explicitly cast to a compatible dtype first.\n",
      "  mapped_row[key] = expanded_scale[key][index]  # Replace with corresponding string\n",
      "/var/folders/zh/79y5ykz51rxcnjchzb73nfxw0000gn/T/ipykernel_14304/3440998859.py:7: FutureWarning: Setting an item of incompatible dtype is deprecated and will raise in a future error of pandas. Value 'I am fairly disorganized.' has dtype incompatible with int64, please explicitly cast to a compatible dtype first.\n",
      "  mapped_row[key] = expanded_scale[key][index]  # Replace with corresponding string\n",
      "/var/folders/zh/79y5ykz51rxcnjchzb73nfxw0000gn/T/ipykernel_14304/3440998859.py:7: FutureWarning: Setting an item of incompatible dtype is deprecated and will raise in a future error of pandas. Value 'I am very organized.' has dtype incompatible with int64, please explicitly cast to a compatible dtype first.\n",
      "  mapped_row[key] = expanded_scale[key][index]  # Replace with corresponding string\n",
      "/var/folders/zh/79y5ykz51rxcnjchzb73nfxw0000gn/T/ipykernel_14304/3440998859.py:7: FutureWarning: Setting an item of incompatible dtype is deprecated and will raise in a future error of pandas. Value 'I am neither particularly organized nor disorganized.' has dtype incompatible with int64, please explicitly cast to a compatible dtype first.\n",
      "  mapped_row[key] = expanded_scale[key][index]  # Replace with corresponding string\n",
      "/var/folders/zh/79y5ykz51rxcnjchzb73nfxw0000gn/T/ipykernel_14304/3440998859.py:7: FutureWarning: Setting an item of incompatible dtype is deprecated and will raise in a future error of pandas. Value 'I am very disorganized.' has dtype incompatible with int64, please explicitly cast to a compatible dtype first.\n",
      "  mapped_row[key] = expanded_scale[key][index]  # Replace with corresponding string\n"
     ]
    }
   ],
   "source": [
    "# Map numeric values to expanded format descriptions\n",
    "def map_values(row):\n",
    "    mapped_row = row.copy()\n",
    "    for key in expanded_scale:\n",
    "        if pd.notna(row[key]):  # Check if the value is not NaN\n",
    "            index = int(row[key]) - 1  # Convert to 0-index\n",
    "            mapped_row[key] = expanded_scale[key][index]  # Replace with corresponding string\n",
    "    return mapped_row\n",
    "\n",
    "# Apply mapping to BFI columns\n",
    "mapped_data = data[sbfi_columns].apply(map_values, axis=1)\n",
    "\n",
    "# Create combined BFI-2 description\n",
    "mapped_data['combined_bfi2'] = mapped_data[[f'bfi{i}' for i in range(1, 61)]].apply(\n",
    "    lambda row: ' '.join(row), axis=1\n",
    ")\n",
    "\n",
    "# Add combined description to original data\n",
    "data['combined_bfi2'] = mapped_data['combined_bfi2']\n",
    "\n",
    "print(\"Personality descriptions created successfully\")\n",
    "print(f\"Final data shape: {data.shape}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-06-26T12:18:43.518710Z",
     "start_time": "2025-06-26T12:18:43.515975Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Sample personality description:\n",
      "I am very outgoing, sociable. I am very compassionate almost always soft-hearted. I am fairly organized. I am somewhat relaxed handle stress somewhat well. I have some artistic interests. I am quite assertive. I am very respectful almost always treat others with respect. I am often lazy. I stay very optimistic after experiencing a setback. I am curious about few things. I often feel excited or eager. I rarely find fault with others. I am very dependable steady. I am fairly moody often have up an...\n"
     ]
    }
   ],
   "source": [
    "# Preview a personality description\n",
    "print(\"Sample personality description:\")\n",
    "print(data.iloc[0]['combined_bfi2'][:500] + \"...\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Multi-Model Simulation Configuration"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-06-26T12:18:43.581168Z",
     "start_time": "2025-06-26T12:18:43.525116Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Prepared 438 participants for simulation\n"
     ]
    }
   ],
   "source": [
    "# Configuration for different models and temperatures\n",
    "models_to_test = ['gpt-4', 'gpt-4o', 'llama', 'deepseek']\n",
    "temperatures = [1.0]  # Test both deterministic and stochastic responses\n",
    "batch_size = 25  # Smaller batch size for stability across different APIs\n",
    "\n",
    "# Create participant data list from DataFrame\n",
    "participants_data = data.to_dict('records')\n",
    "print(f\"Prepared {len(participants_data)} participants for simulation\")\n",
    "\n",
    "# For testing: uncomment these lines to use a subset\n",
    "# models_to_test = ['deepseek']\n",
    "# participants_data = participants_data[:30]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Run Simulations for All Models"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-06-26T13:40:26.403763Z",
     "start_time": "2025-06-26T12:18:43.590710Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "============================================================\n",
      "Starting simulation: gpt-4 with temperature 1.0\n",
      "============================================================\n",
      "Starting simulation for 438 participants using gpt-4\n",
      "Temperature: 1.0, Batch size: 25\n",
      "Processing participants 0 to 24\n",
      "Completed batch 0 to 24\n",
      "Processing participants 25 to 49\n",
      "Completed batch 25 to 49\n",
      "Processing participants 50 to 74\n",
      "Completed batch 50 to 74\n",
      "Processing participants 75 to 99\n",
      "Completed batch 75 to 99\n",
      "Processing participants 100 to 124\n",
      "Completed batch 100 to 124\n",
      "Processing participants 125 to 149\n",
      "Completed batch 125 to 149\n",
      "Processing participants 150 to 174\n",
      "Completed batch 150 to 174\n",
      "Processing participants 175 to 199\n",
      "Completed batch 175 to 199\n",
      "Processing participants 200 to 224\n",
      "Completed batch 200 to 224\n",
      "Processing participants 225 to 249\n",
      "Completed batch 225 to 249\n",
      "Processing participants 250 to 274\n",
      "Completed batch 250 to 274\n",
      "Processing participants 275 to 299\n",
      "Completed batch 275 to 299\n",
      "Processing participants 300 to 324\n",
      "Completed batch 300 to 324\n",
      "Processing participants 325 to 349\n",
      "Completed batch 325 to 349\n",
      "Processing participants 350 to 374\n",
      "Completed batch 350 to 374\n",
      "Processing participants 375 to 399\n",
      "Completed batch 375 to 399\n",
      "Processing participants 400 to 424\n",
      "Completed batch 400 to 424\n",
      "Processing participants 425 to 437\n",
      "Completed batch 425 to 437\n",
      "Results saved to study_2_results/bfi_to_minimarker_gpt_4_temp1_0.json\n",
      "Completed simulation: gpt-4 with temperature 1.0\n",
      "\n",
      "============================================================\n",
      "Starting simulation: gpt-4o with temperature 1.0\n",
      "============================================================\n",
      "Starting simulation for 438 participants using gpt-4o\n",
      "Temperature: 1.0, Batch size: 25\n",
      "Processing participants 0 to 24\n",
      "Completed batch 0 to 24\n",
      "Processing participants 25 to 49\n",
      "Completed batch 25 to 49\n",
      "Processing participants 50 to 74\n",
      "Completed batch 50 to 74\n",
      "Processing participants 75 to 99\n",
      "Completed batch 75 to 99\n",
      "Processing participants 100 to 124\n",
      "Completed batch 100 to 124\n",
      "Processing participants 125 to 149\n",
      "Completed batch 125 to 149\n",
      "Processing participants 150 to 174\n",
      "Completed batch 150 to 174\n",
      "Processing participants 175 to 199\n",
      "Completed batch 175 to 199\n",
      "Processing participants 200 to 224\n",
      "Completed batch 200 to 224\n",
      "Processing participants 225 to 249\n",
      "Completed batch 225 to 249\n",
      "Processing participants 250 to 274\n",
      "Completed batch 250 to 274\n",
      "Processing participants 275 to 299\n",
      "Completed batch 275 to 299\n",
      "Processing participants 300 to 324\n",
      "Completed batch 300 to 324\n",
      "Processing participants 325 to 349\n",
      "Completed batch 325 to 349\n",
      "Processing participants 350 to 374\n",
      "Completed batch 350 to 374\n",
      "Processing participants 375 to 399\n",
      "Completed batch 375 to 399\n",
      "Processing participants 400 to 424\n",
      "Completed batch 400 to 424\n",
      "Processing participants 425 to 437\n",
      "Completed batch 425 to 437\n",
      "Results saved to study_2_results/bfi_to_minimarker_gpt_4o_temp1_0.json\n",
      "Completed simulation: gpt-4o with temperature 1.0\n",
      "\n",
      "============================================================\n",
      "Starting simulation: llama with temperature 1.0\n",
      "============================================================\n",
      "Starting simulation for 438 participants using llama\n",
      "Temperature: 1.0, Batch size: 25\n",
      "Processing participants 0 to 24\n",
      "Completed batch 0 to 24\n",
      "Processing participants 25 to 49\n",
      "Completed batch 25 to 49\n",
      "Processing participants 50 to 74\n",
      "Completed batch 50 to 74\n",
      "Processing participants 75 to 99\n",
      "Completed batch 75 to 99\n",
      "Processing participants 100 to 124\n",
      "Completed batch 100 to 124\n",
      "Processing participants 125 to 149\n",
      "Completed batch 125 to 149\n",
      "Processing participants 150 to 174\n",
      "Completed batch 150 to 174\n",
      "Processing participants 175 to 199\n",
      "Completed batch 175 to 199\n",
      "Processing participants 200 to 224\n",
      "Completed batch 200 to 224\n",
      "Processing participants 225 to 249\n",
      "Completed batch 225 to 249\n",
      "Processing participants 250 to 274\n",
      "Completed batch 250 to 274\n",
      "Processing participants 275 to 299\n",
      "Completed batch 275 to 299\n",
      "Processing participants 300 to 324\n",
      "Completed batch 300 to 324\n",
      "Processing participants 325 to 349\n",
      "Completed batch 325 to 349\n",
      "Processing participants 350 to 374\n",
      "Completed batch 350 to 374\n",
      "Processing participants 375 to 399\n",
      "Completed batch 375 to 399\n",
      "Processing participants 400 to 424\n",
      "Llama error: HTTPSConnectionPool(host='allmodelapi3225011299.services.ai.azure.com', port=443): Read timed out. (read timeout=300)\n",
      "Error in get_personality_response: HTTPSConnectionPool(host='allmodelapi3225011299.services.ai.azure.com', port=443): Read timed out. (read timeout=300)\n",
      "Completed batch 400 to 424\n",
      "Processing participants 425 to 437\n",
      "Completed batch 425 to 437\n",
      "Results saved to study_2_results/bfi_to_minimarker_llama_temp1_0.json\n",
      "Completed simulation: llama with temperature 1.0\n",
      "\n",
      "============================================================\n",
      "Starting simulation: deepseek with temperature 1.0\n",
      "============================================================\n",
      "Starting simulation for 438 participants using deepseek\n",
      "Temperature: 1.0, Batch size: 25\n",
      "Processing participants 0 to 24\n",
      "Completed batch 0 to 24\n",
      "Processing participants 25 to 49\n",
      "Completed batch 25 to 49\n",
      "Processing participants 50 to 74\n",
      "Completed batch 50 to 74\n",
      "Processing participants 75 to 99\n",
      "Completed batch 75 to 99\n",
      "Processing participants 100 to 124\n",
      "DeepSeek error: HTTPSConnectionPool(host='allmodelapi3225011299.services.ai.azure.com', port=443): Read timed out. (read timeout=300)\n",
      "Error in get_personality_response: HTTPSConnectionPool(host='allmodelapi3225011299.services.ai.azure.com', port=443): Read timed out. (read timeout=300)\n",
      "Completed batch 100 to 124\n",
      "Processing participants 125 to 149\n",
      "DeepSeek error: HTTPSConnectionPool(host='allmodelapi3225011299.services.ai.azure.com', port=443): Read timed out. (read timeout=300)\n",
      "Error in get_personality_response: HTTPSConnectionPool(host='allmodelapi3225011299.services.ai.azure.com', port=443): Read timed out. (read timeout=300)\n",
      "Completed batch 125 to 149\n",
      "Processing participants 150 to 174\n",
      "Completed batch 150 to 174\n",
      "Processing participants 175 to 199\n",
      "Completed batch 175 to 199\n",
      "Processing participants 200 to 224\n",
      "Completed batch 200 to 224\n",
      "Processing participants 225 to 249\n",
      "Completed batch 225 to 249\n",
      "Processing participants 250 to 274\n",
      "Completed batch 250 to 274\n",
      "Processing participants 275 to 299\n",
      "Completed batch 275 to 299\n",
      "Processing participants 300 to 324\n",
      "Completed batch 300 to 324\n",
      "Processing participants 325 to 349\n",
      "Completed batch 325 to 349\n",
      "Processing participants 350 to 374\n",
      "Completed batch 350 to 374\n",
      "Processing participants 375 to 399\n",
      "Completed batch 375 to 399\n",
      "Processing participants 400 to 424\n",
      "Completed batch 400 to 424\n",
      "Processing participants 425 to 437\n",
      "Completed batch 425 to 437\n",
      "Results saved to study_2_results/bfi_to_minimarker_deepseek_temp1_0.json\n",
      "Completed simulation: deepseek with temperature 1.0\n",
      "\n",
      "Completed all simulations. Results keys: ['gpt-4_temp1.0', 'gpt-4o_temp1.0', 'llama_temp1.0', 'deepseek_temp1.0']\n"
     ]
    }
   ],
   "source": [
    "# Run simulations for all model-temperature combinations\n",
    "all_results = {}\n",
    "\n",
    "for model in models_to_test:\n",
    "    for temperature in temperatures:\n",
    "        print(f\"\\n{'='*60}\")\n",
    "        print(f\"Starting simulation: {model} with temperature {temperature}\")\n",
    "        print(f\"{'='*60}\")\n",
    "        \n",
    "        config = SimulationConfig(\n",
    "            model=model,\n",
    "            temperature=temperature,\n",
    "            batch_size=batch_size,\n",
    "            max_workers=10\n",
    "        )\n",
    "        \n",
    "        try:\n",
    "            results = run_bfi_to_minimarker_simulation(\n",
    "                participants_data=participants_data,\n",
    "                config=config,\n",
    "                output_dir=\"study_2_results\"\n",
    "            )\n",
    "            \n",
    "            # Store results\n",
    "            key = f\"{model}_temp{temperature}\"\n",
    "            all_results[key] = results\n",
    "            \n",
    "            # Check for any failed participants\n",
    "            failed_count = sum(1 for r in results if isinstance(r, dict) and 'error' in r)\n",
    "            if failed_count > 0:\n",
    "                print(f\"Warning: {failed_count} participants failed. Consider retrying.\")\n",
    "                \n",
    "            print(f\"Completed simulation: {model} with temperature {temperature}\")\n",
    "            \n",
    "        except Exception as e:\n",
    "            print(f\"Error in simulation {model} temp {temperature}: {str(e)}\")\n",
    "            all_results[f\"{model}_temp{temperature}\"] = {\"error\": str(e)}\n",
    "\n",
    "print(f\"\\nCompleted all simulations. Results keys: {list(all_results.keys())}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Retry Failed Participants (if any)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-06-26T13:40:26.456848Z",
     "start_time": "2025-06-26T13:40:26.450171Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Retry process completed\n"
     ]
    }
   ],
   "source": [
    "# Retry any failed participants\n",
    "for key, results in all_results.items():\n",
    "    if isinstance(results, list):\n",
    "        failed_count = sum(1 for r in results if isinstance(r, dict) and 'error' in r)\n",
    "        if failed_count > 0:\n",
    "            print(f\"Retrying {failed_count} failed participants for {key}\")\n",
    "            \n",
    "            # Extract model and temperature from key\n",
    "            model = key.split('_temp')[0]\n",
    "            temperature = float(key.split('_temp')[1])\n",
    "            \n",
    "            config = SimulationConfig(\n",
    "                model=model,\n",
    "                temperature=temperature,\n",
    "                batch_size=batch_size\n",
    "            )\n",
    "            \n",
    "            updated_results = retry_failed_participants(\n",
    "                results=results,\n",
    "                participants_data=participants_data,\n",
    "                prompt_generator=get_prompt,  # Use imported get_prompt function\n",
    "                config=config,\n",
    "                personality_key='combined_bfi2'\n",
    "            )\n",
    "            \n",
    "            all_results[key] = updated_results\n",
    "            \n",
    "            # Save updated results\n",
    "            from simulation_utils import save_simulation_results\n",
    "            save_simulation_results(updated_results, \"study_2_results\", \"bfi_to_minimarker_retried\", config)\n",
    "\n",
    "print(\"Retry process completed\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Results Summary"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-06-26T13:40:26.482708Z",
     "start_time": "2025-06-26T13:40:26.478507Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Simulation Results Summary:\n",
      "==================================================\n",
      "gpt-4_temp1.0:\n",
      "  Total: 438, Successful: 438, Failed: 0\n",
      "  Success Rate: 100.0%\n",
      "\n",
      "gpt-4o_temp1.0:\n",
      "  Total: 438, Successful: 438, Failed: 0\n",
      "  Success Rate: 100.0%\n",
      "\n",
      "llama_temp1.0:\n",
      "  Total: 438, Successful: 438, Failed: 0\n",
      "  Success Rate: 100.0%\n",
      "\n",
      "deepseek_temp1.0:\n",
      "  Total: 438, Successful: 438, Failed: 0\n",
      "  Success Rate: 100.0%\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Analyze results summary\n",
    "print(\"Simulation Results Summary:\")\n",
    "print(\"=\" * 50)\n",
    "\n",
    "for key, results in all_results.items():\n",
    "    if isinstance(results, list):\n",
    "        total_participants = len(results)\n",
    "        successful = sum(1 for r in results if not (isinstance(r, dict) and 'error' in r))\n",
    "        failed = total_participants - successful\n",
    "        success_rate = (successful / total_participants) * 100\n",
    "        \n",
    "        print(f\"{key}:\")\n",
    "        print(f\"  Total: {total_participants}, Successful: {successful}, Failed: {failed}\")\n",
    "        print(f\"  Success Rate: {success_rate:.1f}%\")\n",
    "        print()\n",
    "    else:\n",
    "        print(f\"{key}: FAILED - {results.get('error', 'Unknown error')}\")\n",
    "        print()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Save Preprocessed Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-06-26T13:40:26.612046Z",
     "start_time": "2025-06-26T13:40:26.500366Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Preprocessed data saved to study_2_results/study2_preprocessed_data.csv\n",
      "\n",
      "============================================================\n",
      "SIMULATION COMPLETE!\n",
      "\n",
      "Next steps:\n",
      "1. Run study_2_analysis.ipynb for comprehensive analysis\n",
      "2. Results are saved in study_2_results/ directory\n",
      "3. Preprocessed data available for validation\n",
      "============================================================\n"
     ]
    }
   ],
   "source": [
    "# Save the preprocessed data for reference\n",
    "output_path = Path('study_2_results')\n",
    "output_path.mkdir(exist_ok=True)\n",
    "\n",
    "data.to_csv(output_path / 'study2_preprocessed_data.csv', index=False)\n",
    "print(f\"Preprocessed data saved to {output_path / 'study2_preprocessed_data.csv'}\")\n",
    "\n",
    "print(\"\\n\" + \"=\"*60)\n",
    "print(\"SIMULATION COMPLETE!\")\n",
    "print(\"\\nNext steps:\")\n",
    "print(\"1. Run study_2_analysis.ipynb for comprehensive analysis\")\n",
    "print(\"2. Results are saved in study_2_results/ directory\")\n",
    "print(\"3. Preprocessed data available for validation\")\n",
    "print(\"=\"*60)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
